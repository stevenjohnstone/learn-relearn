<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A First Course in Stochastic Calculus: Chapter One Exercises</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A First Course in Stochastic Calculus: Chapter One Exercises</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#computer-exercises" id="toc-computer-exercises">Computer Exercises</a>
  <ul>
  <li><a href="#distributions-as-histograms" id="toc-distributions-as-histograms">1.1 Distributions as histograms</a></li>
  <li><a href="#the-law-of-large-numbers" id="toc-the-law-of-large-numbers">1.2 The Law of Large Numbers</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem">1.3 Central Limit Theorem</a></li>
  <li><a href="#sampling-cauchy-random-variables" id="toc-sampling-cauchy-random-variables">1.4 Sampling Cauchy Random Variables</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises">Exercises</a>
  <ul>
  <li><a href="#section" id="toc-section">1.1</a></li>
  <li><a href="#equiprobability" id="toc-equiprobability">1.2 Equiprobability</a></li>
  <li><a href="#distribution-as-a-probability-on-mathbbr" id="toc-distribution-as-a-probability-on-mathbbr">1.3 Distribution as a Probability on <span class="math inline">\(\mathbb{R}\)</span></a></li>
  <li><a href="#distribution-of-an-indicator-function" id="toc-distribution-of-an-indicator-function">1.4 Distribution of an Indicator Function</a></li>
  <li><a href="#events-of-probability-one" id="toc-events-of-probability-one">1.5 Events of Probability One</a></li>
  <li><a href="#constructing-a-random-variable-from-another" id="toc-constructing-a-random-variable-from-another">1.6 Constructing a Random Variable from Another</a></li>
  <li><a href="#sum-of-integrable-variables" id="toc-sum-of-integrable-variables">1.7 Sum of Integrable Variables</a></li>
  <li><a href="#jumps-and-probabilities" id="toc-jumps-and-probabilities">1.8 Jumps and Probabilities</a></li>
  <li><a href="#memory-loss-property" id="toc-memory-loss-property">1.9 Memory Loss Property</a></li>
  <li><a href="#gaussian-integration-by-parts" id="toc-gaussian-integration-by-parts">1.10 Gaussian Integration by Parts</a></li>
  <li><a href="#mgf-of-exponential-random-variables" id="toc-mgf-of-exponential-random-variables">1.11 MGF of Exponential Random Variables</a></li>
  <li><a href="#gaussian-tail" id="toc-gaussian-tail">1.12 Gaussian Tail</a></li>
  <li><a href="#expectation-from-cdf" id="toc-expectation-from-cdf">1.13 Expectation from CDF</a></li>
  <li><a href="#characteristic-function" id="toc-characteristic-function">1.14 Characteristic Function</a></li>
  <li><a href="#when-ex-infty" id="toc-when-ex-infty">1.15 When <span class="math inline">\(E(X) &lt; \infty\)</span></a></li>
  <li><a href="#when-ex-0" id="toc-when-ex-0">1.16 When <span class="math inline">\(E(X) = 0\)</span></a></li>
  </ul></li>
  </ul>
</nav>
<section id="computer-exercises" class="level1">
<h1>Computer Exercises</h1>
<section id="distributions-as-histograms" class="level2">
<h2 class="anchored" data-anchor-id="distributions-as-histograms">1.1 Distributions as histograms</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># (a)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.default_rng().uniform(<span class="dv">0</span>, <span class="dv">1</span>, N)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># (b)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins, label<span class="op">=</span><span class="st">'Unform'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch1_files/figure-html/cell-2-output-1.png" width="575" height="411" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Uniform Distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (c)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins, cumulative<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'CDF X ~ U(0,1)'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-3-output-1.png" width="592" height="411"></p>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Redo (b) and (c) for X^2</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>samples_squared <span class="op">=</span> [ x<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> x <span class="kw">in</span> samples]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.hist(samples_squared, bins, label<span class="op">=</span><span class="st">'PDF X^2'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-4-output-1.png" width="583" height="411"></p>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plt.hist(samples_squared, bins, cumulative<span class="op">=</span><span class="va">True</span>, label<span class="op">=</span><span class="st">'CDF X^2'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-5-output-1.png" width="592" height="411"></p>
</div>
</div>
</section>
<section id="the-law-of-large-numbers" class="level2">
<h2 class="anchored" data-anchor-id="the-law-of-large-numbers">1.2 The Law of Large Numbers</h2>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (a)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.default_rng().exponential(<span class="dv">1</span>, N)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>averages <span class="op">=</span> [sample<span class="op">/</span>(idx <span class="op">+</span><span class="dv">1</span>) <span class="cf">for</span> idx, sample <span class="kw">in</span> <span class="bu">enumerate</span>(np.cumsum(samples))]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.hist(averages, bins)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-6-output-1.png" width="583" height="411"></p>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plt.plot(averages)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'average of N samples'</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'N'</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-7-output-1.png" width="589" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (b)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> average(n):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(np.random.default_rng().exponential(<span class="dv">1</span>, n))<span class="op">/</span>n</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.hist([average(<span class="dv">100</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10000</span>)], bins, label<span class="op">=</span><span class="st">'average(100)'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.hist([average(<span class="dv">10000</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10000</span>)], bins, label<span class="op">=</span><span class="st">'average(10000)'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-8-output-1.png" width="583" height="411"></p>
</div>
</div>
<p>Note how the averages of 10000 samples have less variance than the averages of 100 samples.</p>
</section>
<section id="central-limit-theorem" class="level2">
<h2 class="anchored" data-anchor-id="central-limit-theorem">1.3 Central Limit Theorem</h2>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Y(N):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">sum</span> <span class="op">=</span> np.<span class="bu">sum</span>(np.random.default_rng().exponential(<span class="dv">1</span>, N))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="bu">sum</span> <span class="op">-</span> N) <span class="op">/</span>np.sqrt(N)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">50</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.hist([ Y(<span class="dv">100</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, samples)], bins, label<span class="op">=</span><span class="st">'Y(100)'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.hist(np.random.default_rng().normal(<span class="dv">0</span>, <span class="dv">1</span>, samples), bins, label<span class="op">=</span><span class="st">'N(0,1)'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-9-output-1.png" width="575" height="411"></p>
</div>
</div>
</section>
<section id="sampling-cauchy-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="sampling-cauchy-random-variables">1.4 Sampling Cauchy Random Variables</h2>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> invF(y):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tan((y <span class="op">-</span> <span class="fl">0.5</span>)<span class="op">*</span>np.pi)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [ invF(u) <span class="cf">for</span> u <span class="kw">in</span> np.random.default_rng().uniform(<span class="dv">0</span>, <span class="dv">1</span>, N) ]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Cauchy'</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.hist(np.random.default_rng().normal(<span class="dv">0</span>, <span class="dv">1</span>, N), bins, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'normal'</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-10-output-1.png" width="583" height="411"></p>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [ s<span class="op">/</span>(idx <span class="op">+</span> <span class="dv">1</span>) <span class="cf">for</span> idx, s <span class="kw">in</span> <span class="bu">enumerate</span>(np.cumsum(samples)) ]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.plot(mean, label<span class="op">=</span><span class="st">'average of N samples'</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'N'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-11-output-1.png" width="579" height="429"></p>
</div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cauchy_empirical_mean(N):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>([ invF(u) <span class="cf">for</span> u <span class="kw">in</span> np.random.default_rng().uniform(<span class="dv">0</span>, <span class="dv">1</span>, N) ])<span class="op">/</span>N</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">40</span>, <span class="dv">40</span>, <span class="dv">1000</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.hist([cauchy_empirical_mean(<span class="dv">10</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10000</span>)], bins, label<span class="op">=</span><span class="st">'N=10'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.hist([cauchy_empirical_mean(<span class="dv">100</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10000</span>)], bins, label<span class="op">=</span><span class="st">'N=100'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-12-output-1.png" width="575" height="411"></p>
</div>
</div>
<p>Note how the emperical means with 10 samples and 100 samples appear to be identically distributed. The Cauchy distribution has no defined mean (even though it is symmetrical about 0) so the Central Limit Theorem does not apply.</p>
<p>If we have two iid variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with Cauchy distribution i.e.&nbsp;with pdf</p>
<p><span class="math display">\[
f(x) = \frac{1}{\pi} \frac{1}{1 + x^2} dx.
\]</span></p>
<p>We can get the distribution of <span class="math inline">\(\frac{1}{2}(X + Y)\)</span> by considering the characteristic function of the distribution, <span class="math inline">\(e^{-|t|}\)</span>.</p>
<p>The characteristic function of <span class="math inline">\(\frac{1}{2}(X + Y)\)</span> is</p>
<p><span class="math display">\[
E(e^{it(X + Y)/2}) = E(e^{itX/2}) E(e^{itY/2}) = e^{-2|t/2|} = e^{-|t|}.
\]</span></p>
<p>This tells us that <span class="math inline">\(\frac{1}{2}(X + Y)\)</span> has the same distribution as <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. So, when we calculate empirical means of Cauchy distribution independent variables, the result does not converge to a constant plus a narrow Gaussian error: instead, we get a random variable with the same distribution the samples, regardless of how many samples we take!</p>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">1.1</h2>
<p>Prove proposition 1.3.</p>
<ol type="1">
<li>Finite additivity: if two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint, then <span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span></li>
</ol>
<p>This follows from Definition 2.3 (3) with <span class="math inline">\(A = A_1\)</span>, <span class="math inline">\(B = A_2\)</span> and <span class="math inline">\(A_i = \emptyset\)</span> for <span class="math inline">\(i &gt; 2\)</span>:</p>
<p><span class="math display">\[
\begin{align}
P(A \cup B) &amp;= P (\cup_{i} A_i) = P(A_1) + P(A_2) + \cup_{i &gt; 2} P(A_i) \\
            &amp;= P(A) + P(B) + \cup_{i &gt; 2} P(\emptyset) \\
            &amp;= P(A) + P(B).
\end{align}
\]</span></p>
<ol start="2" type="1">
<li>For any event <span class="math inline">\(A\)</span>, <span class="math inline">\(P(A^c) = 1 - P(A)\)</span>:</li>
</ol>
<p><span class="math inline">\(\Omega\)</span> is the disjoint union of <span class="math inline">\(A\)</span> and <span class="math inline">\(A^c\)</span>. By (1) above <span class="math display">\[
1 = P(\Omega) = P(A \cup A^c) = P(A) + P(A^c)
\]</span> so <span class="math display">\[
P(A) = 1 - P(A^c).
\]</span></p>
<ol start="3" type="1">
<li>For any events <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span>:</li>
</ol>
<p>Express <span class="math inline">\(A \cup B = (A\setminus B) \cup B\)</span> where the union is disjoint. Using (1)</p>
<p><span class="math display">\[
P(A \cup B) = P(A\setminus B) + P(B).
\]</span> Now, <span class="math display">\[
P(A\setminus B) + P(A\cap B) = P(A)
\]</span> and so <span class="math display">\[
P(A\setminus B) = P(A) - P(A\cap B).
\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[
P(A \cup B) = P(A \setminus B) + P(B) = P(A) + P(B) - P(A \cap B).
\]</span></p>
<ol start="4" type="1">
<li>Monotonicity: If <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(P(A) \leq P(B)\)</span>.</li>
</ol>
<p>If <span class="math inline">\(A \subseteq B\)</span>, then <span class="math inline">\(B = A \cup (B \setminus A)\)</span> where the union is disjoint. It follows that</p>
<p><span class="math display">\[
P(B) = P(A) + P(B \setminus A) \geq P(A).
\]</span></p>
</section>
<section id="equiprobability" class="level2">
<h2 class="anchored" data-anchor-id="equiprobability">1.2 Equiprobability</h2>
<p>Let <span class="math inline">\(\Omega\)</span> be a sample space with a finite number of outcomes. We define</p>
<p><span class="math display">\[
P(A) = \#A/\#\Omega
\]</span> for <span class="math inline">\(A \subseteq \Omega\)</span>.</p>
<p><span class="math inline">\(P\)</span> is a probability on <span class="math inline">\(\Omega\)</span>.</p>
<p>Proof:</p>
<ol type="1">
<li><p>Since counts are positive, <span class="math inline">\(P &gt;= 0\)</span>. Since <span class="math inline">\(\#A \leq \#\Omega\)</span>, <span class="math inline">\(P \leq 1\)</span> and so <span class="math inline">\(P(A) \in [0, 1]\)</span> for <span class="math inline">\(A \subseteq \Omega\)</span>.</p></li>
<li><p>The empty set <span class="math inline">\(\emptyset\)</span> has no elements, so <span class="math inline">\(P(\emptyset) = 0\)</span>. It is easy to see that <span class="math display">\[
P(\Omega) = \frac{\# \Omega}{ \# \Omega} =1.
\]</span></p></li>
<li><p>Additivity: for any infinite, mutually disjoint sequence of events <span class="math inline">\(A_1, A_2, \ldots\)</span> there exists <span class="math inline">\(N\)</span> such that <span class="math inline">\(A_n = \emptyset\)</span> for all <span class="math inline">\(n \geq N\)</span>. In fact, <span class="math inline">\(N\)</span> must be less than <span class="math inline">\(\# \Omega\)</span>.</p></li>
</ol>
<p>Then</p>
<p><span class="math display">\[
\begin{align}
P(\cup_{i=1}^{\infty} A_n) &amp;= P(\cup_{i=1}^{N} A_n)\\
&amp;= \frac{\sum_{i=1}^N \# A_n}{\# \Omega} \\
&amp;= \sum_{i=1}^N P(A_n) \\
&amp;= \sum_{i=1}^{\infty} P(A_n).
\end{align}
\]</span>.</p>
</section>
<section id="distribution-as-a-probability-on-mathbbr" class="level2">
<h2 class="anchored" data-anchor-id="distribution-as-a-probability-on-mathbbr">1.3 Distribution as a Probability on <span class="math inline">\(\mathbb{R}\)</span></h2>
<p>Let <span class="math inline">\(\rho_X\)</span> be the distribution of random variables <span class="math inline">\(X\)</span> on some probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. <span class="math inline">\(\rho_X\)</span> has the properties of a probability on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>Proof:</p>
<p>Let <span class="math inline">\(\mathcal{F}_X\)</span> be the <span class="math inline">\(\sigma\)</span>-field of set in <span class="math inline">\(\mathbb{R}\)</span> such that <span class="math inline">\(A \in \mathcal{F} \iff \{\omega \in \Omega: X(\omega) \in A\} \in \mathcal{F}\)</span>.</p>
<p>We define <span class="math inline">\(P_X\)</span> on <span class="math inline">\(\mathcal{F}_X\)</span> by</p>
<p><span class="math display">\[
P_X(A) = P(\{\omega \in \Omega: X(\omega) \in A\}).
\]</span></p>
<p>We note that <span class="math inline">\(P_X\)</span> extends <span class="math inline">\(\rho_X\)</span>.</p>
<p>For notational simplicity, we write</p>
<p><span class="math display">\[
X^{-1}(A) = \{\omega \in \Omega: X(\omega) \in A\}.
\]</span></p>
<p>Clearly, for any <span class="math inline">\(A \in \mathcal{F}_X\)</span>, <span class="math inline">\(P_X(A) = P(X^{-1}(A)) \in [0,1]\)</span>, so satisfies (1) of Definition 1.2.</p>
<p>The pre-image of <span class="math inline">\(\emptyset\)</span>, <span class="math inline">\(X^{-1}(\emptyset)\)</span> must be itself empty. Therefore,</p>
<p><span class="math display">\[
P_X(\emptyset) = P(\emptyset) = 0.
\]</span></p>
<p>Similarly, the pre-image of <span class="math inline">\(\mathbb{R}\)</span> must be all of <span class="math inline">\(\Omega\)</span> and so</p>
<p><span class="math display">\[
P_X(\mathbb{R}) = P(\Omega) = 1.
\]</span></p>
<p>This shows that <span class="math inline">\(P_X\)</span> satisfies (2) of Definition 2.1.</p>
<p>Let <span class="math inline">\(A_1, A_2, \ldots\)</span> be an infinite sequence of events in <span class="math inline">\(\mathcal{F}_X\)</span> that are mutually disjoint. Note that the sequence of pre-images <span class="math inline">\(X^{-1}(A_1), X^{-1}(A_2), \ldots\)</span> are also mutually disjoint. Since <span class="math inline">\(P\)</span> is a probability we can use its additivity to prove the additivity of <span class="math inline">\(P_X\)</span>:</p>
<p><span class="math display">\[
P_X(\cup_{i=1}^{\infty} A_i) = P(\cup_{i=1}^{\infty} X^{-1}(A_i)) = \sum_{i=1}^{\infty} P(X^{-1}(A_i)) = \sum_{i=1}^{\infty} P_X(A_i).
\]</span></p>
<p>This shows that <span class="math inline">\(P_X\)</span> satisfies (3) of Definition 2.1.</p>
</section>
<section id="distribution-of-an-indicator-function" class="level2">
<h2 class="anchored" data-anchor-id="distribution-of-an-indicator-function">1.4 Distribution of an Indicator Function</h2>
<p>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> and <span class="math inline">\(A \in \mathcal{F}\)</span> with <span class="math inline">\(0 &lt; P(A) &lt; 1\)</span>. What is the distribution of <span class="math inline">\(1_A\)</span>?</p>
<p>Observe that <span class="math inline">\(P(1_A \leq x) = 0\)</span> when <span class="math inline">\(x &lt; 0\)</span> because the indicator is a non-negative function. When <span class="math inline">\(0 \leq x &lt; 1\)</span>, <span class="math inline">\(1_A(\omega) &gt; x\)</span> for all <span class="math inline">\(\omega \in A\)</span> but <span class="math inline">\(1_A(\omega) = 0 &lt;= x\)</span> for <span class="math inline">\(\omega \in A^c\)</span>. It follows that <span class="math display">\[P(1_A \leq x) = P(\{\omega \in \Omega : 1_A(\omega) \leq x\}) =P(\{\omega \in \Omega : \omega \in A^c\}) = P(A^c).\]</span> For <span class="math inline">\(x \geq 1\)</span>, <span class="math inline">\(1_A \leq x\)</span> is true for all values of <span class="math inline">\(\omega \in \Omega\)</span> because the maximum value of the indicator function is <span class="math inline">\(1\)</span>. Therefore, with <span class="math inline">\(F\)</span> denoting the CDF of <span class="math inline">\(1_A\)</span>: <span class="math display">\[
F(x) = P(1_A \leq x) = \begin{cases}
0 &amp; \text{if } x &lt; 0,\\
P(A^c) &amp; \text{if } 0 \leq x &lt; 1,\\
1 &amp; \text{if } x \geq 1.
\end{cases}
\]</span></p>
</section>
<section id="events-of-probability-one" class="level2">
<h2 class="anchored" data-anchor-id="events-of-probability-one">1.5 Events of Probability One</h2>
<p>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space and <span class="math inline">\(A_1, A_2, \ldots\)</span> be a sequence of events in <span class="math inline">\(\mathcal{F}\)</span> such that <span class="math inline">\(P(A_n) = 1\)</span> for all <span class="math inline">\(n \geq 1\)</span>. We show that</p>
<p><span class="math display">\[
P(\cap_{i\geq 1} A_n) = 1.
\]</span></p>
<p>Define <span class="math inline">\(B_n = A_n^c\)</span> for all <span class="math inline">\(n \geq 1\)</span>. We note that</p>
<p><span class="math display">\[
P(B_n) = 1 - P(A_n) = 0
\]</span> for all <span class="math inline">\(n \geq 0\)</span>.</p>
<p>Define a sequence of events <span class="math inline">\(C_n = \cup_{i =1}^n B_i\)</span> and note that the sequence is increasing. Continuity of probability gives</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(C_n) = P(\cup_{i=1}^{\infty} B_i).
\]</span></p>
<p>Using <span class="math inline">\(P(A \cup B) \leq P(A) + P(B)\)</span>, we see that</p>
<p><span class="math display">\[
P(C_n) \leq \sum_{i = 1}^n P(B_i) = 0
\]</span></p>
<p>and so <span class="math inline">\(P(C_n) = 0\)</span> and <span class="math display">\[\lim_{n \to \infty} P(C_n) = P(\cup_{i=1}^{\infty} B_i) = 0\]</span>.</p>
<p>To finish the proof, we note that</p>
<p><span class="math display">\[\begin{align}
P(\cap_{i=1}^{\infty} A_i) &amp;= 1 - P((\cap_{i=1}^{\infty} A_i)^c)\\
                           &amp;= 1 - P( \cup_{i =1}^{\infty} A_i^c)\\
                           &amp;= P(\cup_{i=1}^{\infty} B_i) = 0.
\end{align}\]</span></p>
</section>
<section id="constructing-a-random-variable-from-another" class="level2">
<h2 class="anchored" data-anchor-id="constructing-a-random-variable-from-another">1.6 Constructing a Random Variable from Another</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> this is uniformly distributed on <span class="math inline">\([-1, 1]\)</span>. Define <span class="math inline">\(Y = X^2\)</span>.</p>
<ol type="a">
<li>Find the CDF of Y and plot its graph.</li>
</ol>
<p>First, the CDF:</p>
<p>Let <span class="math inline">\(F\)</span> denote the CDF of <span class="math inline">\(Y\)</span> and <span class="math inline">\(F_X\)</span> denote the CDF of X.</p>
<p>Then <span class="math display">\[
G(x) = P(Y \leq x) = P(X^2 \leq x) = P(X \leq \sqrt{x}) = F(\sqrt{x}).
\]</span></p>
<p>Now,</p>
<p><span class="math display">\[
F(x) =
\begin{cases}
0 &amp; \text{if } x \leq 0, \\
x &amp; \text{if } 0 &lt; x &lt; 1, \\
1 &amp; \text{if } x \geq 1.
\end{cases}
\]</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>I’ve taken <span class="math inline">\(F\)</span> to be different to that in the book (the first case in the book uses <span class="math inline">\(x &lt; 0\)</span>) so that the resulting <span class="math inline">\(G\)</span> has a well-defined derivative at zero.</p>
</div>
</div>
</div>
<p>and so</p>
<p><span class="math display">\[
G(x) =
\begin{cases}
0 &amp; \text{if } x &lt;= 0, \\
\sqrt{x} &amp; \text{if } 0 &lt; x &lt; 1, \\
1 &amp; \text{if } x \geq 1.
\end{cases}
\]</span></p>
<p>We plot this below:</p>
<div class="cell" data-collapse="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> G(x):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> x <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> x <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.sqrt(x)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>xticks <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">100</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>plt.plot(xticks, [G(x) <span class="cf">for</span> x <span class="kw">in</span> xticks])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-13-output-1.png" width="571" height="411"></p>
</div>
</div>
<ol start="2" type="a">
<li>the PDF of Y is given by</li>
</ol>
<p><span class="math display">\[
p(y) = \frac{dG}{dy}(y) = \begin{cases}
0 &amp; \text{if } x \leq 0, \\
\frac{1}{2\sqrt{x}} &amp; \text{if } 0 &lt; x &lt; 1, \\
0 &amp; \text{if } 0 \geq 1.
\end{cases}
\]</span></p>
<p>The plot is below</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> p(x):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> x <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> x <span class="op">&lt;</span> <span class="dv">1</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span><span class="op">/</span>( <span class="dv">2</span> <span class="op">*</span> np.sqrt(x)))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>xticks <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.plot(xticks, [p(x) <span class="cf">for</span> x <span class="kw">in</span> xticks])</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>xticks <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">50</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.plot(xticks, [p(x) <span class="cf">for</span> x <span class="kw">in</span> xticks])</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>xticks <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">50</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.plot(xticks, [p(x) <span class="cf">for</span> x <span class="kw">in</span> xticks])</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="ch1_files/figure-html/cell-14-output-1.png" width="558" height="411"></p>
</div>
</div>
</section>
<section id="sum-of-integrable-variables" class="level2">
<h2 class="anchored" data-anchor-id="sum-of-integrable-variables">1.7 Sum of Integrable Variables</h2>
<p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two integrable random variables on the same probability space. Then <span class="math inline">\(aX + bY\)</span> is also an integrable random variable for any <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p>Proof:</p>
<p>We assert that <span class="math inline">\(aX + b Y\)</span> is a random variable and are left to show that</p>
<p><span class="math display">\[
E(\left|aX + bY\right|) &lt; \infty.
\]</span></p>
<p>By the triangle inequality,</p>
<p><span class="math display">\[
\left| aX + bY \right| \leq \left| a \right| \left| X \right| + \left| b \right| \left| Y \right|
\]</span> and so we can conclude that</p>
<p><span class="math display">\[
E(\left|aX + bY\right|) &lt; \infty.
\]</span></p>
</section>
<section id="jumps-and-probabilities" class="level2">
<h2 class="anchored" data-anchor-id="jumps-and-probabilities">1.8 Jumps and Probabilities</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(F_X\)</span> be its CDF. Then</p>
<p><span class="math display">\[
P(X = a) = F_X(a) - \lim_{x \to a-} F_X(x) = F_X(a) - F_X(a-).
\]</span></p>
<p>Proof:</p>
<p>Define a decreasing sequence of events <span class="math inline">\(A_n = \{ X \in (a - 1/n, a]\}\)</span> and note that <span class="math inline">\(\cap_{i=1}^{\infty} A_n = \{ X = a\}\)</span>. By continuity of probability</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(A_n) = P(X = a).
\]</span></p>
<p>Now,</p>
<p><span class="math display">\[\begin{align}
P(X = a) &amp;= \lim_{n \to \infty} P(A_n)\\
         &amp;= \lim_{n \to \infty} (F_X(a) - F_X(a - 1/n))\\
         &amp;= F_X(a) - \lim_{n \to \infty} (F_X(a - 1/n)) = F_X(a) - F_X(a-).
\end{align}\]</span></p>
</section>
<section id="memory-loss-property" class="level2">
<h2 class="anchored" data-anchor-id="memory-loss-property">1.9 Memory Loss Property</h2>
<p>Let <span class="math inline">\(Y\)</span> be an exponential random variable with parameter <span class="math inline">\(\lambda\)</span>. The for any <span class="math inline">\(s, t &gt; 0\)</span></p>
<p><span class="math display">\[
P(Y &gt; t + s | Y &gt; s) = P(Y &gt; t).
\]</span></p>
<p>Proof:</p>
<p>The CDF of the exponential distribution is</p>
<p><span class="math display">\[
F(t) = 1 - e^{-\lambda t}.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
P(Y &gt; t) = 1 - F(t) = e^{-\lambda t}.
\]</span></p>
<p>Now</p>
<p><span class="math display">\[\begin{align}
P(Y &gt; t + s | Y &gt; s) &amp; =  \frac{P(Y &gt; t + s \cap Y &gt; s)}{P(Y &gt; s)} \\
                     &amp; =  \frac{P(Y &gt; t + s)}{P(Y &gt; s)}  \\
                     &amp; =  \frac{e^{-\lambda(t + s)}}{e^{-\lambda s}} \\
                     &amp; =  e^{-\lambda t} \\
                     &amp; =  P(Y &gt; t).
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(P(Y &gt; t + s \cap Y &gt; s) = P(Y &gt; t + s)\)</span> because <span class="math inline">\(\{ \omega \in \Omega | Y(\omega) &gt; t + s \} \subseteq \{ \omega \in \Omega | Y(\omega) &gt; t \}\)</span> and so <span class="math inline">\(\{ \omega \in \Omega | Y(\omega) &gt; t + s \} \cap \{ \omega \in \Omega | Y(\omega) &gt; t \} = \{ \omega \in \Omega | Y(\omega) &gt; t + s\}\)</span>.</p>
</section>
<section id="gaussian-integration-by-parts" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-integration-by-parts">1.10 Gaussian Integration by Parts</h2>
<ol type="a">
<li>Let <span class="math inline">\(Z\)</span> be a standard Gaussian random variable. Then</li>
</ol>
<p><span class="math display">\[
E(Zg(Z)) = E(g'(Z))
\]</span></p>
<p>when both expectations are well-defined.</p>
<p>Proof:</p>
<p>We can use LOTUS and integration by parts:</p>
<p><span class="math display">\[\begin{align}
E(Z g(Z)) = \int_{-\infty}^{\infty} z g(z) p(z) dz  &amp;=  \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} z g(z) e^{-z^2/2} dz \\
&amp;= \int_{-\infty}^{\infty} g(z) (-\frac{dp}{dz}) dz \\
&amp;= -g(z) p(z) \rvert_{-\infty}^{\infty} + \int_{-\infty}^{\infty} g'(z) p(z) dz \\
&amp;= E(g'(Z)).
\end{align}\]</span></p>
<p>Note that</p>
<p><span class="math display">\[
-g(z) p(z) \rvert_{-\infty}^{\infty} = 0
\]</span></p>
<p>follows from <span class="math inline">\(zg(z) p(z)\)</span> being integrable.</p>
<ol start="2" type="a">
<li>In particular, if <span class="math inline">\(g(z) = z^{n + 1}\)</span>, then <span class="math inline">\(g'(z) = (n +1) z^{n}\)</span> so</li>
</ol>
<p><span class="math display">\[
E( Z g(Z)) = E(Z^{n+2}) = E((n + 1) Z^{n}).
\]</span></p>
<p>We see that</p>
<p><span class="math display">\[
\begin{align}
E(Z^{2n}) &amp;= (2n -1) E(Z^{2n -2})\\
          &amp;= (2n -1) (2n -3) E(Z^{2n - 4})\\
          &amp;= (2n - 1) (2n -3) \ldots 1 E(Z^2)\\
          &amp;= (2n -1) (2n -3) \ldots 1.
\end{align}
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
\begin{align}
E(Z^{2n + 1}) &amp;= (2n) E(Z^{2n -1})\\
          &amp;= (2n) (2n -2) E(Z^{2n - 3})\\
          &amp;= (2n) (2n -2) \ldots 1 E(Z)\\
          &amp;= 0.
\end{align}
\]</span></p>
<p>Setting <span class="math inline">\(X = \sigma Z\)</span>, we find that</p>
<p><span class="math display">\[
E(X^{2n}) = \sigma^{2n} (2n -1)(2n -3) \ldots 1.
\]</span></p>
<p>We can’t have a mean different from zero: without symmetry about zero the special structure of odd and even functions would be lost.</p>
</section>
<section id="mgf-of-exponential-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="mgf-of-exponential-random-variables">1.11 MGF of Exponential Random Variables</h2>
<p>Show that for a random variable <span class="math inline">\(X \sim exp(\lambda)\)</span></p>
<p><span class="math display">\[
E(e^{t X}) = \frac{ \lambda}{\lambda - t }, t &lt; \lambda.
\]</span></p>
<p>Using LOTUS and choosing <span class="math inline">\(t &lt; \lambda\)</span></p>
<p><span class="math display">\[\begin{align}
E(e^{tX}) &amp;= \int_{0}^{\infty} e^{tx} \lambda e^{-\lambda x} dx \\
          &amp;= \int_0^{\infty} e^{(t - \lambda) x} dx \label{exp:integral} \tag{*} \\
          &amp;= \frac{1}{t - \lambda} e^{(t - \lambda) x} \rvert_{0}^{\infty} \\
          &amp;= \frac{ \lambda}{\lambda - t }.
\end{align}\]</span></p>
<p>Note that if <span class="math inline">\(t \geq \lambda\)</span>, then the integral <span class="math inline">\(\eqref{exp:integral}\)</span> is not well-defined.</p>
<p>We can calculate <span class="math inline">\(E(X)\)</span>:</p>
<p><span class="math display">\[
E(X) = \frac{d}{dt} E(e^{tX}) \rvert_{t=0} = \frac{\lambda}{(\lambda -t)^2}\rvert_{t=0} = \frac{1}{\lambda}.
\]</span></p>
<p>We can also calculate <span class="math inline">\(Var(X)\)</span>:</p>
<p><span class="math display">\[
E(X^2) = \frac{d^2}{dt^2} E(e^{tX}) \rvert_{t=0} = \frac{2\lambda}{(\lambda -t)^3} \rvert_{t=0} = \frac{2}{\lambda^2}.
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
Var(X) = E(X^2) - E(X)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} =\frac{1}{\lambda^2}.
\]</span></p>
</section>
<section id="gaussian-tail" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-tail">1.12 Gaussian Tail</h2>
<p>Consider a random variable <span class="math inline">\(X\)</span> with finite MGF such that</p>
<p><span class="math display">\[\begin{equation}
E(e^{\lambda X}) \leq e^{\lambda^2/2}
\label{gtail} \tag{1}
\end{equation}\]</span></p>
<p>for all <span class="math inline">\(\lambda \in \mathbb{R}\)</span>.</p>
<p>Prove that for <span class="math inline">\(a &gt; 0\)</span></p>
<p><span class="math display">\[
P(X &gt; a) \leq e^{-a^2/2}.
\]</span></p>
<p>For <span class="math inline">\(\lambda &gt; 0\)</span> <span class="math display">\[
P(X &gt; a) \leq P(e^{\lambda X} &gt; e^{\lambda a})
\]</span> by monotonicity of the exponential. By Markov’s inequality and <span class="math inline">\(\eqref{gtail}\)</span> <span class="math display">\[
P(e^{\lambda X} &gt; e^{\lambda a}) \leq \frac{E(e^{\lambda X})}{e^{\lambda a}} \leq e^{\lambda^2/2 - \lambda a}.
\]</span></p>
<p>Let <span class="math inline">\(f(\lambda) = e^{\lambda^2/2 - \lambda a}\)</span>. The minimum for <span class="math inline">\(f\)</span> is found by differentiation:</p>
<p><span class="math display">\[
f'(\lambda) = (\lambda - a) e^{\lambda^2/2 - \lambda a}
\]</span></p>
<p>and <span class="math inline">\(f'(\lambda) = 0\)</span> is solved for <span class="math inline">\(\lambda = a\)</span>. Therefore,</p>
<p><span class="math display">\[
P(X &gt; a) \leq e^{a^2/2 - a^2} = e^{-a^2/2}.
\]</span></p>
</section>
<section id="expectation-from-cdf" class="level2">
<h2 class="anchored" data-anchor-id="expectation-from-cdf">1.13 Expectation from CDF</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable such that <span class="math inline">\(X &gt;= 0\)</span>. Then</p>
<p><span class="math display">\[
E(X) = \int_0^{\infty} P(X &gt; x) dx.
\]</span></p>
<p>Proof:</p>
<p><span class="math display">\[
\int_0^{\infty} P(X &gt; x) dx = \int_0^{\infty} (1 - F(x)) dx
\]</span></p>
<p>where <span class="math inline">\(F(x) = P(X &lt;= x)\)</span>. We can write</p>
<p><span class="math display">\[
1 - F(x) = \lim_{x \to \infty} F(x) - F(x) = \int_x^{\infty} dF(t)
\]</span></p>
<p>where the integral is understood in the Lebesgue sense.</p>
<p>Now,</p>
<p><span class="math display">\[
\int_0^{\infty} P(X &gt; x) dx = \int_0^{\infty} \int_x^{\infty} dF(t) dx
\]</span></p>
<p>and by changing the order of integration (and appealing to Fubini’s Theorem)</p>
<p><span class="math display">\[
\int_0^{\infty} P(X &gt; x) dx = \int_0^{\infty} \int_0^t dx dF(t) = \int_0^{\infty} t dF(t) = E(X).
\]</span></p>
<p>Subtle point: we’re using the fact that X &gt;= 0 to arrive at zero for the lower limit of the inner integral.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Consequence: LOTUS (Law Of The Unconcious Statistician)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(g: \mathbb{R} \mapsto \mathbb{R}\)</span> be measurable. Then <span class="math inline">\(E(g(X)) = \int_{-\infty}^{\infty} g(x) dF(x)\)</span>.</p>
<p>First suppose that <span class="math inline">\(g &gt;= 0\)</span>. Then</p>
<p><span class="math display">\[
E(g(X)) = \int_0^{\infty} P(g(X) &gt; x) dx
\]</span> by the result above.</p>
<p>Now,</p>
<p><span class="math display">\[
\int_0^{\infty} P(g(X) &gt; x) dx = \int_0^{\infty} \int_{\{z: g(z) &gt; x\}} dF(z) dx.
\]</span></p>
<p>Changing the order of integration, we see get</p>
<p><span class="math display">\[
\int_{-\infty}^{\infty} \int_0^{g(z)} dx dF(z) = \int_{-\infty}^{\infty} g(x) dF(x).
\]</span></p>
<p>Now suppose we have general <span class="math inline">\(g\)</span>. Split <span class="math inline">\(g\)</span> into the sum of non-negative and negative components</p>
<p><span class="math display">\[
g = g_+ - g_{-}
\]</span></p>
<p><span class="math display">\[
\begin{align}
E(g(X)) &amp;= E(g_+(X)) - E(g_{-}(X)) \\
&amp;= \int_{-\infty}^{\infty} g_+(x) dF(x) - \int_{-\infty}^{\infty} g_{-}(x) dF(x) \\
&amp;= \int_{-\infty}^{\infty} g(x) dF(x).
\end{align}
\]</span></p>
</div>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Consequence: Limit of xP(X&gt;x)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(E(X_+) &lt; \infty\)</span>, then <span class="math inline">\(\lim_{x \to \infty} x P(X &gt; x) = 0\)</span>.</p>
<p>We prove this by contradiction. Suppose that there exists <span class="math inline">\(c &gt; 0\)</span> such that <span class="math inline">\(lim_{x \to \infty} xP(X &gt;x) \geq c\)</span>. Then for some <span class="math inline">\(x(c)\)</span>, <span class="math inline">\(xP(X&gt;x) \geq c\)</span> for all <span class="math inline">\(x &gt; x(c)\)</span> and so</p>
<p><span class="math display">\[
E(X+) = \int_0^{\infty} P(X &gt; x) dx \geq \int_{x(c)}^{\infty} P(X &gt; x) dx \geq \int_{x(c)}^{\infty} \frac{c}{x} dx = \infty.
\]</span></p>
<p>This contradicts <span class="math inline">\(E(X_+) &lt; \infty\)</span>.</p>
<p>Contrast this with the Markov inequality which states that</p>
<p><span class="math display">\[
P(X &gt; x) &lt;= \frac{E(X)}{x}
\]</span></p>
<p>which puts a bound on <span class="math inline">\(x P(X &gt; x)\)</span>:</p>
<p><span class="math display">\[
x P(X &gt; x) &lt;= E(X).
\]</span></p>
</div>
</div>
</div>
<p>Take a random variable <span class="math inline">\(X\)</span> such the <span class="math inline">\(E(|X|) &lt; \infty\)</span>. Prove that</p>
<p><span class="math display">\[
E(X) = \int_0^{\infty} P(X &gt; x) dx - \int_{-\infty}^0 P(X \leq x) dx.
\]</span></p>
<p>Define <span class="math inline">\(X_+ = X 1_{X \geq 0}\)</span> and <span class="math inline">\(X_{-} = X 1_{X &lt; 0}\)</span> and note that <span class="math inline">\(X = X_+ + X_{-}\)</span>. Then</p>
<p><span class="math display">\[
E(X) = E(X_+) + E (X_{-})
\]</span> by linearity of expectation. From above,</p>
<p><span class="math display">\[
E(X_+) = \int_0^{\infty} P(X_+ &gt; x) dx = \int_0 ^{\infty} P(X &gt; x) dx.
\]</span></p>
<p>Set <span class="math inline">\(Y = -X_{-}\)</span>. Then <span class="math inline">\(Y &gt; 0\)</span> and</p>
<p><span class="math display">\[
E(Y) = \int_0^{\infty} P(Y &gt; y) dy.
\]</span></p>
<p>Now,</p>
<p><span class="math display">\[\begin{align}
E(X 1_{X &lt; 0}) &amp;= - E(- X_{-}) \\
               &amp;= - E(Y) \\
               &amp;= -\int_0^{\infty} P(-X_{-} &gt;y) dy \\
               &amp;= \int_{0}^{-\infty} P(X_{-} &lt; x) dx \\
               &amp;= \int_0^{-\infty} P(X &lt; x) dx
\end{align}\]</span></p>
<p>Then</p>
<p><span class="math display">\[
E(X) = \int_0^{\infty} P(X &gt; x) dx - \int_{-\infty}^0 P(X &lt; x) dx.
\]</span></p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The result we have to prove is not generally true
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\int (P(X &lt;= x) -P(X&lt;x)) dx = \int P(X = x) dx = 0
\]</span></p>
<p>when <span class="math inline">\(X\)</span> has a PDF (the CDF is at least continuous). In this case, we can state</p>
<p><span class="math display">\[
E(X) = \int_0^{\infty} P(X &gt; x) dx - \int_{-\infty}^0 P(X \leq x) dx.
\]</span></p>
</div>
</div>
</section>
<section id="characteristic-function" class="level2">
<h2 class="anchored" data-anchor-id="characteristic-function">1.14 Characteristic Function</h2>
<ol type="a">
<li>Note that the expectation of a complex-valued random variable (or indeed the complex-valued random variable) is defined in Chapter 1.</li>
</ol>
<p>A good definition is</p>
<p><span class="math display">\[
E(Z) = E(\Re(Z)) + i E(\Im(Z)).
\]</span></p>
<p>Note that if <span class="math inline">\(E(|Z|) &lt; \infty\)</span>, then the real and imaginary parts of <span class="math inline">\(Z\)</span> are also integrable.</p>
<p>We can extend LOTUS to complex valued functions. Suppose that <span class="math inline">\(g: \mathbb{R} \to \mathbb{C}\)</span> and <span class="math inline">\(E(|g(X)|) &lt; \infty\)</span>. We can decompose <span class="math inline">\(g\)</span> as</p>
<p><span class="math display">\[
g = g_1 + i g_2.
\]</span></p>
<p>Note that <span class="math inline">\(g_1(X)\)</span> and <span class="math inline">\(g_2(X)\)</span> are integrable.</p>
<p>where <span class="math inline">\(g_1, g_2 : \mathbb{R} \to \mathbb{R}\)</span>. Then by linearity of expectation</p>
<p><span class="math display">\[\begin{align}
E(g(X)) &amp;= E(g_1(X)) + i E(g_2(X))\\
        &amp;= \int_{-\infty}^{\infty} g_1(x) dF(x) + i \int_{-\infty}^{\infty} g_2(x) dF(x)\\
        &amp;= \int_{-\infty}^{\infty} (g_1(x) + i g_2(x)) dF(x)\\
        &amp;= \int_{-\infty}^{\infty} g(x) dF(x).
\end{align}\]</span></p>
<p>Using this result, we see that</p>
<p><span class="math display">\[
E(e^{itX}) = E(\cos(tX)) + i E(\sin(tX))
\]</span></p>
<p>from the familiar Euler identity <span class="math inline">\(e^{itx} = \cos(tx) + i \sin(tx)\)</span>.</p>
<p>The same result can be argued using the Taylor series:</p>
<p><span class="math display">\[\begin{align}
E(e^{itX}) &amp;= \sum_{n=0}^{\infty} E( \frac{(itX)^n}{n!})\\
           &amp;= \sum_{n=0}^{\infty} E( \frac{((-1)^{n} t^{2n} X^{2n}}{(2n)!}) + i \sum_{i=0}^{\infty} E( \frac{((-1)^{n} t^{2n +1} X^{2n + 1}}{(2n + 1)!})\\
           &amp;= E(\cos(tX)) + iE(\sin(tX).
\end{align}\]</span></p>
<ol start="2" type="a">
<li>Let <span class="math inline">\(X \sim \mathcal{N}(0, 1)\)</span>. Then</li>
</ol>
<p><span class="math display">\[
E(e^{itX}) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} e^{itx} e^{-\frac{1}{2}x^2} dx.
\]</span></p>
<p>Define <span class="math inline">\(g(t) = E(e^{itX})\)</span> and try to build a differential equation we can solve which is hopefully equal to the desired result.</p>
<p>Using the Dominated Convergence Theorem, we can take differentiation inside the expectation integral (the derivative of the integrand is dominated by <span class="math inline">\(e^{-\frac{1}{2}x^2}\)</span> which is integrable).</p>
<p>We get</p>
<p><span class="math display">\[\begin{align}
g'(t) &amp;= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} i x e^{itx} e^{-\frac{1}{2}x^2} dx.
\end{align}\]</span></p>
<p>Use integration by parts:</p>
<p>Set <span class="math inline">\(u = ie^{itx}\)</span> so <span class="math inline">\(du = -t e^{itx}\)</span> and</p>
<p><span class="math display">\[\begin{align}
dv &amp;= \frac{1}{\sqrt{2 \pi}} x e^{-\frac{1}{2}x^2}\\
   &amp;= -\frac{1}{\sqrt{2 \pi}} \frac{d}{dx}(e^{-\frac{1}{2}x^2}).
\end{align}\]</span></p>
<p>Performing the integration by parts we see that <span class="math display">\[
\begin{align}
g'(t) &amp;= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} i x e^{itx} e^{-\frac{1}{2}x^2} dx \\
      &amp;= -\frac{1}{\sqrt{2 \pi}} ie^{itx} e^{-\frac{1}{2}x^2} \rvert_{-\infty}^{\infty} -  \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} te^{itx}  e^{-\frac{1}{2}x^2} dx \\
      &amp;= 0 + iE(e^{itX})\\
      &amp;= -t g(t). \label{characteristic:diffeqn} \tag{*}
\end{align}
\]</span> Moreover,</p>
<p><span class="math display">\[
g(0) = E(e^{0}) = E(1) = 1.
\]</span></p>
<p>The unique solution to <span class="math inline">\(\eqref{characteristic:diffeqn}\)</span> is</p>
<p><span class="math display">\[
g(t) = \frac{1}{2}e^{-t^2/2}
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
E(e^{itX}) = \frac{1}{2}e^{-t^2/2}.
\]</span></p>
<p>Let <span class="math inline">\(Z = \sigma X + \mu\)</span> for <span class="math inline">\(\sigma &gt; 0\)</span> and <span class="math inline">\(\mu \in \mathbb{R}\)</span>. Then</p>
<p><span class="math display">\[\begin{align}
E(e^{itZ}) &amp;= E(e^{it(\sigma X + \mu)}) \\
           &amp;= E(e^{\sigma X} e^{it\mu}) \\
           &amp;= e^{it\mu}  E(e^{it\sigma X})\\
           &amp;= e^{it\mu}e^{-(t\sigma)^2/2}\\
           &amp;= e^{it \mu - \sigma^2t^2/2}.
\end{align}\]</span></p>
</section>
<section id="when-ex-infty" class="level2">
<h2 class="anchored" data-anchor-id="when-ex-infty">1.15 When <span class="math inline">\(E(X) &lt; \infty\)</span></h2>
<p>Let <span class="math inline">\(X \geq 0\)</span> be a random variable on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. If <span class="math inline">\(E(X) &lt; \infty\)</span>, then <span class="math inline">\(P(X = \infty) = 0\)</span>.</p>
<p>Proof:</p>
<p>Let <span class="math inline">\(A_n = \{\omega \in \Omega: X(\omega) &gt; n \}\)</span> for <span class="math inline">\(n &gt; 0\)</span>.</p>
<p>We note that the sequence of events <span class="math inline">\(A_n\)</span> is decreasing and so by continuity of probability</p>
<p><span class="math display">\[
\lim_{n\to \infty} P(A_n) = P(\cap A_n) = P(X = \infty).
\]</span></p>
<p>The proof is complete with an application of Markov’s inequality: <span class="math display">\[
P(X = \infty) = \lim_{n \to \infty} P(A_n) \leq \lim_{n\to\infty} \frac{1}{n} E(X) = 0.
\]</span></p>
</section>
<section id="when-ex-0" class="level2">
<h2 class="anchored" data-anchor-id="when-ex-0">1.16 When <span class="math inline">\(E(X) = 0\)</span></h2>
<p>Let <span class="math inline">\(X \geq 0\)</span> be a random variable on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>. If <span class="math inline">\(E(X) = 0\)</span>, then <span class="math inline">\(P(X = 0) = 1\)</span>.</p>
<p>Proof:</p>
<p>By Markov’s inequality</p>
<p><span class="math display">\[
P(X &gt; 1/n) \leq n E(X) = 0
\]</span></p>
<p>and so <span class="math inline">\(P(X &gt; 1/n) = 0\)</span> for <span class="math inline">\(n = 1, 2, \ldots\)</span>. It follows that</p>
<p><span class="math display">\[
P(X \leq 1/n) = 1 - P(X &gt; 1/n) = 1.
\]</span></p>
<p>The sequence of events <span class="math inline">\(\{ X \leq 1/n \}\)</span> is decreasing and</p>
<p><span class="math display">\[
\{ X = 0 \} = \cap \{ X \leq 1/n \}.
\]</span></p>
<p>By the continuity of probability</p>
<p><span class="math display">\[
P(X = 0) = \lim_{n \to \infty} P(X \leq 1/n) = 1.
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>