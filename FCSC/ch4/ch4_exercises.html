<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Solutions to exercises and computer experiments">

<title>learn-relearn - Exercises</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">learn-relearn</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-first-course-in-stochastic-calculus" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">First Course in Stochastic Calculus</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-first-course-in-stochastic-calculus">    
        <li class="dropdown-header">First Course in Stochastic Calculus</li>
        <li>
    <a class="dropdown-item" href="../../FCSC/ch1/ch1_experiments.html">
 <span class="dropdown-text">Chapter 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FCSC/ch2/ch2_experiments.html">
 <span class="dropdown-text">Chapter 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FCSC/ch3/ch3_experiments.html">
 <span class="dropdown-text">Chapter 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FCSC/ch4/ch4_experiments.html">
 <span class="dropdown-text">Chapter 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../FCSC/ch5/ch5_experiments.html">
 <span class="dropdown-text">Chapter 5</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-probabilistic-machine-learning" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Probabilistic Machine Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-probabilistic-machine-learning">    
        <li class="dropdown-header">Probabilistic Machine Learning</li>
        <li>
    <a class="dropdown-item" href="../../PML/ch2.html">
 <span class="dropdown-text">Probabilistic Machine Learning</span></a>
  </li>  
    </ul>
  </li>
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Exercises</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">First Course in Stochastic Calculus</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Chapter 1</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch1/ch1_experiments.html" class="sidebar-item-text sidebar-link">Computer Experiments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch1/ch1_exercises.html" class="sidebar-item-text sidebar-link">Exercises</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">Chapter 2</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch2/ch2_experiments.html" class="sidebar-item-text sidebar-link">Computer Experiments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch2/ch2_exercises.html" class="sidebar-item-text sidebar-link">Exercises</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">Chapter 3</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch3/ch3_experiments.html" class="sidebar-item-text sidebar-link">Computer Experiments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch3/ch3_exercises.html" class="sidebar-item-text sidebar-link">Exercises</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Chapter 4</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch4/ch4_experiments.html" class="sidebar-item-text sidebar-link">Computer Experiments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch4/ch4_exercises.html" class="sidebar-item-text sidebar-link active">Exercises</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">Chapter 5</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch5/ch5_experiments.html" class="sidebar-item-text sidebar-link">Computer Experiments</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../FCSC/ch5/ch5_exercises.html" class="sidebar-item-text sidebar-link">Exercises</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Exercises</h1>
</div>

<div>
  <div class="description">
    Solutions to exercises and computer experiments
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#conditional-expectation-of-continuous-random-variables" id="toc-conditional-expectation-of-continuous-random-variables">4.1 Conditional Expectation of Continuous Random Variables</a></li>
  <li><a href="#exercises-on-sigma-fields" id="toc-exercises-on-sigma-fields">4.2 Exercises on Sigma-Fields</a></li>
  <li><a href="#proof-of-theorem-4.16" id="toc-proof-of-theorem-4.16">4.3 Proof of Theorem 4.16</a></li>
  <li><a href="#another-look-at-conditional-expectation-for-gaussians" id="toc-another-look-at-conditional-expectation-for-gaussians">4.4 Another Look at Conditional Expectation For Gaussians</a></li>
  <li><a href="#gaussian-conditioning" id="toc-gaussian-conditioning">4.5 Gaussian Conditioning</a></li>
  <li><a href="#gaussian-conditioning-1" id="toc-gaussian-conditioning-1">4.6 Gaussian Conditioning</a></li>
  <li><a href="#gaussian-conditioning-2" id="toc-gaussian-conditioning-2">4.7 Gaussian Conditioning</a></li>
  <li><a href="#gaussian-conditioning-3" id="toc-gaussian-conditioning-3">4.8 Gaussian Conditioning</a></li>
  <li><a href="#properties-of-conditional-expectation" id="toc-properties-of-conditional-expectation">4.9 Properties of Conditional Expectation</a></li>
  <li><a href="#square-of-brownian-motion" id="toc-square-of-brownian-motion">4.10 Square of Brownian Motion</a></li>
  <li><a href="#geometric-poisson-process" id="toc-geometric-poisson-process">4.11 Geometric Poisson Process</a></li>
  <li><a href="#another-brownian-martingale" id="toc-another-brownian-martingale">4.12 Another Brownian Martingale</a></li>
  <li><a href="#limit-of-geometric-brownian-motion" id="toc-limit-of-geometric-brownian-motion">4.13 Limit of Geometric Brownian Motion</a></li>
  <li><a href="#gamblers-ruin-at-french-roulette" id="toc-gamblers-ruin-at-french-roulette">4.14 Gambler’s Ruin at French Roulette</a></li>
  <li><a href="#la-martingale-classique" id="toc-la-martingale-classique">4.15 La Martingale Classique</a></li>
  <li><a href="#a-martingale-from-conditional-expectation" id="toc-a-martingale-from-conditional-expectation">4.16 A Martingale From Conditional Expectation</a></li>
  <li><a href="#joint-distribution-of-max_t-leq-t-b_t-b_t" id="toc-joint-distribution-of-max_t-leq-t-b_t-b_t">4.17 Joint Distribution of <span class="math inline">\((max_{t \leq T} B_t, B_T)\)</span></a></li>
  <li><a href="#zeros-of-brownian-motion" id="toc-zeros-of-brownian-motion">4.18 Zeros of Brownian Motion</a></li>
  <li><a href="#doobs-maximal-inequalities" id="toc-doobs-maximal-inequalities">4.19 Doob’s Maximal Inequalities</a></li>
  <li><a href="#an-application-of-doobs-maximal-inequalities" id="toc-an-application-of-doobs-maximal-inequalities">4.20 An Application of Doob’s Maximal Inequalities</a></li>
  <li><a href="#an-example-of-fubinis-theorem" id="toc-an-example-of-fubinis-theorem">4.21 An Example of Fubini’s Theorem</a></li>
  </ul>
</nav>
<div class="hidden">
<p><span class="math display">\[
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\P}{\operatorname{P}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\qed}{\tag*{$\square$}}
\def\iddots{{\kern3mu\raise1mu{.}\kern3mu\raise6mu{.}\kern3mu
\raise12mu{.}}}
\]</span></p>
</div>
<section id="conditional-expectation-of-continuous-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectation-of-continuous-random-variables">4.1 Conditional Expectation of Continuous Random Variables</h2>
<p>Let <span class="math inline">\((X, Y)\)</span> be two random variables with joint density <span class="math inline">\(f(x, y)\)</span> on <span class="math inline">\(\mathbb{R}^2\)</span>. Suppose that</p>
<p><span class="math display">\[
\int_{\mathbb{R}} f(x, y)\, dx &gt; 0
\]</span> for every <span class="math inline">\(y \in \mathbb{R}\)</span>.</p>
<p>Then <span class="math inline">\(\E(Y | X) = h(X)\)</span> where</p>
<p><span class="math display">\[
h(x) = \frac{\int_{\mathbb{R}} y f(x, y) dy}{\int_{\mathbb{R}} f(x, y) dy}.
\]</span></p>
<p>Proof:</p>
<p>The conditional expectation <span class="math inline">\(E(Y | X)\)</span> is the function <span class="math inline">\(\eta : \mathbb{R} \to \mathbb{R}\)</span> satisifying</p>
<p><span id="eq-conditional-orthogonal"><span class="math display">\[
\E(g(X) Y) = \E(g(X)\eta(X))
\tag{1}\]</span></span></p>
<p>for any bounded random variable of the form <span class="math inline">\(g(X)\)</span> for some function <span class="math inline">\(g\)</span>. That is,</p>
<p><span class="math display">\[
Y - \eta
\]</span> is othorgonal to <span class="math inline">\(g(X)\)</span>.</p>
<p>We can show that <span class="math inline">\(h(X) = \E(Y | X)\)</span> by showing that it satisfies (<a href="#eq-conditional-orthogonal">Equation&nbsp;1</a>) and invoking the uniqueness of such a function.</p>
<p>Let <span class="math inline">\(g\)</span> be any function such that <span class="math inline">\(g(X)\)</span> is bounded and measurable. Using LOTUS</p>
<p><span class="math display">\[
\begin{align}
\E(g(X) Y) &amp;= \int \int g(x) y f(x, y)\, dx\, dy \\
&amp;= \int g(x) \left (\int y f(x, y) \, dy\,\right) dx \\
&amp;= \int g(x) \left (\frac{\int y f(x, y) \, dy}{\int f(x, y)\, dy}\right )\left(\int f(x, y)\, dy\right) \, dx \\
&amp;= \int \int g(x) h(x) f(x, y) \,dx\,dy \\
&amp;= \int g(x) h(x) f_X(x) \,dx \\
&amp;= \E(g(X) h(X)).
\end{align}
\]</span></p>
<p>Therefore <span class="math inline">\(h= \E(Y|X)\)</span>. In particular, setting <span class="math inline">\(g = 1\)</span>, we see that</p>
<p><span class="math display">\[
\E(\E(Y|X)) = \E(Y).
\]</span></p>
</section>
<section id="exercises-on-sigma-fields" class="level2">
<h2 class="anchored" data-anchor-id="exercises-on-sigma-fields">4.2 Exercises on Sigma-Fields</h2>
<ol type="a">
<li>Let <span class="math inline">\(A, B\)</span> be two proper subset of <span class="math inline">\(\Omega\)</span> such that <span class="math inline">\(A \cap B \neq \emptyset\)</span>.</li>
</ol>
<p>Partition <span class="math inline">\(\Omega\)</span> into <span class="math inline">\(4\)</span> disjoint elements of <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[
\Omega = (A\setminus B) \cup (B \setminus A) \cup (A \cap B) \cup (A \cup B)^c.
\]</span></p>
<p>To ease notation, define</p>
<p><span class="math display">\[
\begin{align}
S_0 &amp;= A \setminus B, \\
S_1 &amp;= B \setminus A, \\
S_2 &amp;= A \cap B \\
S_3 &amp;= (A \cup B)^c.
\end{align}
\]</span></p>
<p>Each element of <span class="math inline">\(\sigma\)</span> can be expressed as a union of at most <span class="math inline">\(4\)</span> of these sets: the number of elements of <span class="math inline">\(\sigma\)</span> is <span class="math inline">\(2^4 = 16\)</span>.</p>
<p>Enumerating these:</p>
<p><span class="math display">\[
\begin{align}
0000 &amp;\to \emptyset, \\
0001 &amp;\to A \setminus B, \\
0010 &amp;\to B \setminus A, \\
0011 &amp;\to (A \setminus B) \cup (B \setminus A) = (A \cup B) \setminus (A\cap B), \\
0100 &amp;\to A \cap B, \\
0101 &amp;\to (A \cap B) \cup (A \setminus B) = A, \\
0110 &amp;\to (A \cap B) \cup (B \setminus A) = B, \\
0111 &amp;\to (A \cap B) \cup (B \setminus B) \cup (A \setminus B) = A \cup B, \\
1000 &amp;\to (A \cup B)^c, \\
1001 &amp;\to (A \cup B)^c \cup (A \setminus B) = B^c,\\
1010 &amp;\to (A \cup B)^c \cup (B \setminus A) = A^c, \\
1011 &amp;\to (A \cup B)^c \cup (B \setminus A) \cup (A \setminus B) = (A \cap B)^c,\\
1100 &amp;\to (A \cup B)^c \cup (A \cap B) = (A \setminus B)^c \cap (B \setminus A)^c, \\
1101 &amp;\to (A \cup B)^c \cup (A \cap B) \cup (A \setminus B) = (B \setminus A)^c, \\
1110 &amp;\to (A \cup B)^c \cup (A \cap B) \cup (B \setminus A) = (A \setminus B)^c, \\
1111 &amp;\to \Omega.
\end{align}
\]</span></p>
<p>If <span class="math inline">\(A \cap B = \emptyset\)</span>, then <span class="math inline">\(\Omega\)</span> can be paritioned into the union of <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\((A \cup B)^c\)</span>: there are <span class="math inline">\(2^3 = 8\)</span> events in <span class="math inline">\(\sigma\)</span>. Alternatively, we can just go through the list of <span class="math inline">\(16\)</span> above and cross off those which end up being empty or duplicate.</p>
<ol start="2" type="a">
<li>Show that the singleton <span class="math inline">\(\{b\} \in \mathcal{B}(\mathbb{R})\)</span>.</li>
</ol>
<p>For <span class="math inline">\(n \geq 0\)</span>, <span class="math inline">\(I_n = (b - 1/n, b] \in \mathcal{B}(\mathbb{R})\)</span>. The countable itersection of <span class="math inline">\(\cap_{n \geq 1} I_n = \{b\}\)</span> is also in the sigma-algebra.</p>
<p>All open intervals are in the Borel sigma-field: <span class="math inline">\((a, b) = (a, b] \cap \{b\}^c\)</span>. All closed intervals are in the Borel sigma-field: <span class="math inline">\([a, b] = \{a\} \cup (a, b) \cup \{b\}\)</span>.</p>
<p>Is the subset <span class="math inline">\(\mathbb{Q}\)</span> a Borel set? That is, is <span class="math inline">\(\mathcal{Q} \in \mathcal{B}(\mathbb{R})\)</span>? Yes: the rationals are countable, so they can be expressed as the union of singleton rational sets which are in the sigma-field <span class="math inline">\(\mathcal{B}(\mathbb{R})\)</span>.</p>
</section>
<section id="proof-of-theorem-4.16" class="level2">
<h2 class="anchored" data-anchor-id="proof-of-theorem-4.16">4.3 Proof of Theorem 4.16</h2>
<p>Define <span class="math inline">\(L^2(\G) = L^2(\Omega, \G, \P)\)</span>. Let <span class="math inline">\(Y\)</span> be a random variable in <span class="math inline">\(L^2(\Omega, \F, \P)\)</span> and let $Y^{} satisfy</p>
<p><span id="eq-minimizer"><span class="math display">\[
\min_{Z \in L^2(\G)} \E((Y - Z)^2) = \E((Y - Y^{\star})^2).
\tag{2}\]</span></span></p>
<p>We show that <span class="math inline">\(E(Y | X) = Y^{\star}\)</span>.</p>
<p>Let <span class="math inline">\(W\)</span> be a random variable in <span class="math inline">\(L^2(\G)\)</span>: we show that <span class="math inline">\(W\)</span> is orthogonal to <span class="math inline">\(Y - Y^{\star}\)</span> which shows that <span class="math inline">\(Y^{\star}\)</span> is the orthogonal projection of <span class="math inline">\(Y\)</span> into <span class="math inline">\(L^2(\G)\)</span>.</p>
<p>Developing the sqaure:</p>
<p><span class="math display">\[
\E((W - (Y - Y^{\star}))^2) = \E(W^2) - 2 \E(W(Y - Y^{\star})) + \E((Y - Y^{\star})^2).
\]</span></p>
<p>From the definition of <span class="math inline">\(Y^{\star}\)</span> (and the fact that <span class="math inline">\(W + Y^{\star} \in L^2(\G)\)</span>)</p>
<p><span class="math display">\[
\E((W - (Y - Y^{\star}))^2) \geq \E(Y - Y^{\star}).
\]</span></p>
<p>It follows that <span class="math display">\[
\E(W^2) \geq 2\E(W(Y - Y^{\star})).
\]</span></p>
<p>This holds for any <span class="math inline">\(W\)</span> and so we can assert that <span class="math display">\[
a^2\E(W^2) \geq 2 a\E(W(Y - Y^{\star})).
\]</span> Taking <span class="math inline">\(a &gt; 0\)</span>, we find that</p>
<p><span class="math display">\[
\E(W(Y - Y^{\star})) \leq \frac{a\E(W^w)}{2}
\]</span></p>
<p>and for <span class="math inline">\(a &lt; 0\)</span></p>
<p><span class="math display">\[
\E(W(Y - Y^{\star})) \geq \frac{a\E(W^w)}{2}.
\]</span></p>
<p>Taking the limit as <span class="math inline">\(a \to 0\)</span>, we see that</p>
<p><span class="math display">\[
\E(W(Y - Y^{\star})) = 0
\]</span></p>
<p>which is a defining propery of the conditional expectation <span class="math inline">\(E(Y | \G)\)</span>.</p>
<p>Uniqueness:</p>
<p>Suppose that there are two conditional expectations of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\G\)</span>, <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>. Then for any <span class="math inline">\(W \in L^2(\G)\)</span>,</p>
<p><span class="math display">\[
\E(W(C_1 - C_2)) = 0.
\]</span></p>
<p>Setting <span class="math inline">\(W = C_1 - C_2\)</span>, we see that</p>
<p><span class="math display">\[
\E((C_1 - C_2)^2) = \| C_1 - C_2 \|_2^2 = 0
\]</span></p>
<p>and so <span class="math inline">\(C_1 = C_2\)</span>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The proof of Theorem 4.7 in the book shows uniqueness of <span class="math inline">\(Y^{\star}\)</span> satifying (<a href="#eq-minimizer">Equation&nbsp;2</a>) but does not actually show that a conditional expection is such a variable.</p>
</div>
</div>
</section>
<section id="another-look-at-conditional-expectation-for-gaussians" class="level2">
<h2 class="anchored" data-anchor-id="another-look-at-conditional-expectation-for-gaussians">4.4 Another Look at Conditional Expectation For Gaussians</h2>
<p>Let <span class="math inline">\((X, Y)\)</span> be a Gaussian vector with mean <span class="math inline">\(0\)</span> and covariance matrix</p>
<p><span class="math display">\[
\begin{align}
\mathcal{C} &amp;= \begin{bmatrix}
1 &amp; \rho \\
\rho &amp; 1.
\end{bmatrix}
\end{align}
\]</span></p>
<ol type="a">
<li>The conditional expectation can be calculated using the <span class="math inline">\(L^2\)</span> best approximation form:</li>
</ol>
<p><span class="math display">\[
\begin{align}
\E(Y|X) &amp;= \frac{\E(XY) X}{\E(X^2)} \\
&amp;= \rho X.
\end{align}
\]</span></p>
<ol start="2" type="a">
<li>The joint pdf of <span class="math inline">\((X, Y)\)</span> is</li>
</ol>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fractions <span class="im">import</span> Fraction</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>x, y, rho <span class="op">=</span> sp.symbols(<span class="st">'x, y, rho'</span>, real<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> sp.Matrix([[<span class="dv">1</span>, rho], [rho, <span class="dv">1</span>]])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>Cinv <span class="op">=</span> C.inv()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> sp.Matrix([x, y])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> sp.exp(Fraction(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>) <span class="op">*</span> sp.MatMul(xy.transpose(), Cinv, xy))<span class="op">/</span>((<span class="dv">2</span> <span class="op">*</span> sp.pi)<span class="op">**</span>(Fraction(<span class="bu">len</span>(xy),<span class="dv">2</span>)) <span class="op">*</span> sp.sqrt(C.det()))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>pdf[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<p><span class="math inline">\(\displaystyle \frac{e^{\frac{- 2 \rho x y + x^{2} + y^{2}}{2 \left(\rho - 1\right) \left(\rho + 1\right)}}}{2 \pi \sqrt{1 - \rho^{2}}}\)</span></p>
</div>
</div>
<p><span class="math display">\[
\begin{align}
\frac{\int_{\mathbb{R}} y f(x, y)\, dy}{\int_{\mathbb{R}} f (x, y) \, dy} &amp;= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&amp;= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&amp;= \frac{\int_{\mathbb{R}} (t + \rho x) e^{\frac{-t^2}{2(1-\rho)(1+\rho)}} \, dt}{\int_{\mathbb{R}} e^{\frac{-t^2}{2(1 -\rho)(1 + \rho)}} \, dt} \\
&amp;= \sqrt{1 - \rho^2} \frac{\int_{\mathbb{R}} t e^{-t^2/2}\,dt}{\int_{\mathbb{R}} e^{-t^2/2}\, dt} + \rho x \\
&amp;= \rho x = h(x).
\end{align}
\]</span></p>
<p>This is another way of arriving at <span class="math display">\[
\E(Y|X) = h(X).
\]</span></p>
</section>
<section id="gaussian-conditioning" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-conditioning">4.5 Gaussian Conditioning</h2>
<p><span class="math display">\[
\mathcal{C} = \begin{bmatrix}
2 &amp; 2 &amp; 0 \\
2 &amp; 4 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}.
\]</span></p>
<p>It is easy to see that <span class="math inline">\(\det{C} = 4\)</span>; non-zero determinant means that the vector <span class="math inline">\((X_1, X_2, X_3 X_3)\)</span> is non-degenerate.</p>
<p>From the covariance matrix, we see that <span class="math inline">\(\E(X_1X_3) = \E(X_2 X_3) = 0\)</span>, so <span class="math inline">\(X_3\)</span> is independent of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\E(X_2 | X_1) &amp;= \frac{\E(X_2X_1)}{\E(X_1^2)}X_1 \\
&amp;= \frac{2}{2} X_1 \\
&amp;= X_1.
\end{align}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
X_2 = X_1 + (X_2 - X_1)
\]</span></p>
<p>is a decomposition of <span class="math inline">\(X_2\)</span> into a linear combination of <span class="math inline">\(X_1\)</span> and a random variable independent of <span class="math inline">\(X_1\)</span>, namely <span class="math inline">\(X_2 - X_1\)</span>.</p>
</section>
<section id="gaussian-conditioning-1" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-conditioning-1">4.6 Gaussian Conditioning</h2>
<p><span class="math inline">\((X, Y)\)</span> is a Gaussian random vector with mean <span class="math inline">\(0\)</span> and covariance given by</p>
<p><span class="math display">\[
\mathcal{C} = \begin{bmatrix}
3/16 &amp; 1/8 \\
1/8 &amp; 1/4 \\
\end{bmatrix}.
\]</span></p>
<p><span class="math inline">\((X, Y)\)</span> is non-degenerate because <span class="math inline">\(\det{C} &gt; 0\)</span>:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> sp.Matrix([[Fraction(<span class="dv">3</span>, <span class="dv">16</span>), Fraction(<span class="dv">1</span>, <span class="dv">8</span>)], [Fraction(<span class="dv">1</span>,<span class="dv">8</span>), Fraction(<span class="dv">1</span>,<span class="dv">4</span>)]])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>C.det()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<p><span class="math inline">\(\displaystyle \frac{1}{32}\)</span></p>
</div>
</div>
<p><span class="math display">\[
\E(Y|X) = \frac{\E(YX)}{\E(X^2)}X = (1/8) * (16/3)X =2X/3.
\]</span></p>
<p><span class="math inline">\(W = (Y - 2X/3)\)</span> is independent of <span class="math inline">\(X\)</span> (it is orthogonal to all functions of <span class="math inline">\(X\)</span>). We need</p>
<p><span class="math display">\[
\E(W^2) = \E(Y^2 - 4XY/3 + 4X^2/9) = \frac{1}{4} - \frac{4}{8.3} + \frac{4.3}{9.16} = \frac{1}{6}.
\]</span></p>
<p>We can calculate the MGF of <span class="math inline">\(Y\)</span> conditioned on <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\E(e^{aY} | X) &amp;= \E(e^{a(W + 2X/3)} | X) \\
&amp;= e^{2X/3} \E(e^{aW} X) \\
&amp;= e^{2X/3} \E(e^{aW})  \\
&amp;= e^{2X/3+ a^2/12}  \\
\end{align}
\]</span></p>
<p>and so the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is a Gaussian with mean <span class="math inline">\(2X/3\)</span> and variance <span class="math inline">\(1/6\)</span>.</p>
<p>We define <span class="math inline">\(Z_1 = 16X/3\)</span> and <span class="math inline">\(Z_2 = 6(Y - 2X/3)\)</span>: <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are standard Gaussians and</p>
<p><span class="math display">\[
X = 16X/3
\]</span> and <span class="math display">\[
Y = Z_1/9 + Z_2/6.
\]</span></p>
</section>
<section id="gaussian-conditioning-2" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-conditioning-2">4.7 Gaussian Conditioning</h2>
<p><span class="math display">\[
\mathcal{C} = \begin{bmatrix}
1 &amp; -1 \\
-1 &amp; 2 \\
\end{bmatrix}.
\]</span></p>
<p><span class="math inline">\(Z_1 = X_1\)</span>.</p>
<p><span class="math display">\[
\begin{align}
X_2 &amp;= (X_2 - \E(X_2| Z_1)Z_1) + \E(X_2|Z_1) Z_1 \\
&amp;= (X_2 + Z_1) - Z_1.
\end{align}
\]</span></p>
<p>So, set <span class="math inline">\(Z_2 = X_2 + X_1\)</span>.</p>
<p>Checking:</p>
<p><span class="math display">\[
\E(Z_1) = \E(X_1) = 0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\E(Z_1^2) = \E(X_1^2) = 1.
\]</span></p>
<p><span class="math display">\[
\E(Z_2) = \E(X_2) + \E(X_1) = 0
\]</span> and</p>
<p><span class="math display">\[
\begin{align}
\E(Z_2^2) &amp;= \E(X_1^2 + 2X_1X_2  + X_2^2)  \\
&amp;= 1 -2.1 + 2 \\
&amp;= 1.
\end{align}
\]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[
\begin{align}
\E(Z_2Z_1)  &amp;= \E((X_1 + X_2) X_1) \\
&amp;= \E(X_1^2 + X_1 X_2) \\
&amp;= 1 - 1 \\
&amp;= 0.
\end{align}
\]</span></p>
<p><span class="math display">\[
\E(X_2 | X_1) = \frac{\E(X_2 X_1)}{\E(X_1^2)}X_1 = -X_1.
\]</span></p>
<p><span class="math display">\[
\begin{align}
\E(e^{a X_2} | X_1) &amp;= \E(e^{a (Z_2 - Z_1)}| Z_1) \\
&amp;= e^{-a Z_1} \E(e^{a Z_2}| Z_1) \\
&amp;= e^{-a Z_1} \E(e^{a Z_2}) \\
&amp;= e^{-aZ_1 + a^2/2} \\
&amp;= e^{-aX_1 + a^2/2}.
\end{align}
\]</span> using the independence of <span class="math inline">\(Z_2\)</span> and <span class="math inline">\(Z_1\)</span> and the MGF of a Gaussian of mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. The conditional distribution of <span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_1\)</span> is a Gaussian with mean <span class="math inline">\(-X_1\)</span> and variance <span class="math inline">\(1\)</span>.</p>
</section>
<section id="gaussian-conditioning-3" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-conditioning-3">4.8 Gaussian Conditioning</h2>
<p>We can use the Cholesky factorisation of the covariance matrix to get a mapping from <span class="math inline">\((Z_1, Z_2, Z_3)\)</span> of IID standard Gaussians to <span class="math inline">\((X_1, X_2, X_3)\)</span>:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>sp.Matrix([[<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>],[<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>]]).cholesky()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<p><span class="math inline">\(\displaystyle \left[\begin{matrix}\sqrt{2} &amp; 0 &amp; 0\\\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{6}}{2} &amp; 0\\\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{6}}{6} &amp; \frac{2 \sqrt{3}}{3}\end{matrix}\right]\)</span></p>
</div>
</div>
<p><span class="math display">\[
\begin{align}
X_1 &amp;= \sqrt{2} Z_1, \\
X_2 &amp;= \frac{Z_1}{\sqrt{2}} + \sqrt{\frac{3}{2}} Z_2, \\
X_3  &amp;= \frac{Z_1}{\sqrt{2}} + \frac{Z_2}{\sqrt{6}} + \frac{2 Z_3}{\sqrt{3}}.
\end{align}
\]</span></p>
<p>We note that</p>
<p><span class="math display">\[
X_3 = X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}.
\]</span></p>
<p>This can be used to compute <span class="math inline">\(\E(X_3 |X_2, X_1)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\E(X_3 | X_2, X_1) &amp;= \E(X_2 - \frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}| X_2, X_1) \\
&amp;=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}|X_2, X_1) \\
&amp;=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}) \\
&amp;=  X_2 - \frac{\sqrt{3}}{2} X_1,
\end{align}
\]</span></p>
<p>where we’ve used <span class="math inline">\(\E(X_1|X_1, X_2) = X_1\)</span> and that <span class="math inline">\(Z_3\)</span> is independent of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (they are linear combinations of <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span>).</p>
<p>We can also compute <span class="math inline">\(\E(e^{aX_3} | X_2, X_1)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\E(e^{aX_3}| X_2, X_1) &amp;= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}|X_2, X_1) \\
&amp;= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}) \\
&amp;= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2a^2}{3})}.
\end{align}
\]</span> The conditional distribution is a Gaussian with mean <span class="math inline">\(X_2 - \frac{\sqrt{3}}{2}X_1\)</span> and variance <span class="math inline">\(4/3\)</span>.</p>
</section>
<section id="properties-of-conditional-expectation" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-conditional-expectation">4.9 Properties of Conditional Expectation</h2>
<ol start="2" type="1">
<li>If <span class="math inline">\(Y\)</span> is a <span class="math inline">\(\G\)</span>-measurable random variable and <span class="math inline">\(X\)</span> is another integrable random variable (with <span class="math inline">\(XY\)</span> also integrable), then</li>
</ol>
<p><span class="math display">\[
\E(XY | \G) = Y \E(X | \G).
\]</span></p>
<p>Proof:</p>
<p>Let <span class="math inline">\(W\)</span> be a bounded, <span class="math inline">\(\G\)</span>-measurable random variable.</p>
<p><span class="math display">\[
\begin{align}
\E (W Y \E( X | \G)) &amp;= \E(W XY) \\
&amp;= \E(W \E(XY | \G)).
\end{align}
\]</span></p>
<ol start="3" type="1">
<li>If <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(\G\)</span>, that is, for any events <span class="math inline">\(I = \{Y \in (a, b]\}\)</span> and <span class="math inline">\(A \in \G\)</span>,</li>
</ol>
<p><span class="math display">\[
\P(I \cap A) = \P(I) \P(A),
\]</span> then, <span class="math display">\[
\E(Y | \G) = \E(Y).
\]</span></p>
<p>Proof:</p>
<p>If <span class="math inline">\(W\)</span> is <span class="math inline">\(\G\)</span>-measurable, then <span class="math inline">\(W\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
<p><span class="math display">\[
\begin{align}
\E(W \E(Y)) &amp;= \E(Y) \E(W) &amp; (\text{linearity of expectation}) \\
&amp;= \E(WY) &amp; (\text{$W$ and $Y$ are independent}) \\
&amp;= \E(W \E(Y | \G)) &amp; (\text{by definition}).
\end{align}
\]</span></p>
<p>By uniqueness of the conditional expectation, <span class="math inline">\(\E(Y) = \E(Y | \G)\)</span>.</p>
<ol start="4" type="1">
<li>Linearity: Let <span class="math inline">\(X\)</span> be another integrable random variable on <span class="math inline">\((\Omega, \F, \P)\)</span>. Then</li>
</ol>
<p><span class="math display">\[
\E(a X + b Y| \G) = a \E(X | \G) + b \E( Y | \G),
\]</span> for any <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<p>Proof:</p>
<p><span class="math display">\[
\begin{align}
\E(W (a X + b Y)) &amp;= a \E(WX) + b \E(WY) &amp; (\text{by linearity of expectation}) \\
&amp;= a \E(W\E(X | \G)) + b \E(W \E(Y | \G)) &amp; (\text{by defn of conditional expectation})
\end{align}
\]</span></p>
<p>but</p>
<p><span class="math display">\[
\E(W\E((a X + b Y) | \G)) = \E(W(a X + b Y))
\]</span> by definition. It follows from the uniqueness of the conditional expectation that</p>
<p><span class="math display">\[
\E(W\E((a X + b Y) | \G)) = \E(W(a X + b Y)| \G)
\]</span> for <span class="math inline">\(a, b \in \mathbb{R}\)</span>.</p>
<ol start="5" type="1">
<li>Tower Property: If <span class="math inline">\(\mathcal{H} \subseteq \mathcal{G}\)</span> is a sigma-field of <span class="math inline">\(\Omega\)</span>, the</li>
</ol>
<p><span class="math display">\[
\E(Y | \mathcal{H}) = \E(\E(Y | \mathcal{G})|\mathcal{H}).
\]</span></p>
<p>Proof:</p>
<p><span class="math display">\[
\E(W \E(Y | \mathcal{H})) = \E(W Y)
\]</span></p>
<p>by definition.</p>
<p><span class="math display">\[
\E(W \E(\E(Y | \G) | \mathcal{H})) = \E(W \E(Y | \G)) = \E(WY)
\]</span> by definition. Equating the two, we see that</p>
<p><span class="math display">\[
\E(W \E(\E(Y | \G) | \mathcal{H})) = \E(W \E(Y | \mathcal{H}))
\]</span> and so</p>
<p><span class="math display">\[
\E(Y | \mathcal{H}) = \E(\E(Y | \mathcal{G})| \mathcal{H}).
\]</span></p>
</section>
<section id="square-of-brownian-motion" class="level2">
<h2 class="anchored" data-anchor-id="square-of-brownian-motion">4.10 Square of Brownian Motion</h2>
<p>Let <span class="math inline">\((B_t, t \geq 0)\)</span> be a standard Brownian motion. Then <span class="math inline">\(M_t = B_t^2 - t\)</span> is a martingale for the Brownian filtration.</p>
<p>Proof:</p>
<p>Firstly, the process <span class="math inline">\((M_t)\)</span> is adapted to the Brownian filtration because both <span class="math inline">\(B_t^2\)</span> and <span class="math inline">\(t\)</span> are measurable w.r.t the Brownian filtration at <span class="math inline">\(t\)</span>.</p>
<p>Secondly,</p>
<p><span class="math display">\[
\E(|M_t|) \leq E(B_t^2) + t = 2t &lt; \infty
\]</span> for <span class="math inline">\(t \geq 0\)</span>.</p>
<p>Lastly, we use</p>
<p><span class="math display">\[
\E(B_t - B_s | \F_s) = \E(B_t - B_s) = t - s
\]</span> since <span class="math inline">\(B_t - B_s\)</span> is independent of <span class="math inline">\(B_s\)</span>, <span class="math display">\[
\E(B_s^2 | \F_s) = B_s^2
\]</span> because <span class="math inline">\(B_s^2\)</span> is <span class="math inline">\(F_s\)</span>-measurable:</p>
<p><span class="math display">\[
\begin{align}
\E(M_t | \F_s) &amp;= \E(B_t^2 - t |\F_s) \\
&amp;= \E(B_t^2 | \F_s) - t \\
&amp;= \E((B_t - B_s + B_s)^2 | \F_s) -t \\
&amp;= \E((B_t - B_s)^2 + 2B_s(B_t - B_s) + B_s^2|\F_s) -t \\
&amp;= \E((B_t - B_s)^2) + 2 \E(B_s) \E(B_t - B_s) + B_s^2 - t \\
&amp;= (t -s) + B_s^2 -t \\
&amp;= B_s^2 -t \\
&amp;= M_s
\end{align}
\]</span> for any <span class="math inline">\(s \leq t\)</span>.</p>
</section>
<section id="geometric-poisson-process" class="level2">
<h2 class="anchored" data-anchor-id="geometric-poisson-process">4.11 Geometric Poisson Process</h2>
<p>Let <span class="math inline">\((N_t, t \geq 0)\)</span> be a Poisson proces of intensity <span class="math inline">\(\lambda\)</span>. For <span class="math inline">\(\alpha &gt; 0\)</span>, the process <span class="math inline">\((e^{\alpha N_t - \lambda t(e^{\alpha} -1)}, t \geq 0)\)</span> is a martingale for the filtration of the Poisson process <span class="math inline">\((N_t, t \geq 0)\)</span>.</p>
<p>Proof: Let <span class="math inline">\(M_t\)</span> denote the process. Clearly, <span class="math inline">\(M_t\)</span> is measurable on the filtration of the Poisson process and, using the MGF of the Poisson distribution</p>
<p><span class="math display">\[
\begin{align}
\E(|M_t|) &amp;= \E(M_t) \\
&amp;= e^{-\lambda t(e^{\alpha} -1)} \E(e^{\alpha N_t}) \\
&amp;= e^{-\lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_0 + N_0}) \\
&amp;= e^{-\lambda t(e^{\alpha} -1)} e^{\lambda t (e^{\alpha} - 1)} \E(e^{\alpha N_0}) \\
&amp;= 1 &lt; \infty.
\end{align}
\]</span></p>
<p>Using the MGF of the Poisson distribution (again):</p>
<p><span class="math display">\[
\begin{align}
\E(M_t | \F_s) &amp;= \E(e^{\alpha N_t - \lambda t(e^{\alpha} -1)} | \F_s) \\
&amp;= \E(e^{\alpha N_t} e^{- \lambda t(e^{\alpha} -1)} | \F_s) \\
&amp;= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha N_t} | \F_s) \\
&amp;= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_s + N_s)} | \F_s) \\
&amp;= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_s)} e^{\alpha N_s} | \F_s) \\
&amp;= e^{- \lambda t(e^{\alpha} -1)} e^{\alpha N_s}\E(e^{\alpha (N_t - N_s)}) \\
&amp;= e^{- \lambda t(e^{\alpha} -1)} e^{\alpha N_s}e^{\lambda (t - s)(e^{\alpha} - 1)} \\
&amp;= e^{\alpha N_s -\lambda s(e^{\alpha} - 1)} \\
&amp;= M_s
\end{align}
\]</span> for <span class="math inline">\(t \geq s\)</span>.</p>
</section>
<section id="another-brownian-martingale" class="level2">
<h2 class="anchored" data-anchor-id="another-brownian-martingale">4.12 Another Brownian Martingale</h2>
<section id="a" class="level4">
<h4 class="anchored" data-anchor-id="a">(a)</h4>
<p><span class="math inline">\(M_t = t B_{t} - B_{t}^3/3\)</span> is a martingale</p>
<p>Proof:</p>
<p><span class="math inline">\(M_t\)</span> is measurable wrt the Brownian filtration.</p>
<p>We use a standard trick of creatively adding zero to <span class="math inline">\(B_t\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\E(B_t^3 | \F_s) &amp;= \E(((B_t - B_s) + B_s)^3 | \F_s) \\
&amp;= \E((B_t - B_s)^3 + 3(B_t - B_s)^2B_s + 3(B_t - B_s) B_s^2 + B_s^3 | \F_s) \\
&amp;= \underbrace{\E((B_t -B_s)^3)}_{B_t - B_s, B_s \text{ are independent}} + \underbrace{3B_s \E((B_t - B_s)^2)}_{B_s \text{ is } \F_s-\text{measurable}} + B_s^3 \\
&amp;= 3 B_s(t -s) + B_s^3.
\end{align}
\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[
\begin{align}
M_s &amp;= sB_s - B_s^3/3\\
&amp;= \underbrace{t B_s}_{\E(B_t|\F_s) = B_s} - \E(B_t^3/3|F_s) \\
&amp;= \E(t B_t - B_t^3/3 | F_s) \\
&amp;= \E(M_t | \F_s)
\end{align}
\]</span></p>
<p>for any <span class="math inline">\(s \leq t\)</span>.</p>
</section>
<section id="b" class="level4">
<h4 class="anchored" data-anchor-id="b">(b)</h4>
<p>Let <span class="math inline">\(a &gt; 0\)</span></p>
<p><span class="math inline">\(M_t = t B_{t} - B_{t}^3/3\)</span> is a martingale; the stopped martingale <span class="math inline">\(M_{t \wedge \tau}\)</span> is also a martingale and so</p>
<p><span class="math display">\[
\E(M_{\tau \wedge t}) = \E(M_0) = 0.
\]</span></p>
<p>We can’t use Doob’s optional stopping theorem directly as <span class="math inline">\(M_{\tau\wedge t}\)</span> is not bounded. However, we can argue that</p>
<p><span class="math display">\[
\lim_{t \to \infty} \E(\tau \wedge t B_{\tau \wedge t}) = \E(\tau B_\tau)
\]</span> by the dominated convergence theorem:</p>
<p><span class="math display">\[
\left | \tau \wedge t B_{\tau \wedge t} \right| \leq \left| \tau (a \vee b) \right|
\]</span> and <span class="math inline">\(\E(| \tau |) = \E(\tau) &lt; \infty\)</span>. Similar reasoning for <span class="math inline">\(B_{\tau \wedge t}^3\)</span> gives</p>
<p><span class="math display">\[
\begin{align}
0 &amp;= \E(M_0) = E(M_{\tau\wedge t})\\
&amp;= \lim_{t\to \infty} E(M_{\tau \wedge t})\\
&amp;= \E(\tau B_{\tau} - B_{\tau}^3/3).
\end{align}
\]</span></p>
<p>We apply this to get our result:</p>
<p><span class="math display">\[
\begin{align}
\E(\tau B_{\tau}) &amp;= \E(\tau B_{\tau} - B_{\tau}^3/3 + B_{\tau}^3/3) \\
&amp;= \E(B_{\tau}^3/3) \\
&amp;=  a^3 \P(B_{\tau} = a) + (-b)^3\P(B_{\tau} = -b)  \\
&amp;=  \frac{a^3b + (-b)^3a}{3(a + b)} \\
&amp;=  \frac{ab(a^2 -b^2)}{3(a + b)} \\
&amp;= \frac{ab}{3}(a -b).
\end{align}
\]</span></p>
</section>
<section id="c" class="level4">
<h4 class="anchored" data-anchor-id="c">(c)</h4>
<p>The errata mentions that the notation chosen is unfortunate. Let’s consider</p>
<p><span class="math display">\[
M_t = e^{\lambda B_t - \lambda^2 t/2}.
\]</span></p>
<p><span class="math inline">\(M_t\)</span> is a martingale for <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<p>Proof:</p>
<p><span class="math inline">\(M_t\)</span> is adapted to the Brownian filtration.</p>
<p><span class="math inline">\(M_t\)</span> is integrable:</p>
<p><span class="math display">\[
\begin{align}
\E(|M_t|) &amp;= \E(e^{\lambda B_t - \lambda^2 t/2}) \\
&amp;= e^{-\lambda^2 t/2} \underbrace{e^{\lambda^2 t/2}}_{\text{MGF of } \lambda B_t} \\
&amp;= 1.
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\E(e^{\lambda B_t - \lambda^2 t/2}| \F_s) &amp;= e^{\lambda^2 t/2} \E(e^{\lambda (B_t - B_s) + \lambda B_s } | \F_s) \\
&amp;= e^{\lambda^2 t/2} e^{\lambda B_s} \E(e^{\lambda (B_t - B_s)}) \\
&amp;= e^{\lambda^2 t/2} e^{\lambda B_s} \underbrace{e^{\lambda^2(t-s)/2}}_{\text{MGF of } \lambda(B_t - B_s)} \\
&amp;= M_s.
\end{align}
\]</span> <span class="math display">\[\qed\]</span></p>
<p>We can use Doob’s optional stopping theorem to assert that</p>
<p><span class="math display">\[
\E(e^{\lambda B_{\tau} - \lambda^2 \tau/2}) = 1.
\]</span></p>
</section>
<section id="d" class="level4">
<h4 class="anchored" data-anchor-id="d">(d)</h4>
<p>With the notation <span class="math inline">\(X_\tau = \lambda B_{\tau} - \lambda^2 \tau/2\)</span>: <span class="math display">\[
\begin{align}
\frac{d}{d\lambda}\E(e^{X_\tau}) &amp;=  \E(e^{X_{\tau}}\frac{dX_\tau}{d\lambda}) = 0, \\
\frac{d^2}{d\lambda^2}\E(e^{X_\tau}) &amp;=  \E(e^{X_{\tau}}(\frac{d^2X_\tau}{d\lambda^2} + (\frac{dX_{\tau}}{d\lambda})^2)) = 0,\\
\frac{d^3}{d\lambda^3}\E(e^{X_\tau}) &amp;=  \E(\frac{dX_{\tau}}{d\lambda} e^{X_{\tau}}(\frac{d^2X_\tau}{d\lambda^2} + (\frac{dX_{\tau}}{d\lambda})^2) + e^{X_{\tau}}(\frac{d^3X_{\tau}}{d\lambda^3} + 2\frac{dX_{\tau}}{d\lambda}\frac{d^2X_{\tau})}{d\lambda^2}) = 0\\
\end{align}
\]</span> for <span class="math inline">\(n \geq 1\)</span>.</p>
<p>We have the following:</p>
<p><span class="math display">\[
\begin{align}
\frac{dX_\tau}{d\lambda} &amp;= B_{\tau} - \lambda \tau, \\
\frac{d^2X_\tau}{d\lambda^2} &amp;= -\tau, \\
\frac{d^3X_\tau}{d\lambda^3} &amp;= 0.
\end{align}
\]</span></p>
<p>Substituting and taking the limit as <span class="math inline">\(\lambda \to 0\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\frac{d^3}{d \lambda^3} \E(e^{X_{\tau}}) &amp;= \E(e^{X_\tau}\left ((B_\tau - \lambda \tau)(-\tau +B_{\tau}^2 -2 \lambda \tau B_{\tau} + \lambda \tau^2) -2 \tau(B_\tau - \lambda \tau) \right)) \\
&amp; \to \E(B_{\tau}^3 - 3 \tau B_{\tau}) =0.
\end{align}
\]</span></p>
<p>As we have already seen <span class="math display">\[
\begin{align}
\E(B_{\tau}^3) &amp;= (a^3b - b^3a)/(a + b) = ab(a^2 - b^2)/(a+b) = ab(a - b)
\end{align}
\]</span> so</p>
<p><span class="math display">\[
\E(\tau B_{\tau}) = \frac{1}{3}\E(B_{\tau}^3) = \frac{ab}{3}(a -b).
\]</span></p>
</section>
</section>
<section id="limit-of-geometric-brownian-motion" class="level2">
<h2 class="anchored" data-anchor-id="limit-of-geometric-brownian-motion">4.13 Limit of Geometric Brownian Motion</h2>
<p>Define <span class="math inline">\(M_t = S_0 e^{\sigma B_t + \mu t}\)</span> for fixed <span class="math inline">\(\sigma &gt; 0\)</span> andd <span class="math inline">\(\mu &lt; 0\)</span>.</p>
<p>We have <span class="math display">\[
\lim_{t \to \infty} \frac{B_t}{t} = 0
\]</span> almost surely: in particular, there is an event <span class="math inline">\(A \subseteq \Omega\)</span> with <span class="math inline">\(\P(A) =1\)</span> such that for any <span class="math inline">\(\varepsilon &gt; 0\)</span> there exists <span class="math inline">\(t(\varepsilon) &gt; 0\)</span> such that for <span class="math inline">\(t &gt; t(\varepsilon)\)</span></p>
<p><span class="math display">\[
|B_t| &lt; \varepsilon t.
\]</span></p>
<p>For <span class="math inline">\(\omega \in A\)</span>, choose <span class="math inline">\(\varepsilon &lt; - \mu/ 2\sigma\)</span>, then</p>
<p><span class="math display">\[
e^{\sigma B_t + \mu t} \leq e^{\sigma |B_t| + \mu t} &lt; e^{\frac{\mu t}{2 \sigma}}
\]</span> for <span class="math inline">\(t &gt; t(\varepsilon)\)</span> i.e.&nbsp;for <span class="math inline">\(\omega \in A\)</span>,</p>
<p><span class="math display">\[
e^{\sigma B_t + \mu t} \to 0
\]</span> as <span class="math inline">\(t \to 0\)</span>.</p>
<p>On the other hand,</p>
<p><span class="math display">\[
\begin{align}
\E(e^{\sigma B_t + \mu t}| \F_s) &amp;= e^{\mu t} \E(e^{\sigma(B_t - B_s) + \sigma B_t}| \F_s) \\
&amp;=  e^{\sigma B_s + \mu t} \E(e^{\sigma(B_t - B_s)}) \\
&amp;=  e^{\sigma B_s + \mu t} e^{\sigma^2(t-s)^2/2}. \\
\end{align}
\]</span></p>
<p>Taking the expectation of both sides we see that</p>
<p><span class="math display">\[
\E(M_t) = \E(M_s)e^{\sigma^2(t-s)^2/2}.
\]</span></p>
<p>In particular,</p>
<p><span class="math display">\[
\E(M_t) = S_0 e^{\sigma^2 t^2/2}.
\]</span></p>
<p>In <span class="math inline">\(L^1\)</span>:</p>
<p><span class="math display">\[
\| M_t \|_1 = |S_0| e^{\sigma^2 t^2/2} \to \infty
\]</span> as <span class="math inline">\(t \to \infty\)</span>.</p>
<p>In <span class="math inline">\(L^2\)</span>:</p>
<p><span class="math display">\[
\| M_t \|_2^2  = \E(M_t^2) = S_0^2 e^{2 \sigma^2 t^2} \to \infty
\]</span> as <span class="math inline">\(t \to \infty\)</span>.</p>
<p>Therefore <span class="math inline">\(M_t \to 0\)</span> almost surely but does not converge to zero in <span class="math inline">\(L^1\)</span> or <span class="math inline">\(L^2\)</span>.</p>
</section>
<section id="gamblers-ruin-at-french-roulette" class="level2">
<h2 class="anchored" data-anchor-id="gamblers-ruin-at-french-roulette">4.14 Gambler’s Ruin at French Roulette</h2>
<p>Let <span class="math inline">\((S_n, n \geq 0)\)</span> be a simple random walk with bias starting at <span class="math inline">\(S_0 = 100\)</span> with</p>
<p><span class="math display">\[
S_n = S_0 + \sum_{k=1}^n X_k,
\]</span> where <span class="math inline">\(\P(X_1 = +1) = p\)</span> and <span class="math inline">\(\P(X_1 = -1) = 1 - p =q\)</span> with <span class="math inline">\(p &lt; 1/2\)</span>.</p>
<section id="a-1" class="level4">
<h4 class="anchored" data-anchor-id="a-1">(a)</h4>
<p><span class="math display">\[
M_n = (q/p)^{S_n}
\]</span></p>
<p>is a martingale for the filtration <span class="math inline">\((\F_n, n \in \mathbb{N})\)</span> where <span class="math display">\[
\F_n = \sigma(X_m, m \leq n).
\]</span></p>
<p>Proof:</p>
<p><span class="math inline">\(M_n\)</span> is measurable wrt to <span class="math inline">\(\F_n\)</span> as it is a function of measurable random variables on <span class="math inline">\(\F_n\)</span>.</p>
<p>For <span class="math inline">\(n \in \mathbb{N}\)</span></p>
<p><span class="math display">\[
\begin{align}
\E(|M_n|) &amp;= \E(|(q/p)^{S_n}|) \\
&amp;= \E((q/p)^{S_n}) \\
&amp;= 2^{S_n} &lt; \infty.
\end{align}
\]</span></p>
<p><span class="math inline">\(M_n\)</span> has the martingale property:</p>
<p><span class="math display">\[
\begin{align}
\E(M_n |\F_m) &amp;= \E((q/p)^{S_n} | \F_m) \\
&amp;= \E((q/p)^{S_n - S_m}(q/p)^{S_m}| \F_m) \\
&amp;= (q/p)^{S_m} \underbrace{\E((q/p)^{S_n - S_m})}_{\text{increments are independent}} \\
&amp;= M_s \E((q/p)^{X_{m+1} + \cdots + X_{n}}) \\
&amp;= M_s \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \P(\sum_{i=1}^{n-m} X_i = k) \\
&amp;= M_s \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \binom{n-m}{(k + (n-m))/2}p^{(k + (n-m))/2}q^{(n-m) -(k +(n+m))/2} \\
&amp;= M_s (pq)^{(n-m)/2} \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \binom{n-m}{(k + (n-m))/2}(p/q)^{k/2} \\
&amp;= M_s (pq)^{(n-m)/2} \sum_{k=-(n-m),k+=2}^{n-m} \binom{n-m}{(k + (n-m))/2}(q/p)^{k/2} \\
&amp;= M_s (pq)^{(n-m)/2} \sum_{k=0}^{n-m} \binom{n-m}{k}(q/p)^{k -(n-m)/2} \\
&amp;= M_s p^{(n-m)} \sum_{k=0}^{n-m} \binom{n-m}{k}(q/p)^k \\
&amp;= M_s p^{(n-m)} (1 + q/p)^{n-m} \\
&amp;= M_s p^{(n-m)} (1/p)^{n-m} \\
&amp;= M_s.
\end{align}
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="b-1" class="level4">
<h4 class="anchored" data-anchor-id="b-1">(b)</h4>
<p>Define the stopping time <span class="math inline">\(\tau = \min\{n \geq 0: S_n = 200 \text{ or } S_n = 0\}\)</span>. Then</p>
<p><span class="math display">\[
\P(\tau &lt; \infty) = 1.
\]</span></p>
<p>Proof:</p>
<p>Let <span class="math display">\[
E_n = \{\sum_{i=200n}^{200(n+1)} X_i = 200\}.
\]</span></p>
<p>It is clear that if any <span class="math inline">\(E_n\)</span> occurs, then <span class="math inline">\(\tau &lt; \infty\)</span>: that is,</p>
<p><span class="math display">\[
\{ \tau &lt; \infty \} \subseteq \cap_n E_n^c.
\]</span> By independence,</p>
<p><span class="math display">\[
\P(E_n) = \P(E_0).
\]</span></p>
<p>So</p>
<p><span class="math display">\[
\P(E_0^c \cap \ldots \cap E_n^c) = (1- P(E_0))^n \to 0
\]</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<p>The events <span class="math inline">\(F_n = \cap_{i=0}^n E_n\)</span> are decreasing so we can use continuity of probability to state that</p>
<p><span class="math display">\[
P(\cap_n F_n) = 0.
\]</span></p>
<p>The result follows by noting that <span class="math inline">\(\{ \tau = \infty\} \subseteq \cap_n F_n\)</span>. <span class="math display">\[\qed\]</span>.</p>
<p>Therefore</p>
<p><span class="math display">\[
\P(\tau = \infty) \leq \P(\cap E_n^c) = 0.
\]</span></p>
</section>
<section id="c-1" class="level4">
<h4 class="anchored" data-anchor-id="c-1">(c)</h4>
<p><span class="math display">\[
\P( S_{\tau} = 200) = \frac{1-(q/p)^{100}}{1 - (q/p)^{200}}.
\]</span></p>
<p>Proof:</p>
<p><span class="math inline">\(\tau\)</span> is a stopping time. <span class="math inline">\(M_{\tau \wedge t}\)</span> is bounded so Doob’s optional stopping theorem applies:</p>
<p><span class="math display">\[
\E(M_\tau) = \E(M_0) = (q/p)^{S_0}.
\]</span></p>
<p><span class="math inline">\(S_{\tau}\)</span> is a discrete random variable taking values <span class="math inline">\(0\)</span> and <span class="math inline">\(200\)</span> so it’s easy to calculate the LHS of the above:</p>
<p><span class="math display">\[
\begin{align}
\E((q/p)^{S_\tau}) &amp;= (q/p)^0 \P(S_\tau = 0) + (q/p)^{200} \P(S_\tau = 200) \\
&amp;= (1 - P(S_\tau = 200)) + (q/p)^{200} \P(S_\tau = 200) \\
&amp;= 1 - P(S_\tau = 200)(1 + (q/p)^{200}) = (q/p)^{100}.
\end{align}
\]</span></p>
<p>Rearranging yields the result: <span class="math display">\[
P(S_{\tau} = 200) = \frac{1 - (q/p)^{100}}{1 - (q/p)^{200}}.
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="d-1" class="level4">
<h4 class="anchored" data-anchor-id="d-1">(d)</h4>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">18</span><span class="op">/</span><span class="dv">38</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>((<span class="dv">1</span> <span class="op">-</span> (q<span class="op">/</span>p)<span class="op">**</span><span class="dv">100</span>)<span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span> (q<span class="op">/</span>p)<span class="op">**</span><span class="dv">200</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>2.656069339841474e-05</code></pre>
</div>
</div>
</section>
<section id="e" class="level4">
<h4 class="anchored" data-anchor-id="e">(e)</h4>
<p>We seek a starting point <span class="math inline">\(S_0\)</span> for which the probability of hitting <span class="math inline">\(100\)</span> is the same as <span class="math inline">\(200\)</span>. I estimate this will be close to <span class="math inline">\(200\)</span>, given the numerical experiments on roulette.</p>
<p><span class="math display">\[
\begin{align}
\E((q/p)^{S_\tau}) &amp;= (q/p)^{100} \P(S_\tau = 100) + (q/p)^{200} \P(S_\tau = 200) \\
&amp;= (q/p)^{S_0}
\end{align}
\]</span> because <span class="math inline">\((q/p)^{S_n}\)</span> is a martingale.</p>
<p>If <span class="math display">\[\P(S_\tau = 100) = \P(S_\tau = 200) = 1/2,\]</span> then <span class="math display">\[
\begin{align}
(q/p)^{S_0 - 100} = 1/2(1 + (q/p)^{100})
\end{align}
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
S_0 = \frac{\log(\frac{1 + (q/p)^{100}}{2})}{\log(q/p)} + 100
\]</span></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>S_0 <span class="op">=</span> math.log((<span class="dv">1</span> <span class="op">+</span> (q<span class="op">/</span>p)<span class="op">**</span><span class="dv">100</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>math.log(q<span class="op">/</span>p) <span class="op">+</span> <span class="dv">100</span><span class="op">;</span> S_0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>193.42143861781375</code></pre>
</div>
</div>
</section>
</section>
<section id="la-martingale-classique" class="level2">
<h2 class="anchored" data-anchor-id="la-martingale-classique">4.15 La Martingale Classique</h2>
<p>Define a process <span class="math inline">\(M_n\)</span> by</p>
<p><span class="math display">\[
M_n = \sum_{k=0}^{n-1} 2^k(S_{k+1} - S_k)
\]</span> where <span class="math inline">\(S_0 = 0\)</span> and</p>
<p><span class="math display">\[
S_n = \sum_k X_k
\]</span> where <span class="math inline">\((X_k, k \geq 1)\)</span> are IID random variables that take value <span class="math inline">\(\pm 1\)</span> with probability <span class="math inline">\(1/2\)</span>. We are given that <span class="math inline">\(M_n\)</span> is a martingale.</p>
<section id="a-2" class="level4">
<h4 class="anchored" data-anchor-id="a-2">(a)</h4>
<p>Consider the stopping time <span class="math inline">\(\tau\)</span>, the first time <span class="math inline">\(m\)</span> with <span class="math inline">\(X_m = +1\)</span>. <span class="math display">\[
\E(M_{\tau}) = 1
\]</span> yet</p>
<p><span class="math display">\[
\E(M_0) = 0.
\]</span></p>
<p>Proof:</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The way the winnings/losses <span class="math inline">\(M_n\)</span> is expressed makes this question easy as the telescoping sum is then obvious. It could have been written as</p>
<p><span class="math display">\[
M_n = \sum_{k=0}^{n-1} 2^k X_{k+1}
\]</span> and the trick may not be as immediate unless you are familiar with properties of binary expansions of integers: <span class="math display">\[
2^{n} = 1 + \sum_{k=0}^{n-1} 2^{k}.
\]</span></p>
</div>
</div>
<p><span class="math display">\[
\begin{align}
\E(M_\tau) &amp;= \E(\sum_{k=0}^{m-1} 2^k(S_{k+1} - S_k)) \\
&amp;= \E(\sum_{k=0}^{m-2} 2^k((-1) - (-1)) + 1) \\
&amp;= \E(1) \\
&amp;= 1.
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\E(M_0) &amp;= 0
\end{align}
\]</span> by definition. <span class="math display">\[
\qed
\]</span></p>
</section>
<section id="b-2" class="level4">
<h4 class="anchored" data-anchor-id="b-2">(b)</h4>
<p>Optional stopping doesn’t apply here because <span class="math inline">\(M_{\tau \wedge t}\)</span> is not bounded: <span class="math inline">\(M_{\tau \wedge t}\)</span> can take arbitrarily large negative values before reaching the first <span class="math inline">\(m\)</span> such that <span class="math inline">\(X_m = +1\)</span>.</p>
</section>
<section id="c-2" class="level4">
<h4 class="anchored" data-anchor-id="c-2">(c)</h4>
<p>The weakness of this strategy is that to execute it, the player needs to have unbounded money to gamble and unbounded time to play.</p>
</section>
</section>
<section id="a-martingale-from-conditional-expectation" class="level2">
<h2 class="anchored" data-anchor-id="a-martingale-from-conditional-expectation">4.16 A Martingale From Conditional Expectation</h2>
<p>Let <span class="math inline">\(X \in L^1(\Omega, \F, \P)\)</span>, and let <span class="math inline">\((\F_t, t \geq 0)\)</span> be a filtration. Then the process defined by</p>
<p><span class="math display">\[
M_t = \E(X | \F_t)
\]</span></p>
<p>is a martingale.</p>
<p>Proof:</p>
<p>Clearly, <span class="math inline">\(M_t\)</span> is measurable on <span class="math inline">\(\F_t\)</span>. Also, <span class="math inline">\(M_t\)</span> is integrable:</p>
<p><span class="math display">\[
\E(|M_t|) = \E(|E(X | \F_t)|) \leq \E(\E(|X||\F_t)) = \E(|X|) &lt; \infty.
\]</span></p>
<p>For <span class="math inline">\(s \leq t\)</span></p>
<p><span class="math display">\[
\begin{align}
\E(M_t | \F_s) &amp;= \E(\E(X|\F_t) |F_s) \\
&amp;= \underbrace{\E(X| F_s)}_{\text{by the tower property}} \\
&amp;= M_s.
\end{align}
\]</span></p>
<p><span class="math display">\[\qed\]</span></p>
</section>
<section id="joint-distribution-of-max_t-leq-t-b_t-b_t" class="level2">
<h2 class="anchored" data-anchor-id="joint-distribution-of-max_t-leq-t-b_t-b_t">4.17 Joint Distribution of <span class="math inline">\((max_{t \leq T} B_t, B_T)\)</span></h2>
<p><span class="math display">\[
\P(max_{t \leq T} B_t &gt; m, B_T \leq a) = \P(B_T &gt; 2m -a)
\]</span> and the joint pdf between the two is</p>
<p><span class="math display">\[
f(m, a) = \frac{2(2m -a)}{T^{3/2}\sqrt{2 \pi}} e^{-\frac{(2m -a)^2}{2T}}.
\]</span></p>
<p>Proof:</p>
</section>
<section id="zeros-of-brownian-motion" class="level2">
<h2 class="anchored" data-anchor-id="zeros-of-brownian-motion">4.18 Zeros of Brownian Motion</h2>
<p>For any <span class="math inline">\(t &gt; 0\)</span>, <span class="math inline">\(\P(\max_{s\leq t} B_s &gt; 0) = \P(\min_{s \leq t} B_s &lt; 0) = 1\)</span></p>
<p>Proof:</p>
<p>Bachelier’s formula yields the following:</p>
<p><span class="math display">\[
\P(\max_{s \leq t} B_s \leq 0) = \P(|B_t| \leq 0) = 0.
\]</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why is <span class="math inline">\(P(|B_t| = 0)\)</span>?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We know that <span class="math inline">\(B_t\)</span> is continous: in fact, it is absolutely continous because it has a probability density function. By definition, a random variable <span class="math inline">\(X\)</span> is continous if</p>
<p><span class="math display">\[
\P(X = x) = 0
\]</span> for all <span class="math inline">\(x\)</span>.</p>
</div>
</div>
</div>
<p>Therefore,</p>
<p><span class="math display">\[
\P(\max_{s \leq t} B_s &gt; 0) = 1.
\]</span></p>
<p>The result follows by applying the result to <span class="math inline">\(-B_t\)</span>, which is also a Brownian motion. <span class="math display">\[\qed\]</span></p>
<p>The function <span class="math inline">\(s \mapsto B_s(\omega)\)</span> has infinitely many zeros in the interval <span class="math inline">\([0, t]\)</span>, with probability one.</p>
<p>Proof:</p>
<p>In the interval <span class="math inline">\([0, t]\)</span>, our function takes a maximum value <span class="math inline">\(M&gt; 0\)</span> and a minimum <span class="math inline">\(m&lt;0\)</span>, at <span class="math inline">\(t_M&gt;0\)</span> and <span class="math inline">\(t_m&gt;0\)</span>, respectively. Between these times, the function has a zero by continuity. There exists a <span class="math inline">\(\varepsilon&gt;0\)</span> such that <span class="math inline">\(T = \min(t_M, t_m) - \varepsilon &gt; 0\)</span>. Repeating this argument for <span class="math inline">\([0, T]\)</span> we see that <span class="math inline">\(s \mapsto B_s(\omega)\)</span> has two zeros in [0, t] and we can repeat as many times as we wish to get <span class="math inline">\(n\)</span> zeros for any <span class="math inline">\(n \in \mathbb{N}\)</span> i.e.&nbsp;there are infinitely many zeros of <span class="math inline">\(s \mapsto B_s(\omega)\)</span> in <span class="math inline">\([0, t]\)</span>. <span class="math display">\[\qed\]</span></p>
</section>
<section id="doobs-maximal-inequalities" class="level2">
<h2 class="anchored" data-anchor-id="doobs-maximal-inequalities">4.19 Doob’s Maximal Inequalities</h2>
<p>Let <span class="math inline">\((M_k, k \geq 1)\)</span> be a positive submartingale for the filtration <span class="math inline">\(( \F_k, k \in \mathbb{N})\)</span>. Then for any <span class="math inline">\(1 \leq p &lt; \infty\)</span> and <span class="math inline">\(a &gt; 0\)</span></p>
<p><span class="math display">\[
\P(\max_{k\leq n}M_k &gt; a) \leq \frac{1}{a^p} \E(M_n^p).
\]</span></p>
<p>Proof:</p>
<section id="a-3" class="level4">
<h4 class="anchored" data-anchor-id="a-3">(a)</h4>
<p>Use Jensen’s inequality to show that if <span class="math inline">\((M_k, k \geq 1)\)</span> is a submartingale, then so is <span class="math inline">\((M^p_k, k\geq 1)\)</span> for <span class="math inline">\(1 \leq p &lt; \infty\)</span>. For <span class="math inline">\(x \geq 0\)</span>,</p>
<p><span class="math display">\[
x \mapsto x^p
\]</span> is a convex function for <span class="math inline">\(1 \leq p &lt; \infty\)</span>. The process <span class="math inline">\(M_n\)</span> is positive and so by Jensen’s inequality</p>
<p><span class="math display">\[
\E(M^p_n|\F_m) \geq \E(M_n|\F_m)^p = M_m^p.
\]</span> That is, the process <span class="math inline">\((M_k^p, k\geq 1)\)</span> is submartingale.</p>
<p>If we prove the statement for <span class="math inline">\(p=1\)</span>, then</p>
<p><span class="math display">\[
\frac{1}{a^p}\E(M_n^p) \geq \frac{1}{a}\E(M_n)^p \geq \P(\max_{k \leq n}M_k &gt; a)
\]</span> for <span class="math inline">\(1 \leq p &lt; \infty\)</span>: moreover, we need only prove that <span class="math inline">\(M_n\)</span> is a submartingale for the theorem to apply.</p>
</section>
<section id="b-3" class="level4">
<h4 class="anchored" data-anchor-id="b-3">(b)</h4>
<p>Consider the events</p>
<p><span class="math display">\[
B_k = \cap_{j &lt; k} \{ \omega : M_j(\omega) \leq a\} \cap\{\omega: M_k(\omega) &gt; a\}.
\]</span></p>
<p>The <span class="math inline">\(B_k\)</span>’s are disjoint: if <span class="math inline">\(m &lt; n\)</span> and <span class="math inline">\(\omega \in B_m\)</span>, then <span class="math inline">\(M_m(\omega) &gt; a\)</span> and so <span class="math inline">\(\omega \notin B_n\)</span>.</p>
<p><span class="math display">\[
\cup_{k \leq n} B_k = \{\max_{k \leq n} M_k &gt; a\} \triangleq B.
\]</span></p>
<p>Choose <span class="math inline">\(\omega\)</span> in the LHS set. Then <span class="math inline">\(\omega\)</span> is in exactly one <span class="math inline">\(B_k\)</span> and so <span class="math inline">\(M_k(\omega) &gt; a\)</span> which means that</p>
<p><span class="math display">\[
\max_{k \leq n} M_k(\omega) &gt; a,
\]</span> that is</p>
<p><span class="math display">\[
\cup_{k \leq n} B_k \subseteq \{\max_{k \leq n} M_k &gt; a\}.
\]</span></p>
<p>To prove the reverse inclusion, take <span class="math inline">\(\omega \in \{\max_{k \leq n} M_k &gt; a\}\)</span>. Then, for some <span class="math inline">\(k \leq n\)</span>, <span class="math inline">\(M_k(\omega) &gt; a\)</span> and <span class="math inline">\(M_j(\omega) \leq a\)</span> for <span class="math inline">\(j &lt; k\)</span> and so <span class="math inline">\(\omega \in \cup_{k \leq n} B_k\)</span>.</p>
</section>
<section id="c-3" class="level4">
<h4 class="anchored" data-anchor-id="c-3">(c)</h4>
<p><span class="math display">\[
\E(M_n) \geq \E(M_n \mathbb{1}_B) \geq a \sum_{k \leq n} \P(B_k) = a\P(B).
\]</span></p>
<p>Proof:</p>
<p><span class="math display">\[
\begin{align}
\E(M_n) &amp;\underbrace{\geq \E(M_n \mathbb{1}_B)}_{\text{only true because } M_n &gt;= 0} \\
&amp;= \sum_k\E(M_n \mathbb{1}_{B_k}) \\
&amp;= \sum_k\E(\E(M_n \mathbb{1}_{B_k}| \F_k)) \\
&amp;= \sum_k\E(\underbrace{\mathbb{1}_{B_k}}_{\text{this is $\F_k$-measurable}} \E(M_n| \F_k)) \\
&amp;\geq \sum_k\E(\mathbb{1}_{B_k} \underbrace{M_k}_{\text{submartingale}})) \\
&amp;\geq a\sum_k\E( \mathbb{1}_{B_k} ) \\
&amp;= a \sum_k \P(B_k) \\
&amp;= a \P(B).
\end{align}
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="d-2" class="level4">
<h4 class="anchored" data-anchor-id="d-2">(d)</h4>
<p>For any finite set of times <span class="math inline">\(0 = t_0 &lt; t_1 &lt; \ldots &lt; t_n = t\)</span>, the inequality</p>
<p><span class="math display">\[
\P(\max_{k \leq n} M_{t_k} &gt; a) \leq \frac{1}{a^p} \E(M_n^p)
\]</span></p>
<p>because</p>
<p><span class="math display">\[
S_k = M_{t_k}
\]</span></p>
<p>defines a discrete submartingale.</p>
<p>For each <span class="math inline">\(n\)</span>, define a discretisation of <span class="math inline">\([0, t]\)</span> by</p>
<p><span class="math display">\[
t_k = \frac{kt}{2^n}
\]</span> for <span class="math inline">\(k =1, \ldots, 2^n\)</span>. Define a collection of events</p>
<p><span class="math display">\[
A_n = \{\omega : \max_{k \leq 2^n} M_{t_k} &gt; a\}.
\]</span></p>
<p>The sets are nested:</p>
<p><span class="math display">\[
A_m \subseteq A_n
\]</span> if <span class="math inline">\(m \leq n\)</span>. For each <span class="math inline">\(n\)</span>,</p>
<p><span class="math display">\[
P(A_n) \leq \frac{1}{a^p}\E(M_t^p).
\]</span></p>
<p>Using the continuity of probability</p>
<p><span class="math display">\[
\P(A) = \lim_{n\to \infty} \P(A_n)
\]</span> where <span class="math display">\[
A = \cup_{n} A_n.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\P(\max_{s \in \mathcal{D}} M_s &gt; a) \leq \frac{1}{a^p} \E(M_t^p)
\]</span> where <span class="math inline">\(\mathcal{D}\)</span> is the dense subset of <span class="math inline">\([0, t]\)</span> of real numbers of the form <span class="math display">\[
kt/2^n
\]</span> for some <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>It is clear that</p>
<p><span class="math display">\[
\{\omega: \max_{s \in \mathcal{D}} M_s(\omega) &gt; a\} \subseteq \{\omega: \max_{s \in [0, t]} M_s(\omega) &gt; a\}.
\]</span></p>
<p>The reverse inclusion follows from the fact that</p>
<p><span class="math display">\[
s \mapsto M_s(\omega)
\]</span> is continous, almost surely: <span class="math inline">\(M_s(\omega)\)</span> will attain a maximum on <span class="math inline">\([0, t]\)</span> and can be approximated to arbitrary precision with <span class="math inline">\(M_{s'}(\omega)\)</span> for <span class="math inline">\(s' \in \mathcal{D}\)</span>.</p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
</section>
<section id="an-application-of-doobs-maximal-inequalities" class="level2">
<h2 class="anchored" data-anchor-id="an-application-of-doobs-maximal-inequalities">4.20 An Application of Doob’s Maximal Inequalities</h2>
<section id="a-4" class="level4">
<h4 class="anchored" data-anchor-id="a-4">(a)</h4>
<p>If <span class="math inline">\((B_t, t \geq 0)\)</span> is a Brownian motion, then</p>
<p><span class="math display">\[
\lim_{n \to \infty} \frac{B_n}{n} = 0
\]</span> a.s. when <span class="math inline">\(n\)</span> is an integer.</p>
<p>Proof:</p>
<p>We do the usual telescoping sum trick:</p>
<p><span class="math display">\[
\begin{align}
\frac{B_n}{n} &amp;= \frac{\sum_{k=0}^{n-1} (B_{k+1} - B_k)}{n} \\
&amp;= \frac{\sum_{k=0}^{n-1} X_k}{n}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(X_k = B_{k+1} - B_k\)</span> are Brownian increments. Using the strong law of large numbers</p>
<p><span class="math display">\[
\lim_{n \to \infty} \frac{B_n}{n} = \lim_{n\to\infty} \bar{X_k} = \E(X_0) = 0.
\]</span></p>
<p><span class="math display">\[\qed\]</span></p>
</section>
<section id="b-4" class="level4">
<h4 class="anchored" data-anchor-id="b-4">(b)</h4>
<p><span class="math display">\[
\sum_{n \geq 0} \P(\max_{0 \leq s \leq 1} |B_{n+s} - B_n| &gt; \delta n) &lt; \infty
\]</span></p>
<p>for any <span class="math inline">\(\delta &gt; 0\)</span>.</p>
<p>Proof:</p>
<p>The process <span class="math inline">\(M_t = |B_{n+t} - B_n|\)</span> is a positive submartingale of the filteration <span class="math inline">\((\G_s = \F_{n+s}, s \geq 0)\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\E(M_t | \G_s) &amp;= \E(|B_{n+t} - B_n| | \F_{n+s}) \\
&amp; \geq \left| \E(B_{n + t} - B_n| \F_{n +s })\right| \\
&amp;=\left| \E(B_{n + t}|\F_{n+s}) - \E(B_n| \F_{n +s })\right | \\
&amp;=\left | \underbrace{B_{n+s}}_{\text{martingale property}} - \E(B_n| \F_{n +s })\right| \\
&amp;=\left | B_{n+s} - \underbrace{B_n}_{B_n \text{is } \F_{n+s} \text{ measurable}} \right| \\
&amp;= M_s
\end{align}
\]</span> for <span class="math inline">\(s \leq t\)</span>.</p>
<p>Doob’s maximal inequality applies to <span class="math inline">\(M_t\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\P(\max_{0 \leq s \leq 1}|B_{n + s} - B_n| &gt; \delta n) &amp;\leq \frac{1}{\delta^2 n^2}\E((B_{n+1} - B_n)^2) \\
&amp;= \frac{1}{\delta^2 n^2}\\
\end{align}
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="c-4" class="level4">
<h4 class="anchored" data-anchor-id="c-4">(c)</h4>
<p>Taking the sum <span class="math display">\[
\sum_n \P(\max_{0 \leq s \leq 1}|B_{n + s} - B_n| &gt; \delta n) = \frac{\pi^2}{6 \delta^2} &lt; \infty.
\]</span></p>
<p>As a consequence</p>
<p><span class="math display">\[
\lim_{n\to\infty} \max_{0 \leq s \leq 1} \frac{|B_{n+s} - B_n|}{n} = 0
\]</span> almost surely.</p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="d-3" class="level4">
<h4 class="anchored" data-anchor-id="d-3">(d)</h4>
<p>If <span class="math inline">\(t_n \uparrow \infty\)</span>, then</p>
<p><span class="math display">\[
\lim_{n\to \infty} \frac{B_{t_n}}{t_n} = 0.
\]</span></p>
<p>Proof:</p>
<p>Let <span class="math inline">\(k = \lfloor t_n \rfloor\)</span> and <span class="math inline">\(s = t_n - k\)</span>. Then</p>
<p><span class="math display">\[
\begin{align}
\lim_{n\to \infty} \frac{B_{t_n}}{t_n} &amp;= \lim_{n\to\infty} \frac{B_{k+s} - B_k + B_k}{t_n} \\
&amp;\leq \lim_{n\to\infty} \frac{B_{k+s} - B_k + B_k}{k} \\
&amp;\leq \lim_{n\to\infty} \max_{0 \leq s \leq 1}\frac{|B_{k+s} - B_k| + |B_k|}{k} \\
&amp; = 0.
\end{align}
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
<section id="e-1" class="level4">
<h4 class="anchored" data-anchor-id="e-1">(e)</h4>
<p>If <span class="math display">\[
X_t = t B_{1/t},
\]</span> then <span class="math display">\[
\lim_{t \to 0+} X_t = 0
\]</span> almost surely.</p>
<p>Proof:</p>
<p><span class="math display">\[
X_{\frac{1}{t}} = \frac{B_t}{t} \to 0
\]</span> as <span class="math inline">\(t \to \infty\)</span>, almost surely.</p>
<p><span class="math display">\[
\qed
\]</span></p>
</section>
</section>
<section id="an-example-of-fubinis-theorem" class="level2">
<h2 class="anchored" data-anchor-id="an-example-of-fubinis-theorem">4.21 An Example of Fubini’s Theorem</h2>
<p>Let <span class="math inline">\((X_n, n \geq 1)\)</span> be a sequence of random variables on <span class="math inline">\((\Omega, \F, \P)\)</span>. If</p>
<p><span class="math display">\[
\sum_n \E(|X_n|) &lt; \infty
\]</span> then</p>
<p><span class="math display">\[
\E \left(\sum_{n} X_n \right ) = \sum_n \E(X_n).
\]</span></p>
<p>Proof:</p>
<p>Let</p>
<p><span class="math display">\[
S_n = \sum_{k=1}^n X_k.
\]</span></p>
<p>Almost surely</p>
<p><span class="math display">\[
S_n \to \sum_{k=1}^{\infty} X_k
\]</span> as <span class="math inline">\(n \to \infty\)</span> and</p>
<p><span class="math display">\[
| S_n | =  |\sum_{k=1}^{n} X_k| \leq \sum_{k=1}^{n} |X_k| \leq \sum_{k=1}^{\infty} |X_k|.
\]</span></p>
<p>So <span class="math inline">\(S_n\)</span> is dominated by</p>
<p><span class="math display">\[
\sum_k |X_k|
\]</span> and this is integrable by our assumptions:</p>
<p><span class="math display">\[
\E(\sum_{k=1}^{\infty} |X_k|)  \leq \sum_{k=1}^{\infty} \E(|X_k|) &lt; \infty
\]</span></p>
<p>So we can invoke the dominated convergence theorem to state that</p>
<p><span class="math display">\[
\lim_{n\to \infty} \E(S_n) = \E(\lim_{n \to \infty} S_n).
\]</span> That is</p>
<p><span class="math display">\[
\sum_n \E(X_n) = \E\left(\sum_n X_n\right).
\]</span></p>
<p><span class="math display">\[
\qed
\]</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>