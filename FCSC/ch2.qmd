---
title: "A First Course in Stochastic Calculus"
subtitle: "Chapter Two Exercises"
toc: true
toc-location: body
execute:
    freeze: true
description: "Solutions to exercises and computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---

# Computer Experiments
## 2.1 The Box-Muller Method

Let $U_1 \sim U(0,1)$ and $U_2 \sim U(0,1)$. Define random variables

$$
Z_1 = \sqrt{-2\log(U_1)} \cos(2 \pi U_2)
$$
and
$$
Z_2 = \sqrt{-2\log(U_1)} \sin(2 \pi U_2).
$$

Generate $10000$ samples of $(Z_1, Z_2)$ and plot the histograms of each random variable.

```{python}
import numpy as np
import matplotlib.pyplot as plt

rg = np.random.default_rng()

N = 10000
bins=100

U1 = rg.uniform(0, 1, N)
U2 = rg.uniform(0, 1, N)

normal = rg.normal(0, 1, N)

Z1 = [np.sqrt(-2 * np.log(u[0])) * np.cos(2 * np.pi * u[1]) for u in zip(U1, U2)]
Z2 = [np.sqrt(-2 * np.log(u[0])) * np.sin(2 * np.pi * u[1]) for u in zip(U1, U2)]
```
```{python}
plt.hist(normal, bins=bins, label='normal', alpha=0.5)
plt.hist(Z1, bins=bins, label='Z1', alpha=0.5)
plt.legend(loc='upper right')
plt.plot()
```

```{python}
plt.hist(normal, bins=bins, label='normal', alpha=0.5)
plt.hist(Z2, bins=bins, label='Z2', alpha=0.5)
plt.legend(loc='upper right')
plt.plot()
```

```{python}
plt.hist2d(Z1, Z2, bins=bins, density=True)
plt.plot()
```

{{< include /_macros.qmd >}}


# Exercises

## 2.1 An Example of Uncorrelated Random Variables that are not Independent

Let $X$ be a standard Gaussian. Show that $\Cov(X^2, X) = 0$.

$$
\Cov(X^2, X) = \E(X^3) - \E(X^2)\E(X)
$$

The standard Gaussian has odd moments equal to zero so

$$
\Cov(X^2, X) = 0 - \E(X^2).0 = 0.
$$

If you don't have the knowledge at your fingertips, there's always
direct calculation:

We already know that $\E(X) = 0$ for the standard Gaussian (it has mean $0$).

Using integration by parts:
$$
\begin{align}
\E(X^3) & = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{\infty} x^3 e^{-x^2/2} dx \\ 
&=  \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{\infty} x^2 \frac{d}{dx}(-e^{-x^2/2}) dx \\ 
&= \frac{1}{\sqrt{2 \pi}} (-x^2 e^{-x^2/2} \rvert_{-\infty}^{\infty} + \int_{-\infty}^{\infty} 2 x e^{-x^2/2} dx) \\ 
& = 2 \E(X) = 0.
\end{align}
$$

## 2.2 Sum of Exponentials is Gamma

The sum of $n$ IID random variables with exponential distribution
with parameter $\lambda$ is gamma with pdf

$$
\begin{align}
f(x) = \frac{\lambda^n}{(n-1)!}x^{n-1} e^{-\lambda x} &, x \geq 0.
\end{align}
$$ {#eq-gamma-pdf}

Proof:

The pdf of the sum of two IID random variables is the convolution of the pdfs of the summands.

Therefore,
$$
\begin{align}
f(x) &= \int_{0}^{x} \lambda^2 e^{-\lambda(x - y)}e^{-\lambda y} dy \\ 
&= \lambda^2 e^{-\lambda x} \int_{0}^{x} dy \\ 
&= \lambda^2 x e^{-\lambda x}.
\end{align}
$$

So, it's at least plausible.

To prove the result, we use the MGF of the exponential random variables $X$ with parameter $\lambda$:

$$
\begin{align}
\E(e^{tX}) &= \frac{\lambda}{\lambda -t} &, t < \lambda.
\end{align}
$$

Let $X_i$ be a collection of IID exponential random variables with
parameter $\lambda$. Then $Z = \sum X_i$ satisfies

$$
\E(e^{tZ}) = \prod_{i=1}^n \E(e^{tX_i}) = \frac{\lambda^n}{(\lambda -t)^n}.
$$

Suppose that $Y$ has pdf (@eq-gamma-pdf), then for $t < \lambda$ we see that be repeated integration by parts

$$
\begin{align}
\E(e^{tY}) &= \int_0^{\infty} e^{tx} \frac{\lambda^n}{(n-1)!}x^{n-1} e^{-\lambda x} dx \\ 
&=    \int_0^{\infty} \frac{\lambda^n}{(n-1)!}x^{n-1} e^{(t -\lambda) x} dx \\ 
&= \int_0^{\infty} \frac{\lambda^n}{(n-1)!} x^{n-1} \frac{(-1)^n}{(\lambda -t)^n} \frac{d^n}{dx^n}e^{(t-\lambda)x} dx \\ 
&= \frac{\lambda^n}{(n-1)!} \frac{(-1)^n}{(\lambda -t)^n} \int_0^{\infty} x^{n-1} \frac{d^n}{dx^n}e^{(t-\lambda)x} dx \\ 
&= \frac{\lambda^n}{(n-1)!} \frac{(-1)^n}{(\lambda -t)^n} (-(n-1)\int_0^{\infty} x^{n-2} \frac{d^{n-1}}{dx^{n-1}}e^{(t-\lambda)x} dx) \\ 
&= \frac{\lambda^n}{(\lambda - t)^n} \frac{(-1)^n}{(n-1)!}(-1)^{n-1} (n-1)!(e^{(t-\lambda)x}\rvert_0^{\infty}) \\ 
&= \frac{\lambda^n}{(\lambda - t)^n} (-1)^{2n-1} (-1) \\ 
&= \frac{\lambda^n}{(\lambda -t)^n}.
\end{align}
$$

The MGF characterises the distribution of the random variable so the proof is complete.

## 2.3 Why $\sqrt{2 \pi}$?

Using polar coordinates

$$
\begin{align}
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-x^2 - y^2} dx dy &= \int_0^{2 \pi} \int_0^{\infty} r e^{-r^2} dr d\theta \\ 
&=  2 \pi \int_0^{\infty} r e^{-r^2} dr \\ 
&=  2 \pi \int_0^{\infty} \frac{-1}{2} \frac{d}{dr}(e^{-r^2}) dr \\ 
&=  - \pi e^{-r^2} \rvert_0^{\infty} \\ 
&= \pi.
\end{align}
$$

Now,
$$
(\int_{-\infty}^{\infty} e^{-x^2} dx)^2 = 
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{-x^2 - y^2} dx dy = \pi.
$$

Therefore
$$
\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}.
$$

I think the author may have meant for us to show that

$$
\int_{-\infty}^{\infty} e^{-x^2/2} dx = \sqrt{2 \pi}
$$

which follows by a change of variables $x = y/\sqrt{2}$

$$
\begin{align}
\int_{-\infty}^{\infty} e^{-x^2} dx &= \int_{-\infty}^{\infty} e^{-(y/\sqrt{2})^2} \frac{1}{\sqrt{2}} dy \\
&= \frac{1}{\sqrt{2}} \int_{-\infty}^{\infty} e^{-y^2/2} dy = \sqrt{\pi}.
\end{align}
$$






## 2.4 Box-Muller

Let $U_1 \sim U(0,1)$ and $U_2 \sim U(0,1)$. Define random variables

$$
Z_1 = \sqrt{-2 \log(U_1)} \cos(2 \pi U_2)
$$
and
$$
Z_2 = \sqrt{-2 \log(U_1)} \sin(2 \pi U_2).
$$

Show that $Z_1$ and $Z_2$ are independent standard Gaussians.

Change to polar coordinates.

Note that

$$
R = \sqrt{Z_1^2 + Z_2^2} = \sqrt{-2 \log(U_1)}
$$

and

$$
\tan(\Theta) = \frac{Z_2}{Z_1} = \tan(2 \pi U_2).
$$
so
$$
\Theta = 2 \pi U_2
$$.

The random variable $R$ has CDF 

$$
\begin{align}
F_R(r) &= P(R \leq r) \\ 
&= P(\sqrt{-2 \log(U_2)} \leq r) \\
&= P(U_2 \geq e^{-r^2/2}) \\ 
&= 1 - P(U_2 < e^{-r^2/2})
&= 1 - \begin{cases}
0 & \text{if } e^{-r^2/2} <0, \\ 
e^{-r^2/2} & \text{for } 0 \leq e^{-r^2/2} < 1, \\ 
1 & \text{if } e^{-r^2/2} \geq 1
\end{cases} \\
&= 1 - e^{-r^2/2}.
\end{align}
$$

Obviously, $\Theta \sim U(0, 2 \pi)$. Therefore, $(Z_1, Z_2)$ has the same distribution as $(X, Y)$ where $X, Y$ are IID standard Gaussians.

## 2.5 Marginally Gaussian but not Jointly Gaussian.

Let $X$ be a standard Gaussian and define

$$
Y = \begin{cases}
X & \text{if } |X| \leq 1, \\
-X & \text{otherwise.}
\end{cases}
$$

$Y$ is also a standard Gaussian.

Proof:

Let 

$$
g(x) = \begin{cases}
x & \text{if } |x| \leq 1, \\
-x & \text{otherwise.}
\end{cases}
$$

Then the MGF of $Y$ can be expressed (using LOTUS) as

$$
\begin{align}
\sqrt{2 \pi} \E(e^{tY}) &= \sqrt{2 \pi} \E(e^{tg(X)}) \\ 
&= \int_{-\infty}^{\infty} e^{tg(x)} e^{-x^2/2} dx \\ 
&= \int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\int_{-\infty}^{-1} + \int_1^{\infty}) e^{-t x} e^{-x^2/2} dx \\ 
&= \int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\int_{\infty}^{1} + \int_{-1}^{-\infty}) - e^{t x} e^{-x^2/2} dx \\ 
&= \int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\int_{1}^{\infty} + \int_{-\infty}^{-1} e^{t x} e^{-x^2/2} dx \\ 
&= \int_{-\infty}^{\infty} e^{tx} e^{-x^2/2} dx \\ 
&= \sqrt{2 \pi} \E(e^{tX}).
\end{align}
$$

Therefore, $Y$ and $X$ are indentically distributed. They are definitely
not independent as $Y$ is a function of $X$ and so we have no
right to expect that $X + Y$ is also Gaussian.

To see that $X +Y$ is not Gaussian, note that its range is in $[-2,2]$; there are lower bounds on Gaussian tails which are non-zero.

```{python}
X = rg.normal(0,1, N)
Y = [x if np.abs(x) <=1 else -x for x in X]
plt.hist(X+Y, bins=bins, label='X + Y', alpha=0.5)
plt.hist(X, bins, label='X', alpha=0.5)
plt.hist(Y, bins, label='Y', alpha=0.5)
plt.legend(loc='upper right')
plt.plot()
```

```{python}
data=[X+Y, X, Y]
ax = plt.subplot()
ax.violinplot(data, range(len(data)), vert=False)
ax.set_yticks(range(len(data)))
ax.set_yticklabels(['X+Y', 'X', 'Y'])
plt.plot()
```

## 2.7 The Covariance of a Random Vector is Always Positive Semidefinite

Let $\mathcal{C}$ be the covariance matrix of a random vector $X = (X_i)$.

We must demonstrate that

$$
\sum_{i,j} a_i a_j \mathcal{C}_{ij} \geq 0
$$ {#eq-covariance-pos-semidef}
for any $a \in \mathbb{R}^n$. 

Define
$$
Y = \sum_{i=1}^n a_i X_i
$$

and calculate the variance of $Y$ showing that it is equal to the left-hand side of (@eq-covariance-pos-semidef):

$$
\begin{align}
\Var(Y) &= \E(Y^2) - \E(Y)^2 \\ 
      &= \E(\sum_{i,j=1} a_i a_j X_i X_j ) - \sum_{i,j=1}^n a_i a_j \E(X_i)\E(X_j) \\ 
      &= \sum_{i,j=1}^n a_i a_j (\E(X_i X_j) - \E(X_i) \E(X_j)) \\ 
      &=  \sum_{i,j=1}^n a_i a_j \mathcal{C}_{ij}.
\end{align}
$$

The proof is complete by noting that $\Var(Y) \geq 0$.


::: {.callout-note}
The variance of a random variable is invariant under translation i.e.

if $X$ is a random variable and $c \in \mathbb{R}$, then

$$
\Var(X + c) = \Var(X).
$$
This follows by a simple book-keeping exercise:
$$
\begin{align}
\Var(X+c) &= \E(X^2 + 2cX + c^2) - (\E(X) + c)^2\\ 
&= \E(X^2) +2c\E(x) +c^2 - \E(X)^2  - 2x \E(X) - c^2 \\ 
&= \E(X^2) - \E(X)^2 \\ 
&= \Var(X).
\end{align}
$$

It is clear that $\Var(X) \geq 0$ when $X$ has mean zero:

$$
\Var(X) = \E(X^2) = \int_{-\infty}^{\infty} x^2 dF(x) \geq 0.
$$

If $m$ is the mean of $X$, then $X -m$ has mean zero and so using invariance of the variance:

$$
\Var(X) = \Var(X - m) \geq 0.
$$

:::

 
::: {.callout-note}

$X$ is a constant with probability 1 $\iff \Var(X) = 0$.

Proof:

If $X = c$, then
$$
\Var(X) = \E(c^2) - \E(c)^2 = c^2 - c^2 = 0.
$$

If $\Var(X) = 0$,  then $\Var(X - \E(X)) = 0$, by translation invariance of $\Var$. Let $Y = X - \E(X)$. Then

$$
\Var(Y) = \int_{-\infty}^{\infty} y^2 dF_Y(y)
$$
and so 
$$
F_Y(y) = \begin{cases}
0 & \text{for } y < 0, \\ 
1 & \text{otherwise}.
\end{cases}
$$
Therefore, $Y = 0$ with probability 1 and so $X = \E(X)$ with probability 1 i.e. $X$ is a constant.
:::

Since the LHS of (@eq-covariance-pos-semidef) is the variance of $\sum_i a_i X_i$, we see that $\mathcal{C}$ is positive _definite_
if and only if the variance of $\sum_i a_i X_i$ is non-zero for all non-zero $a$.

The covariance matrix of a random vector $X = (X_i)$ is postive-definite if and only if each linear combination of the coordinates is non-constant.

When $X_i$ are jointly Gaussian, then each linear
combination is a Gaussian random variable: the only constant a linear combination of Gaussian cvariables can sum to is zero.

Therefore, for jointly Gaussian random variables,
the covariance matrix is positive-definite if and only if the
the variables are linearly independent.



 





