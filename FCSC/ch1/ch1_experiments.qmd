---
title: "Computer Experiments"
toc: true
toc-location: body
execute:
    freeze: true
description: "Computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---


## 1.1 Distributions as histograms

```{python}
#| fig-cap: "Uniform Distribution"

import numpy as np
import matplotlib.pyplot as plt

# (a)
N = 10000
samples = np.random.default_rng().uniform(0, 1, N)

# (b)

bins = 50
plt.hist(samples, bins, label='Unform')
plt.legend(loc='upper right')
plt.show()
```


```{python}
# (c)
plt.hist(samples, bins, cumulative=True, label='CDF X ~ U(0,1)')
plt.legend(loc='upper left')
plt.show()
```

```{python}
# Redo (b) and (c) for X^2

samples_squared = [ x**2 for x in samples]


plt.hist(samples_squared, bins, label='PDF X^2')
plt.legend(loc='upper right')
plt.show()
```

```{python}
plt.hist(samples_squared, bins, cumulative=True, label='CDF X^2')
plt.legend(loc='upper left')
plt.show()
```

## 1.2 The Law of Large Numbers

```{python}
# (a)
N = 10000
samples = np.random.default_rng().exponential(1, N)
averages = [sample/(idx +1) for idx, sample in enumerate(np.cumsum(samples))]

bins = np.linspace(0, 2, 100)
plt.hist(averages, bins)
plt.show()
```

```{python}
plt.plot(averages)
plt.ylabel('average of N samples')
plt.xlabel('N')
plt.show()
```

```{python}
# (b)

def average(n):
    return np.sum(np.random.default_rng().exponential(1, n))/n

bins = np.linspace(-0, 2, 100)
plt.hist([average(100) for _ in range(0, 10000)], bins, label='average(100)', alpha=0.5)
plt.hist([average(10000) for _ in range(0, 10000)], bins, label='average(10000)', alpha=0.5)
plt.legend(loc='upper right')
plt.show()
```


Note how the averages of 10000 samples have less variance than the averages of 100 samples.

## 1.3 Central Limit Theorem
```{python}
def Y(N):
    sum = np.sum(np.random.default_rng().exponential(1, N))
    return (sum - N) /np.sqrt(N)

samples = 10000

bins = np.linspace(-3, 3, 50)


plt.hist([ Y(100) for _ in range(0, samples)], bins, label='Y(100)', alpha=0.5)
plt.hist(np.random.default_rng().normal(0, 1, samples), bins, label='N(0,1)', alpha=0.5)
plt.legend(loc='upper right')
plt.show()
```
## 1.4 Sampling Cauchy Random Variables

```{python}
def invF(y):
    return np.tan((y - 0.5)*np.pi)

N = 10000
samples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]


bins = np.linspace(-10, 10, 100)
plt.hist(samples, bins, alpha=0.5, label='Cauchy')
plt.hist(np.random.default_rng().normal(0, 1, N), bins, alpha=0.5, label='normal')
plt.legend(loc='upper right')
plt.show()
```

```{python}
for i in range(0,4):
    samples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]
    plt.plot([ s/(idx + 1) for idx, s in enumerate(np.cumsum(samples)) ], label=f'average {i}')

plt.legend(loc='upper right')
plt.xlabel('N')
plt.show()
```

```{python}
def cauchy_empirical_mean(N):
    return np.sum([ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ])/N


bins = np.linspace(-40, 40, 1000)
plt.hist([cauchy_empirical_mean(10) for _ in range(0, 10000)], bins, label='N=10', alpha=0.5)
plt.hist([cauchy_empirical_mean(100) for _ in range(0, 10000)], bins, label='N=100', alpha=0.5)
plt.legend(loc='upper right')
plt.show()
```

Note how the empirical means with 10 samples and 100 samples appear to be identically distributed. The Cauchy distribution has no defined mean (even though it is symmetrical about 0) so the Central Limit Theorem does not apply.

If we have two iid variables $X$ and $Y$ with Cauchy distribution i.e. with pdf

$$
f(x) = \frac{1}{\pi} \frac{1}{1 + x^2} dx.
$$

We can get the distribution of $\frac{1}{2}(X + Y)$ by considering the characteristic function of the distribution, $e^{-|t|}$.

The characteristic function of $\frac{1}{2}(X + Y)$ is

$$
E(e^{it(X + Y)/2}) = E(e^{itX/2}) E(e^{itY/2}) = e^{-2|t/2|} = e^{-|t|}.
$$

This tells us that $\frac{1}{2}(X + Y)$ has the same distribution as $X$ and $Y$. So, when we calculate empirical means of Cauchy distribution independent variables, the result does not converge to a constant plus a narrow Gaussian error: instead, we get a random variable with the same distribution the samples, regardless of how many samples we take!

