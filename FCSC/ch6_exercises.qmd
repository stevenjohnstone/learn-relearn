---
title: "Chapter 6 Exercises"
toc: true
toc-location: body
execute:
    freeze: true
description: "Solutions to exercises and computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---

{{< include /_macros.qmd >}}

## 6.1 Rotational Symmetry of 2D Brownian Motion


Let $B_t = (B_t^{(1)}, B_t^{(2)})$ be a two-dimensional Brownian motion.
Let

$$
M = \begin{bmatrix}
\cos{\theta} & \sin{\theta} \\
-\sin{\theta} & \cos{\theta}.
\end{bmatrix}
$$

Then $W_t = M B_t$ is also a two-dimensional Brownian motion.

Proof:
By matrix multiplication
$$
M B_t = (\cos{\theta} B_t^{(1)} + \sin{\theta} B_t^{(2)}, -\sin{\theta} B_t^{(1)} + \cos{\theta} B_t^{(2)}).
$$

Clearly, $M B_t$ is a linear combination of Gaussian vectors because $(B_t, t \geq 0)$ is Gaussian. Therefore, if the covariance of the components is zero, the components are independent:


$$
\begin{align}
\E((\cos{\theta} B_s^{(1)} + \sin{\theta} B_s^{(2)})(-\sin{\theta} B_t^{(1)} + \cos{\theta} B_t^{(2)})) \\
= \E(\cos{\theta}\sin{\theta} B_s^{(2)} B_t^{(2)} - \cos{\theta}\sin{\theta} B_s^{(1)}B_t^{(1)} ) \\
= \cos{\theta}\sin{\theta}(t\wedge s -t \wedge s) \\
= 0.
\end{align}
$$

::: {.callout-note}
$$
\E(B_s^{(1)}B_t^{(2)}) = \E(B_s^{(1)}) \E(B_t^{(2)}) = 0.
$$
:::

Using the independence of the two Brownian processes and the variance properties of Brownian motions we see that
$$
\begin{align}
\E(W_t^{(1)} W_s^{(1)}) &= \E(\cos^2\theta B_t^{(1)} B_s^{(1)} + \cos{\theta}\sin{\theta} (B_t^{(1)}B_s^{(2)} + B_t^{(2)} B_s^{(1)}) + \sin^2\theta B_t^{(2)} B_s^{(2)}) \\
&= \cos^2\theta t \wedge s + \sin^2\theta t \wedge s \\
&= t \wedge s.
\end{align}
$$

Similarly,
$$
\E(W_t^{(2)} W_s^{(2)}) = t \wedge s.
$$

This characterises Brownian motions and the proof is complete.
$$\qed$$

## 6.2 Orthogonal Symmetry of Brownian Motion

Let $B_t = (B_t^{(i)}, i \leq d)$ be a $d$-dimensional Brownian motion. Let $M$ be a $d \times d$ orthogonal matrix ($M^{-1} = M^T$). The the process $W_t = M B_t$ is a $d$-dimensional Brownian motion.

Proof:

$W_t$ is a Gaussian process because it is a linear transformation of $B_t$ so it is a Gaussian process. It
remains to be shown that the components of $W_t$ are independent Brownian motions. Calculating the covariances,
we see that the components are independent and the each individual component is Brownian:


$$
\begin{align}
\E(W_t^{(i)} W_s^{(j)}) & = \E(\sum_{k} M_{ik} B_t^{(k)} \sum_{l} M_{jl} B_s^{(l)}) \\
&= \E(\sum_{k} M_{ik} B_t^{(k)} M_{jk} B_s^{(k)}) \\
&= \E(\sum_k M_{ik} M^T_{kj} B_t^{(k)} B_s^{(k)}) \\
&= \sum_k (M_{ik}M^T_{kj} t \wedge s ) \\
&= t \wedge s \sum_k M_{ik} M^{-1}_{kj} \\
&= (t \wedge s) \delta_{ij}.
\end{align}
$$

$$\qed$$

## 6.3 Races Between Brownian Motions

Let $B_t = (B_{1, t}, B_{2,t})$ be a two-dimensional Brownian motion.

The probability that $B_{1, t}$ reaches $1$ before $B_{2, t}$ is $1/2$. What is the probability that $B_{1, t}$ reaches $2$ before $B_{2, t}$ reaches $1$?

Let 
$$
X = \min\{t \geq 0: B_{1, t} \geq 2\}
$$
and
$$
Y = \min\{t \geq 0: B_{2, t} \geq 1 \}.
$$
We seek
$$
\P(X < Y).
$$

$X$ and $Y$ are independent with probability density functions

$$
p_X(x) = \frac{2}{\sqrt{2 \pi}} \frac{e^{-2/x}}{x^{3/2}}
$$
and
$$
p_Y(x) = \frac{1}{\sqrt{2 \pi}} \frac{e^{-1/2y}}{y^{3/2}}.
$$

Therefore,

$$
\P(X < Y) = \int_0^{\infty} \frac{2}{\sqrt{2 \pi}} \frac{e^{-2/x}}{x^{3/2}} \int_x^{\infty} \frac{1}{\sqrt{2 \pi}} \frac{e^{-1/2y}}{y^{3/2}} dy\,dx.
$$

Notice that

$$
\begin{align}
\frac{1}{\sqrt{2 \pi}}\frac{e^{-1/2y}}{y^{3/2}} &= \frac{d}{dy} \int_{1/\sqrt{y}}^{\infty} \frac{2}{\sqrt{2 \pi}} e^{-u^2/2} du \\
&= \frac{d}{dy}\left(-\erf(\frac{1}{\sqrt{2y}})\right). \\
\end{align}
$$

It follows from this and a change of variables that
$$
\begin{align}
\P(X < Y) &= \frac{2}{\sqrt{2 \pi}} \int_0^{\infty} \frac{e^{-2x}}{x^{3/2}} \erf(\frac{1}{\sqrt{2x}}) dx \\
&= \frac{2}{\sqrt{2 \pi}} \int_0^{\infty} e^{-4u^2} \erf(u) 2 \sqrt{2} \underbrace{du}_{u= 1/\sqrt{2x}} \\
&=\frac{4}{\sqrt{\pi}} \int_0^{\infty} e^{-4u^2} \erf(u) du.
\end{align}
$$

Using 4.3.2 from [this table of integrals](https://nvlpubs.nist.gov/nistpubs/jres/73b/jresv73bn1p1_a1b.pdf)

$$
\begin{align}
\P(X < Y) &= \frac{4}{\sqrt{\pi}}\left(\frac{\sqrt{\pi}}{4} - \frac{1}{2 \pi} \tan^{-1}(2)\right) \\
&= 1 - \frac{2}{\pi} \tan^{-1}(2) \\
&= \frac{2}{\pi} \tan^{-1}(\frac{1}{2}) \\
&\approx 0.3.
\end{align}
$$


This can be checked by simulation:

```{python}
import numpy as np

def race():
  delta = 0.1
  b_1, b_2 = 0, 0
  while True:
    if b_1 >= 1:
      return 1
    if b_2 >= 2:
      return 2
    b_1 += np.random.default_rng().normal(0, np.sqrt(delta))
    b_2 += np.random.default_rng().normal(0, np.sqrt(delta))

total_wins = 0
trials = 1000
for _ in range(trials):
  winner = race()
  if winner == 2:
    total_wins += 1

total_wins/trials
```

In fact, a solution to a more general problem for constants $a, b \in \mathbb{R}\setminus 0$ where
$$
X = \min\{t \geq 0: B_{1, t} \geq a\}
$$
and
$$
Y = \min\{t \geq 0: B_{2, t} \geq b \}
$$
is available by the same method:

$$
\P(X < Y) = \frac{2}{\pi} \tan^{-1}\left(\frac{b}{a}\right).
$$

An alternative approach is to consider the equivalent complex Brownian motion
$$
B_t = B_{1, t} + i B_{2, t}
$$
and apply the conformal mapping
$$
w(z) = (z - c)^2
$$
where $c = a + i b$ to recast the problem as finding the probability that
a complex Brownian motion starting at $w(0) = c^2$ crosses the negative $x$-axis before it
crosses the positive $x$-axis.

The function
$$
f(z) = \frac{1}{\pi}\tan^{-1}(y/x) = \frac{1}{\pi}\arg(z)
$$
is harmonic on the half-plane containing $c^2$ and is equal to $1$ on the negative $x$-axis and $0$ on the positive $x$-axis.

::: {.callout-note}
$f$ is harmonic because

$$
f(z) = \frac{\theta}{\pi}
$$
when $z = re^{i \theta}$ and writing
the Laplacian in polar coordinates

$$
\begin{align}
\Delta f &= \partial^2_r f + \frac{1}{r}\partial_r + \frac{1}{r^2} \partial^2_{\theta} f \\
&= \frac{1}{r^2} \partial^2_{\theta} \theta \\
&= 0.
\end{align}
$$

On the negative real axis, $f(z) = \pi/\pi = 1$ and on the positive real axis, $f(z) = 0$.
:::

Therefore, if $\tau$ is the hitting time for crossing the $x$-axis and $B_{\tau}$ is the Brownian process starting at $c^2$ at this
time, then
$$
\begin{align}
f(c^2) &= \frac{2}{\pi} \tan^{-1}(b/a) \\
&= \E(f(B_{\tau})) \\
&= \P(\arg(B_{\tau}) = \pi) 1 + \P(\arg(B_{\tau}) = 0) 0 \\
&= \P(\arg(B_{\tau}) = \pi) \\
&= \P(\text{crosses the negative $x$-axis before the positive}).
\end{align}
$$
Under the conformal map $w$, the line $x = b$ is mapped to the negative $x$-axis so
this is the probability we seek.



## 6.4 Drill

$$
X_t = (B^{(1)}_t)^2 + (B^{(2)}_t)^2
$$
$$
Y_t = \exp(B^{(1)}_t)\cos{B^{(2)}_t}.
$$


Apply Itô's formula to

$$
f(x, y) = x^2 + y^2
$$
to get
$$
\begin{align}
f(B^{(1)}_s, B^{(2)}_s) &= (B^{(1)}_t)^2 + (B^{(2)}_t)^2 \\
&= \int_0^t \partial_1 f(s, B^{(1)}_s, B^{(2)}_s) dB_s^{(1)} \\
&+ \int_0^t \partial_2 f(s, B^{(1)}_s, B^{(2)}_s) dB_s^{(2)} \\
&+ \frac{1}{2}\int_0^t (\partial_0 f(s, B^{(1)}_s, B^{(2)}_s) + \Delta f(s, B^{(1)}_s, B^{(2)}_s) ) ds \\
&= \int_0^t 2 B_s^{(1)} dB_s^{(1)} + \int_0^t 2 B_s^{(2)} dB_s^{(2)} + 2t.
\end{align}
$$


Apply Itô's formula to
$$
g(x, y) = \exp(x) \cos{y}
$$
to get

$$
\begin{align}
g(B_t^{(1)}, B_t^{(2)}) -g(B_0^{(1)}, B_0^{(2)}) &= \exp(B_t^{(1)})\cos{B_t^{(2)}} - 1 \\
&= \sum_{i=1}^2 \int_0^t \partial_i g( B_s^{(1)}, B_s^{(2)})d B_s^{(i)} + \int_0^t (\partial_0 g(B_s^{(1)}, B_s^{(2)})) + \frac{1}{2} \Delta g(B_s^{(1)}, B_s^{(2)})) ds \\
&= \int_0^t \exp(B_s^{(1)}) \cos{B_s^{(2)}} d B_s^{(1)} - \int_0^t \exp(B_s^{(1)})\sin{B_s^{(2)}} d B_s^{(2)}.
\end{align}
$$

## 6.5 Cross-Variation of $B_t^{(1)}$ and $B_t^{(2)}$

Let $(t_j, j \leq n)$ be a sequence of partitions of $[0, t]$ such that

$$
\max_{j} |t_{j+1} -t_j| \to 0
$$
as $n \to \infty$.

Then

$$
\lim_{n\to \infty} \sum_{j=0}^n (B_{t_{j+1}}^{(1)} - B_{t_j}^{(1)})(B_{t_{j+1}}^{(2)} - B_{t_j}^{(2)}) = 0
$$
in $L^2$.

Proof:

For notational simplicity, define

$$
X_{1,j} = B_{t_{j+1}}^{(1)} - B_{t_j}^{(1)}
$$
and

$$
X_{2,j} = B_{t_{j+1}}^{(2)} - B_{t_j}^{(2)}.
$$
Then the second moment of the sum can be expressed
$$
\begin{align}
\E(\sum_{j=0}^n\sum_{k=0}^{n} X_{1,j}X_{2,j} X_{1, k} X_{2,k}).
\end{align}
$$
For $j = k$,

$$
\begin{align}
\E(X_{1, j} X_{2, j} X_{1, k} X_{2, k}) &= \E(X_{1,j}^2 X_{2, j}^2) \\
&= \underbrace{\E(X_{1,j}^2) \E(X_{2,j}^2)}_{\text{independence}} \\
&= (t_{j+1} - t_j)^2.
\end{align}
$$
For $j \neq k$,

$$
\begin{align}
\E(X_{1, j} X_{2, j} X_{1, k} X_{2, k}) &= \E(X_{1, j})\E(X_{2, j}) \E(X_{1, k}) \E(X_{2, k})
&= 0
\end{align}
$$
due to independence of the increments of each Brownian motion and the independence of the Brownian motions.

Therefore, the limit of the second moment of the sum is
$$
\begin{align}
\sum_{j=0}^n (t_{j+1} - t_j)^2 &\leq \max_{j} |t_{j+1} -t_j| \sum_{j=0}^n |t_{j+1} -t_j| \\
&= t \max_{j} |t_{j+1} -t_j| \\
& \to 0.
\end{align}
$$

$$\qed$$

## 6.6 A Function of $B_t$

Consider a two-dimensional Brownian motion $B_t = (B_t^{(1)}, B_t^{(2)})$ and a process

$$
Z_t = B_t^{(1)} B_t ^{(2)}.
$$

$Z_t$ can be written as a sum of Itô integrals and a Riemann integral using Itô's formula.

Define
$$
f(x, y) = xy
$$
so that
$$
\partial_0 f(x, y) = 0,
$$
$$
\partial_1 f(x, y) = y,
$$
$$
\partial_2 f(x, y) = x,
$$
and
$$
\Delta f(x, y) = 0.
$$.

It follows that

$$
B_t^{(1)}B_t^{(2)} = \int_0^t B_t^{(2)} dB_s^{(1)} + \int_0^t B_t^{(1)} dB_s^{(2)}.
$$
Note that the Riemann integral part of the expression is zero because $f$ is harmonic.

Since, trivially,
$$
\partial_0 f(x,y) = -\frac{1}{2} \Delta f(x, y),
$$

$Z_t$ is a martingale.

## 6.7 Another Function of $B_t$

Let

$$
f(x, y) = x^3 - 3 xy
$$
and consider the process
$$
Z_t = f(B_t^{(1)}, B_t^{(2)}).
$$

We have

$$
\partial_0 f(x, y) = 0,
$$

$$
\partial_1 f(x, y) = 3x^2 - 3 y,
$$

$$
\partial_2 f(x, y) = 3 x,
$$
and
$$
\Delta f(x, y) = 6 x.
$$

It follows from Itô's formula that

$$
\begin{align}
Z_t &= (B_t^{(1)})^3 - 2B_t^{(1)}B_t^{(2)} \\
&= \int_0^t 3(B_s^{(1)})^2 - 3 B_s d B_s^{(1)} + \int_0^t 3 B_s^{(1)} dB_s^{(2)} + \int_0^t 3 B_s^{(1)} ds.
\end{align}
$$

$Z_t$ is not a martingale;

$$
Z_t - \int_0^t 3 B_s^{(1)} ds
$$
is a martingale.

## 6.8 Harmonic Functions

$$
h(x) = \begin{cases}
\log{\|x\|} & d=2\\
\|x\|^{2 -d} & d \geq 3
\end{cases} x \in \mathbb{R}^d.
$$

$h$ is harmonic.

Proof:

For $d=2$,

$$
\begin{align}
\Delta \log(\|x\|) & = \Delta \log(\sqrt{x_1^2 + x_2^2}) \\
&= \partial_1 (\frac{x_1}{(x_1^2 + x_2^2)}) + \partial_2 (\frac{x_2}{x_1^2 + x_2^2}) \\
&= \frac{1}{x_1^2 + x_2^2} - \frac{2 x_1^2}{(x_1^2 + x_2)^2} +  \frac{1}{x_1^2 + x_2^2} - \frac{2 x_2^2}{(x_1^2 + x_2)^2} \\
&= 0.
\end{align}
$$

For $d \geq 3$,
$$
\begin{align}
\Delta \|x\|^{2 -d} &= \nabla.(\nabla \|x\|^{2-d}) \\
&= \nabla.((2-d)\|x\|^{-d}x) \\
&= \sum_{i=1}^d  ((2 -d)(-d/2) \|x\|^{-(d+2)} 2x^2_i + (2 -d) \|x\|^{-d}) \\
&= (2-d) \|x\|^{-d} \sum_{i=1}^d (1 - \frac{d x_i^2}{\|x\|^2} ) \\
&= (2 -d) \|x\|^{-d} (d - \frac{d}{\|x\|^2} \sum_{i=1}^d x_i^2 ) \\
&= (2 -d) \|x\|^{-d} (d - \frac{d}{\|x\|^2} \|x \|^2 ) \\
&= 0.
\end{align}
$$

$$\qed$$

For $a, b \in \mathbb{R}$, $a h(x) + b$ is harmonic.

Proof:

$$
\Delta (a h(x) + b) = a \Delta h(x) = 0.
$$

$$\qed$$

## 6.9 Waiting Time for $d > 2$

Let $B_t$ be a $d$-dimensional Brownian motion.
Define a process

$$
M_t = \sum_{i=1}^d (B_t^{(i)})^2 - dt
$$
for $t \geq 0$.

Let $f(t, x) = \|x\|^2 -dt$ so that

$$
M_t = f(t, B_t).
$$

We have

$$
\partial_0 f(t, x) = -d,
$$
$$
\partial_i f(t, x) = 2 x_i,
$$
and
$$
\Delta f(t, x) = 2d.
$$
Applying Itô's formula, we see that
$$
\begin{align}
M_t &= \sum_{i=1}^d \int_0^t 2 B_s^{(i)} d B_s^{(i)}
\end{align}
$$
so $M_t$ is a martingale.

Let
$$
\tau = \min \{t \geq 0: \|B_t\| \geq 1 \}.
$$

The stopped process $M_{\tau \wedge t}$ is a martingale and

$$
\E_x(M_{\tau \wedge t}) = M_0 = \|x\|^2.
$$
The LHS is
$$
\begin{align}
\E_x(M_{\tau \wedge t}) &= \E_x(\sum_{i=1}^d (B_{\tau \wedge t}^{(i)})^2) - d E_x( \tau \wedge t).
\end{align}
$$

Taking the limit as $t \to \infty$,

$$
\begin{align}
\|x\|^2 &= \underbrace{\E_x(\sum_{i=1}^d (B_\tau^{(i)})^2)}_{\text{bounded convergence thm}} - \underbrace{d \E_x(\tau)}_{\text{monotone convergence thm}} \\
&= 1 - d \E_x(\tau).
\end{align}
$$
Therefore,

$$
\E_x(\tau) = \frac{1}{d} (1 - \|x\|^2).
$$

::: {.callout-note}
It makes intuitive sense that the expected time to exit the unit ball for a Brownian motion starting
at, say, the origin is $1/d$. As $d$ increases the expected time decreases as we only need one of the
component, independent Brownian motions to leave the interval $(-1, 1)$. The more dimensions we have, the more
possibilities there are for an exit event.
:::

## 6.10 The Heat Equation

$$
f(t, x) = \E_x(g(B_t))
$$
where $B_t$ is a Brownian motion starting at $x$. This can be rewritten as
$$
f(t, x) = \E(g(x + B_t))
$$
where $B_t$ is a Brownian motion starting at $0$.

As an integral
$$
f(t, x) = \int g(x +y) \frac{e^{-\frac{\|y\|^2}{2t}}}{(2\pi t)^{d/2}} dy.
$$

Taking the derivative inside the integral
$$
\begin{align}
\partial_0 f(t, x) &= \int g(x + y) ((\|y\|^2/(2t^2))(2\pi t)^{d/2} - (d/2)(2 \pi t)^{d/2 -1} 2 \pi)e^{\frac{-\|y\|^2}{2t}})/(2 \pi t)^d dy \\
&= \int g(x + y) (\|y\|^2/(2t^2) - d/(2t)) \frac{e^{\frac{-\|y\|^2}{2t}}}{(2 \pi t)^d} dy \\
&= \E(g(x+ B_t)(\|B_t\|^2/(2t^2) - d/(2t))).
\end{align}
$$

Taking the Laplacian inside the integral
$$
\begin{align}
\Delta f(t, x) &= \int \Delta g(x + y) \frac{e^{\frac{-\|y\|^2}{2t}}}{(2 \pi t)^{d/2}} dy \\
&= \sum_{i=1}^{d} \E(\partial^2_i g(x + B_t)) \\
&= \sum_{i=1}^{d} \E(((B_t^{(i)})^2/t^2 - 1/t)g(x + B_t)) \\
&= \E(g(x + y)(\|B_t\|^2/t^2 - d/t)) \\
&= 2 \partial_0 f(t, x).
\end{align}
$$



::: {.callout-note}
Gaussian integration by parts
$$
\E(F'(Z)) = \E(Z F(Z))
$$
for a standard Gaussian $Z$. For a Brownian motion,

$$
G(B_t) = G(\sqrt{t} Z) = F(Z).
$$

$$
F'(Z) = G'(\sqrt{t} Z) \sqrt{t} = G'(B_t) \sqrt{t}.
$$

Therefore,
$$
\E(F'(Z)) = \E(G'(B_t) \sqrt{t}) = \E(Z F(Z)) = \E(\frac{1}{\sqrt{t}} B_t G(B_t)).
$$
That is
$$
t \E(G'(B_t)) = \E(B_t G(B_t)).
$$

Define

$$
H(x) = x G(x)
$$
so that
$$
H'(x) = xG'(x) + G(x).
$$
Then
$$
\begin{align}
\E(H'(B_t)) &= \E(\frac{1}{t} B_t H(B_t)) \\
&= \E(\frac{1}{t}B_t(\frac{1}{t} B_t G(B_t))) \\
&= \E(\frac{B_t^2}{t^2} G(B_t)).
\end{align}
$$
Therefore, 
$$
\begin{align}
\E(G''(B_t)) &= \E(\frac{B_t^2}{t^2} G(B_t) - \frac{1}{t}G(B_t)) \\
&= \E((\frac{B_t^2}{t^2} - \frac{1}{t}) G(B_t)).
\end{align}
$$

Similar relations hold when $G: \mathbb{R}^d \to \mathbb{R}$ takes as argument a $d$-dimensional Brownian motion:

$$
t \E(\partial_i G(B_t)) = \E(B^{(i)}_t G(B_t)),
$$
and
$$
\E(\partial_i^2 G(B_t) = \E((\frac{(B^{(i)}_t)^2}{t^2} - \frac{1}{t}) G(B_t)).
$$

:::

::: {.callout-note}
# Taking Derivatives Inside Expectations

From Kolmogorov, Foundations of Probability Theory, p44:

If the mathematical expectation $\E(x(t))$ is finite for any $t$, and $x(t)$ is always differentiable
for any $t$, while the derivative $x'(t)$ of $x(t)$ with respect to $t$ is always less in absolute value
than some constant $M$, then

$$
\frac{d}{dt} \E(x(t)) = E(x'(t)).
$$
:::

