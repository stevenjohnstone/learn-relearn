---
title: "Chapter 4 Exercises"
toc: true
toc-location: body
execute:
    freeze: true
description: "Solutions to exercises and computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---

{{< include /_macros.qmd >}}

## 4.1 Conditional Expectation of Continuous Random Variables

Let $(X, Y)$ be two random variables with joint density $f(x, y)$ on $\mathbb{R}^2$. Suppose that

$$
\int_{\mathbb{R}} f(x, y)\, dx > 0
$$
for every $y \in \mathbb{R}$.

Then $\E(Y | X) = h(X)$ where

$$
h(x) = \frac{\int_{\mathbb{R}} y f(x, y) dy}{\int_{\mathbb{R}} f(x, y) dy}.
$$

Proof:

The conditional expectation $E(Y | X)$ is the function $\eta : \mathbb{R} \to \mathbb{R}$ satisifying

$$
\E(g(X) Y) = \E(g(X)\eta(X))
$$ {#eq-conditional-orthogonal}

for any bounded random variable of the form $g(X)$ for some function $g$. That is,

$$
Y - \eta
$$
is othorgonal to $g(X)$.

We can show that $h(X) = \E(Y | X)$ by showing that it satisfies (@eq-conditional-orthogonal) and invoking the uniqueness of such a function.

Let $g$ be any function such that $g(X)$ is bounded and measurable. Using LOTUS

$$
\begin{align}
\E(g(X) Y) &= \int \int g(x) y f(x, y)\, dx\, dy \\
&= \int g(x) \left (\int y f(x, y) \, dy\,\right) dx \\
&= \int g(x) \left (\frac{\int y f(x, y) \, dy}{\int f(x, y)\, dy}\right )\left(\int f(x, y)\, dy\right) \, dx \\
&= \int \int g(x) h(x) f(x, y) \,dx\,dy \\
&= \int g(x) h(x) f_X(x) \,dx \\
&= \E(g(X) h(X)).
\end{align}
$$

Therefore $h= \E(Y|X)$. In particular, setting $g = 1$, we see that

$$
\E(\E(Y|X)) = \E(Y).
$$

## 4.2 Exercises on Sigma-Fields

(a) Let $A, B$ be two proper subset of $\Omega$ such that $A \cap B \neq \emptyset$.

Partition $\Omega$ into $4$ disjoint elements of $\sigma$:

$$
\Omega = (A\setminus B) \cup (B \setminus A) \cup (A \cap B) \cup (A \cup B)^c.
$$

To ease notation, define

$$
\begin{align}
S_0 &= A \setminus B, \\
S_1 &= B \setminus A, \\
S_2 &= A \cap B \\
S_3 &= (A \cup B)^c.
\end{align}
$$

Each element of $\sigma$ can be expressed as a union of at most $4$ of these sets: 
the number of elements of $\sigma$ is $2^4 = 16$. 

Enumerating these:

$$
\begin{align}
0000 &\to \emptyset, \\
0001 &\to A \setminus B, \\
0010 &\to B \setminus A, \\
0011 &\to (A \setminus B) \cup (B \setminus A) = (A \cup B) \setminus (A\cap B), \\
0100 &\to A \cap B, \\
0101 &\to (A \cap B) \cup (A \setminus B) = A, \\
0110 &\to (A \cap B) \cup (B \setminus A) = B, \\
0111 &\to (A \cap B) \cup (B \setminus B) \cup (A \setminus B) = A \cup B, \\
1000 &\to (A \cup B)^c, \\
1001 &\to (A \cup B)^c \cup (A \setminus B) = B^c,\\
1010 &\to (A \cup B)^c \cup (B \setminus A) = A^c, \\
1011 &\to (A \cup B)^c \cup (B \setminus A) \cup (A \setminus B) = (A \cap B)^c,\\
1100 &\to (A \cup B)^c \cup (A \cap B) = (A \setminus B)^c \cap (B \setminus A)^c, \\
1101 &\to (A \cup B)^c \cup (A \cap B) \cup (A \setminus B) = (B \setminus A)^c, \\
1110 &\to (A \cup B)^c \cup (A \cap B) \cup (B \setminus A) = (A \setminus B)^c, \\
1111 &\to \Omega.
\end{align}
$$

If $A \cap B = \emptyset$, then $\Omega$ can be paritioned into the union of
$A$, $B$, $(A \cup B)^c$: there are $2^3 = 8$ events in $\sigma$. Alternatively, we can just go through the list of $16$ above and cross off those which end up being empty or duplicate.

(b) Show that the singleton $\{b\} \in \mathcal{B}(\mathbb{R})$.

For $n \geq 0$, $I_n = (b - 1/n, b] \in \mathcal{B}(\mathbb{R})$. The countable
itersection of $\cap_{n \geq 1} I_n = \{b\}$ is also in the sigma-algebra.

All open intervals are in the Borel sigma-field: $(a, b) = (a, b] \cap \{b\}^c$.
All closed intervals are in the Borel sigma-field: $[a, b] = \{a\} \cup (a, b) \cup \{b\}$.

Is the subset $\mathbb{Q}$ a Borel set? That is, is $\mathcal{Q} \in \mathcal{B}(\mathbb{R})$? Yes: the rationals are countable, so they can be expressed as the union of singleton rational sets which are in the sigma-field $\mathcal{B}(\mathbb{R})$.


## 4.3 Proof of Theorem 4.16

Define $L^2(\G) = L^2(\Omega, \G, \P)$. Let $Y$ be a random variable in $L^2(\Omega, \F, \P)$ and let $Y^{\star} satisfy

$$
\min_{Z \in L^2(\G)} \E((Y - Z)^2) = \E((Y - Y^{\star})^2).
$$ {#eq-minimizer}

We show that $E(Y | X) = Y^{\star}$.


Let $W$ be a random variable in $L^2(\G)$: we show that $W$ is orthogonal to $Y - Y^{\star}$ which shows that $Y^{\star}$ is the
orthogonal projection of $Y$ into $L^2(\G)$.

Developing the sqaure:

$$
\E((W - (Y - Y^{\star}))^2) = \E(W^2) - 2 \E(W(Y - Y^{\star})) + \E((Y - Y^{\star})^2).
$$

From the definition of $Y^{\star}$ (and the fact that $W + Y^{\star} \in L^2(\G)$)

$$
\E((W - (Y - Y^{\star}))^2) \geq \E(Y - Y^{\star}).
$$

It follows that
$$
\E(W^2) \geq 2\E(W(Y - Y^{\star})).
$$

This holds for any $W$ and so we can assert that
$$
a^2\E(W^2) \geq 2 a\E(W(Y - Y^{\star})).
$$
Taking $a > 0$, we find that

$$
\E(W(Y - Y^{\star})) \leq \frac{a\E(W^w)}{2}
$$

and for $a < 0$

$$
\E(W(Y - Y^{\star})) \geq \frac{a\E(W^w)}{2}.
$$

Taking the limit as $a \to 0$, we see that

$$
\E(W(Y - Y^{\star})) = 0
$$

which is a defining propery of the conditional expectation $E(Y | \G)$.

Uniqueness:

Suppose that there are two conditional expectations of $Y$ given $\G$, $C_1$ and $C_2$.
Then for any $W \in L^2(\G)$,

$$
\E(W(C_1 - C_2)) = 0.
$$

Setting $W = C_1 - C_2$, we see that

$$
\E((C_1 - C_2)^2) = \| C_1 - C_2 \|_2^2 = 0
$$

and so $C_1 = C_2$.


::: {.callout-note}
The proof of Theorem 4.7 in the book shows uniqueness of $Y^{\star}$ satifying
(@eq-minimizer) but does not actually show that a conditional expection is
such a variable. 
:::



## 4.4 Another Look at Conditional Expectation For Gaussians

Let $(X, Y)$ be a Gaussian vector with mean $0$ and covariance matrix

$$
\begin{align}
\mathcal{C} &= \begin{bmatrix}
1 & \rho \\
\rho & 1.
\end{bmatrix}
\end{align}
$$

(a) The conditional expectation can be calculated using the $L^2$ best approximation form:

$$
\begin{align}
\E(Y|X) &= \frac{\E(XY) X}{\E(X^2)} \\
&= \rho X.
\end{align}
$$

(b) The joint pdf of $(X, Y)$ is

```{python}
import sympy as sp
from fractions import Fraction
x, y, rho = sp.symbols('x, y, rho', real=True)

C = sp.Matrix([[1, rho], [rho, 1]])
Cinv = C.inv()
xy = sp.Matrix([x, y])


pdf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/((2 * sp.pi)**(Fraction(len(xy),2)) * sp.sqrt(C.det()))
pdf[0]

```



$$
\begin{align}
\frac{\int_{\mathbb{R}} y f(x, y)\, dy}{\int_{\mathbb{R}} f (x, y) \, dy} &= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&= \frac{\int_{\mathbb{R}} (t + \rho x) e^{\frac{-t^2}{2(1-\rho)(1+\rho)}} \, dt}{\int_{\mathbb{R}} e^{\frac{-t^2}{2(1 -\rho)(1 + \rho)}} \, dt} \\
&= \sqrt{1 - \rho^2} \frac{\int_{\mathbb{R}} t e^{-t^2/2}\,dt}{\int_{\mathbb{R}} e^{-t^2/2}\, dt} + \rho x \\
&= \rho x = h(x).
\end{align}
$$

This is another way of arriving at
$$
\E(Y|X) = h(X).
$$

## 4.5 Gaussian Conditioning

$$
\mathcal{C} = \begin{bmatrix}
2 & 2 & 0 \\
2 & 4 & 0 \\
0 & 0 & 1 
\end{bmatrix}.
$$

It is easy to see that $\det{C} = 4$; non-zero determinant means that the vector
$(X_1, X_2, X_3 X_3)$ is non-degenerate.

From the covariance matrix, we see that $\E(X_1X_3) = \E(X_2 X_3) = 0$, so $X_3$ is
independent of $X_1$ and $X_2$.


$$
\begin{align}
\E(X_2 | X_1) &= \frac{\E(X_2X_1)}{\E(X_1^2)}X_1 \\
&= \frac{2}{2} X_1 \\
&= X_1.
\end{align}
$$

Therefore

$$
X_2 = X_1 + (X_2 - X_1)
$$

is a decomposition of $X_2$ into a linear combination of $X_1$ and a random variable
independent of $X_1$, namely $X_2 - X_1$.

## 4.6 Gaussian Conditioning

$(X, Y)$ is a Gaussian random vector with mean $0$ and covariance given by

$$
\mathcal{C} = \begin{bmatrix}
3/16 & 1/8 \\
1/8 & 1/4 \\
\end{bmatrix}.
$$

$(X, Y)$ is non-degenerate because $\det{C} > 0$:

```{python}
C = sp.Matrix([[Fraction(3, 16), Fraction(1, 8)], [Fraction(1,8), Fraction(1,4)]])
C.det()
```

$$
\E(Y|X) = \frac{\E(YX)}{\E(X^2)}X = (1/8) * (16/3)X =2X/3.
$$

$W = (Y - 2X/3)$ is independent of $X$ (it is orthogonal to all functions of $X$). We need

$$
\E(W^2) = \E(Y^2 - 4XY/3 + 4X^2/9) = \frac{1}{4} - \frac{4}{8.3} + \frac{4.3}{9.16} = \frac{1}{6}. 
$$



We can calculate the MGF of $Y$ conditioned on $X$:

$$
\begin{align}
\E(e^{aY} | X) &= \E(e^{a(W + 2X/3)} | X) \\
&= e^{2X/3} \E(e^{aW} X) \\
&= e^{2X/3} \E(e^{aW})  \\
&= e^{2X/3+ a^2/12}  \\
\end{align}
$$

and so the conditional distribution of $Y$ given $X$ is a Gaussian with mean $2X/3$ and variance $1/6$.

We define $Z_1 = 16X/3$ and $Z_2 = 6(Y - 2X/3)$: $Z_1$ and $Z_2$ are standard Gaussians and

$$
X = 16X/3
$$
and
$$
Y = Z_1/9 + Z_2/6.
$$



## 4.7 Gaussian Conditioning

$$
\mathcal{C} = \begin{bmatrix}
1 & -1 \\
-1 & 2 \\
\end{bmatrix}.
$$

$Z_1 = X_1$.

$$
\begin{align}
X_2 &= (X_2 - \E(X_2| Z_1)Z_1) + \E(X_2|Z_1) Z_1 \\
&= (X_2 + Z_1) - Z_1.
\end{align}
$$

So, set $Z_2 = X_2 + X_1$.

Checking:

$$
\E(Z_1) = \E(X_1) = 0
$$

and 

$$
\E(Z_1^2) = \E(X_1^2) = 1.
$$

$$
\E(Z_2) = \E(X_2) + \E(X_1) = 0
$$
and

$$
\begin{align}
\E(Z_2^2) &= \E(X_1^2 + 2X_1X_2  + X_2^2)  \\
&= 1 -2.1 + 2 \\
&= 1.
\end{align}
$$

Moreover,

$$
\begin{align}
\E(Z_2Z_1)  &= \E((X_1 + X_2) X_1) \\
&= \E(X_1^2 + X_1 X_2) \\
&= 1 - 1 \\
&= 0.
\end{align}
$$

$$
\E(X_2 | X_1) = \frac{\E(X_2 X_1)}{\E(X_1^2)}X_1 = -X_1.
$$

$$
\begin{align}
\E(e^{a X_2} | X_1) &= \E(e^{a (Z_2 - Z_1)}| Z_1) \\
&= e^{-a Z_1} \E(e^{a Z_2}| Z_1) \\
&= e^{-a Z_1} \E(e^{a Z_2}) \\
&= e^{-aZ_1 + a^2/2} \\
&= e^{-aX_1 + a^2/2}.
\end{align}
$$
using the independence of $Z_2$ and $Z_1$ and the MGF of a Gaussian of mean $0$ and
variance $1$. The conditional distribution of $X_2$ given $X_1$ is a Gaussian with mean $-X_1$ and variance $1$.

## 4.8 Gaussian Conditioning

We can use the Cholesky factorisation of the covariance matrix
to get a mapping from $(Z_1, Z_2, Z_3)$ of IID standard Gaussians to $(X_1, X_2, X_3)$:

```{python}
sp.Matrix([[2,1,1],[1,2,1],[1,1,2]]).cholesky()
```

$$
\begin{align}
X_1 &= \sqrt{2} Z_1, \\
X_2 &= \frac{Z_1}{\sqrt{2}} + \sqrt{\frac{3}{2}} Z_2, \\
X_3  &= \frac{Z_1}{\sqrt{2}} + \frac{Z_2}{\sqrt{6}} + \frac{2 Z_3}{\sqrt{3}}.
\end{align}
$$

We note that

$$
X_3 = X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}.
$$


This can be used to compute $\E(X_3 |X_2, X_1)$:

$$
\begin{align}
\E(X_3 | X_2, X_1) &= \E(X_2 - \frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}| X_2, X_1) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}|X_2, X_1) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1,
\end{align}
$$

where we've used $\E(X_1|X_1, X_2) = X_1$ and that $Z_3$ is independent of $X_1$ and $X_2$ (they are linear combinations of $Z_1$ and $Z_2$).

We can also compute $\E(e^{aX_3} | X_2, X_1)$:

$$
\begin{align}
\E(e^{aX_3}| X_2, X_1) &= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}|X_2, X_1) \\
&= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}) \\
&= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2a^2}{3})}.
\end{align}
$$
The conditional distribution is a Gaussian with mean $X_2 - \frac{\sqrt{3}}{2}X_1$ and variance $4/3$.

## 4.9 Properties of Conditional Expectation

(2) If $Y$ is a $\G$-measurable random variable and $X$ is another integrable random variable (with $XY$ also integrable), then

$$
\E(XY | \G) = Y \E(X | \G).
$$


Proof:

Let $W$ be a bounded, $\G$-measurable random variable.

$$
\begin{align}
\E (W Y \E( X | \G)) &= \E(W XY) \\
&= \E(W \E(XY | \G)).
\end{align}
$$

(3) If $Y$ is independent of $\G$, that is, for any events $I = \{Y \in (a, b]\}$ and $A \in \G$,

$$
\P(I \cap A) = \P(I) \P(A),
$$
then,
$$
\E(Y | \G) = \E(Y).
$$

Proof:

If $W$ is $\G$-measurable, then $W$ and $Y$ are independent.

$$
\begin{align}
\E(W \E(Y)) &= \E(Y) \E(W) & (\text{linearity of expectation}) \\
&= \E(WY) & (\text{$W$ and $Y$ are independent}) \\
&= \E(W \E(Y | \G)) & (\text{by definition}).
\end{align}
$$

By uniqueness of the conditional expectation, $\E(Y) = \E(Y | \G)$.

(4) Linearity: Let $X$ be another integrable random variable on $(\Omega, \F, \P)$. Then

$$
\E(a X + b Y| \G) = a \E(X | \G) + b \E( Y | \G),
$$
for any $a, b \in \mathbb{R}$.

Proof:

$$
\begin{align}
\E(W (a X + b Y)) &= a \E(WX) + b \E(WY) & (\text{by linearity of expectation}) \\
&= a \E(W\E(X | \G)) + b \E(W \E(Y | \G)) & (\text{by defn of conditional expectation})
\end{align}
$$

but

$$
\E(W\E((a X + b Y) | \G)) = \E(W(a X + b Y))
$$
by definition. It follows from the uniqueness of the conditional expectation that

$$
\E(W\E((a X + b Y) | \G)) = \E(W(a X + b Y)| \G)
$$
for $a, b \in \mathbb{R}$.

(5) Tower Property: If $\mathcal{H} \subseteq \mathcal{G}$ is a sigma-field of $\Omega$, the

$$
\E(Y | \mathcal{H}) = \E(\E(Y | \mathcal{G})|\mathcal{H}).
$$

Proof:

$$
\E(W \E(Y | \mathcal{H})) = \E(W Y)
$$

by definition.

$$
\E(W \E(\E(Y | \G) | \mathcal{H})) = \E(W \E(Y | \G)) = \E(WY)
$$
by definition. Equating the two, we see that

$$
\E(W \E(\E(Y | \G) | \mathcal{H})) = \E(W \E(Y | \mathcal{H}))
$$
and so

$$
\E(Y | \mathcal{H}) = \E(\E(Y | \mathcal{G})| \mathcal{H}).
$$

## 4.10 Square of Brownian Motion

Let $(B_t, t \geq 0)$ be a standard Brownian motion. Then $M_t = B_t^2 - t$ is a martingale for the Brownian filtration.


Proof:


Firstly, the process $(M_t)$ is adapted to the Brownian filtration because both $B_t^2$ and $t$ are measurable w.r.t the Brownian filtration at $t$.

Secondly, 

$$
\E(|M_t|) \leq E(B_t^2) + t = 2t < \infty
$$
for $t \geq 0$.

Lastly, we use

$$
\E(B_t - B_s | \F_s) = \E(B_t - B_s) = t - s
$$
since $B_t - B_s$ is independent of $B_s$,
$$
\E(B_s^2 | \F_s) = B_s^2
$$
because $B_s^2$ is $F_s$-measurable:

$$
\begin{align}
\E(M_t | \F_s) &= \E(B_t^2 - t |\F_s) \\
&= \E(B_t^2 | \F_s) - t \\
&= \E((B_t - B_s + B_s)^2 | \F_s) -t \\
&= \E((B_t - B_s)^2 + 2B_s(B_t - B_s) + B_s^2|\F_s) -t \\
&= \E((B_t - B_s)^2) + 2 \E(B_s) \E(B_t - B_s) + B_s^2 - t \\
&= (t -s) + B_s^2 -t \\
&= B_s^2 -t \\
&= M_s
\end{align}
$$
for any $s \leq t$.

## 4.11 Geometric Poisson Process

Let $(N_t, t \geq 0)$ be a Poisson proces of intensity $\lambda$. For $\alpha > 0$, the process
$(e^{\alpha N_t - \lambda t(e^{\alpha} -1)}, t \geq 0)$ is a martingale for the filtration of the Poisson process $(N_t, t \geq 0)$.


Proof:
Let $M_t$ denote the process. Clearly, $M_t$ is measurable on the filtration of the Poisson process and, using the MGF of the Poisson distribution

$$
\begin{align}
\E(|M_t|) &= \E(M_t) \\
&= e^{-\lambda t(e^{\alpha} -1)} \E(e^{\alpha N_t}) \\
&= e^{-\lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_0 + N_0}) \\
&= e^{-\lambda t(e^{\alpha} -1)} e^{\lambda t (e^{\alpha} - 1)} \E(e^{\alpha N_0}) \\
&= 1 < \infty.
\end{align}
$$


Using the MGF of the Poisson distribution (again):

$$
\begin{align}
\E(M_t | \F_s) &= \E(e^{\alpha N_t - \lambda t(e^{\alpha} -1)} | \F_s) \\
&= \E(e^{\alpha N_t} e^{- \lambda t(e^{\alpha} -1)} | \F_s) \\
&= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha N_t} | \F_s) \\
&= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_s + N_s)} | \F_s) \\
&= e^{- \lambda t(e^{\alpha} -1)} \E(e^{\alpha (N_t - N_s)} e^{\alpha N_s} | \F_s) \\
&= e^{- \lambda t(e^{\alpha} -1)} e^{\alpha N_s}\E(e^{\alpha (N_t - N_s)}) \\
&= e^{- \lambda t(e^{\alpha} -1)} e^{\alpha N_s}e^{\lambda (t - s)(e^{\alpha} - 1)} \\
&= e^{\alpha N_s -\lambda s(e^{\alpha} - 1)} \\
&= M_s
\end{align}
$$
for $t \geq s$.

## 4.12 Another Brownian Martingale

#### (a) 
$M_t = t B_{t} - B_{t}^3/3$ is a martingale

Proof:

$M_t$ is measurable wrt the Brownian filtration.


We use a standard trick of creatively adding zero to $B_t$:

$$
\begin{align}
\E(B_t^3 | \F_s) &= \E(((B_t - B_s) + B_s)^3 | \F_s) \\
&= \E((B_t - B_s)^3 + 3(B_t - B_s)^2B_s + 3(B_t - B_s) B_s^2 + B_s^3 | \F_s) \\
&= \underbrace{\E((B_t -B_s)^3)}_{B_t - B_s, B_s \text{ are independent}} + \underbrace{3B_s \E((B_t - B_s)^2)}_{B_s \text{ is } \F_s-\text{measurable}} + B_s^3 \\
&= 3 B_s(t -s) + B_s^3.
\end{align}
$$

Rearranging:

$$
\begin{align}
M_s &= sB_s - B_s^3/3\\
&= \underbrace{t B_s}_{\E(B_t|\F_s) = B_s} - \E(B_t^3/3|F_s) \\
&= \E(t B_t - B_t^3/3 | F_s) \\
&= \E(M_t | \F_s)
\end{align}
$$

for any $s \leq t$.




#### (b)

Let $a > 0$

$M_t = t B_{t} - B_{t}^3/3$ is a martingale; the stopped martingale $M_{t \wedge \tau}$ is also a martingale and so

$$
\E(M_{\tau \wedge t}) = \E(M_0) = 0.
$$

We can't use Doob's optional stopping theorem directly as $M_{\tau\wedge t}$ is not bounded. However, we can argue that

$$
\lim_{t \to \infty} \E(\tau \wedge t B_{\tau \wedge t}) = \E(\tau B_\tau)
$$
by the dominated convergence theorem:

$$
\left | \tau \wedge t B_{\tau \wedge t} \right| \leq \left| \tau (a \vee b) \right|
$$
and $\E(| \tau |) = \E(\tau) < \infty$. Similar reasoning for $B_{\tau \wedge t}^3$ gives

$$
\begin{align}
0 &= \E(M_0) = E(M_{\tau\wedge t})\\
&= \lim_{t\to \infty} E(M_{\tau \wedge t})\\
&= \E(\tau B_{\tau} - B_{\tau}^3/3).
\end{align}
$$

We apply this to get our result:


$$
\begin{align}
\E(\tau B_{\tau}) &= \E(\tau B_{\tau} - B_{\tau}^3/3 + B_{\tau}^3/3) \\
&= \E(B_{\tau}^3/3) \\
&=  a^3 \P(B_{\tau} = a) + (-b)^3\P(B_{\tau} = -b)  \\
&=  \frac{a^3b + (-b)^3a}{3(a + b)} \\
&=  \frac{ab(a^2 -b^2)}{3(a + b)} \\
&= \frac{ab}{3}(a -b).
\end{align}
$$

#### (c)

The errata mentions that the notation chosen is unfortunate. Let's consider

$$
M_t = e^{\lambda B_t - \lambda^2 t/2}.
$$

$M_t$ is a martingale for $\lambda > 0$.

Proof:

$M_t$ is adapted to the Brownian filtration.

$M_t$ is integrable:

$$
\begin{align}
\E(|M_t|) &= \E(e^{\lambda B_t - \lambda^2 t/2}) \\
&= e^{-\lambda^2 t/2} \underbrace{e^{\lambda^2 t/2}}_{\text{MGF of } \lambda B_t} \\
&= 1.
\end{align}
$$

$$
\begin{align}
\E(e^{\lambda B_t - \lambda^2 t/2}| \F_s) &= e^{\lambda^2 t/2} \E(e^{\lambda (B_t - B_s) + \lambda B_s } | \F_s) \\
&= e^{\lambda^2 t/2} e^{\lambda B_s} \E(e^{\lambda (B_t - B_s)}) \\
&= e^{\lambda^2 t/2} e^{\lambda B_s} \underbrace{e^{\lambda^2(t-s)/2}}_{\text{MGF of } \lambda(B_t - B_s)} \\
&= M_s.
\end{align}
$$
$$\qed$$

We can use Doob's optional stopping theorem to assert that

$$
\E(e^{\lambda B_{\tau} - \lambda^2 \tau/2}) = 1.
$$

#### (d)
With the notation $X_\tau = \lambda B_{\tau} - \lambda^2 \tau/2$:
$$
\begin{align}
\frac{d}{d\lambda}\E(e^{X_\tau}) &=  \E(e^{X_{\tau}}\frac{dX_\tau}{d\lambda}) = 0, \\
\frac{d^2}{d\lambda^2}\E(e^{X_\tau}) &=  \E(e^{X_{\tau}}(\frac{d^2X_\tau}{d\lambda^2} + (\frac{dX_{\tau}}{d\lambda})^2)) = 0,\\
\frac{d^3}{d\lambda^3}\E(e^{X_\tau}) &=  \E(\frac{dX_{\tau}}{d\lambda} e^{X_{\tau}}(\frac{d^2X_\tau}{d\lambda^2} + (\frac{dX_{\tau}}{d\lambda})^2) + e^{X_{\tau}}(\frac{d^3X_{\tau}}{d\lambda^3} + 2\frac{dX_{\tau}}{d\lambda}\frac{d^2X_{\tau})}{d\lambda^2}) = 0\\
\end{align}
$$
for $n \geq 1$.

We have the following:

$$
\begin{align}
\frac{dX_\tau}{d\lambda} &= B_{\tau} - \lambda \tau, \\
\frac{d^2X_\tau}{d\lambda^2} &= -\tau, \\
\frac{d^3X_\tau}{d\lambda^3} &= 0.
\end{align}
$$




Substituting and taking the limit as $\lambda \to 0$:

$$
\begin{align}
\frac{d^3}{d \lambda^3} \E(e^{X_{\tau}}) &= \E(e^{X_\tau}\left ((B_\tau - \lambda \tau)(-\tau +B_{\tau}^2 -2 \lambda \tau B_{\tau} + \lambda \tau^2) -2 \tau(B_\tau - \lambda \tau) \right)) \\
& \to \E(B_{\tau}^3 - 3 \tau B_{\tau}) =0.
\end{align}
$$

As we have already seen
$$
\begin{align}
\E(B_{\tau}^3) &= (a^3b - b^3a)/(a + b) = ab(a^2 - b^2)/(a+b) = ab(a - b)
\end{align}
$$
so

$$
\E(\tau B_{\tau}) = \frac{1}{3}\E(B_{\tau}^3) = \frac{ab}{3}(a -b).
$$


















## 4.13 Limit of Geometric Brownian Motion

Define $M_t = S_0 e^{\sigma B_t + \mu t}$ for fixed $\sigma > 0$ andd $\mu < 0$.

We have
$$
\lim_{t \to \infty} \frac{B_t}{t} = 0
$$
almost surely: in particular, there is an event $A \subseteq \Omega$ with $\P(A) =1$ such that
for any $\varepsilon > 0$ there exists $t(\varepsilon) > 0$ such that for $t > t(\varepsilon)$

$$
|B_t| < \varepsilon t.
$$

For $\omega \in A$, choose $\varepsilon < - \mu/ 2\sigma$, then

$$
e^{\sigma B_t + \mu t} \leq e^{\sigma |B_t| + \mu t} < e^{\frac{\mu t}{2 \sigma}}
$$
for $t > t(\varepsilon)$ i.e. for $\omega \in A$, 

$$
e^{\sigma B_t + \mu t} \to 0
$$
as $t \to 0$.

On the other hand,

$$
\begin{align}
\E(e^{\sigma B_t + \mu t}| \F_s) &= e^{\mu t} \E(e^{\sigma(B_t - B_s) + \sigma B_t}| \F_s) \\
&=  e^{\sigma B_s + \mu t} \E(e^{\sigma(B_t - B_s)}) \\
&=  e^{\sigma B_s + \mu t} e^{\sigma^2(t-s)^2/2}. \\
\end{align}
$$

Taking the expectation of both sides we see that

$$
\E(M_t) = \E(M_s)e^{\sigma^2(t-s)^2/2}.
$$

In particular,

$$
\E(M_t) = S_0 e^{\sigma^2 t^2/2}.
$$


In $L^1$:

$$
\| M_t \|_1 = |S_0| e^{\sigma^2 t^2/2} \to \infty
$$
as $t \to \infty$.

In $L^2$:

$$
\| M_t \|_2^2  = \E(M_t^2) = S_0^2 e^{2 \sigma^2 t^2} \to \infty
$$
as $t \to \infty$.

Therefore $M_t \to 0$ almost surely but does not converge to zero in $L^1$ or $L^2$.

## 4.14 Gambler's Ruin at French Roulette

Let $(S_n, n \geq 0)$ be a simple random walk with bias starting at $S_0 = 100$ with

$$
S_n = S_0 + \sum_{k=1}^n X_k,
$$
where $\P(X_1 = +1) = p$ and $\P(X_1 = -1) = 1 - p =q$ with $p < 1/2$.

#### (a)
$$
M_n = (q/p)^{S_n}
$$

is a martingale for the filtration $(\F_n, n \in \mathbb{N})$ where 
$$
\F_n = \sigma(X_m, m \leq n).
$$

Proof:

$M_n$ is measurable wrt to $\F_n$ as it is a function of measurable random variables on $\F_n$.


For $n \in \mathbb{N}$

$$
\begin{align}
\E(|M_n|) &= \E(|(q/p)^{S_n}|) \\
&= \E((q/p)^{S_n}) \\
&= 2^{S_n} < \infty.
\end{align}
$$

$M_n$ has the martingale property:

$$
\begin{align}
\E(M_n |\F_m) &= \E((q/p)^{S_n} | \F_m) \\
&= \E((q/p)^{S_n - S_m}(q/p)^{S_m}| \F_m) \\
&= (q/p)^{S_m} \underbrace{\E((q/p)^{S_n - S_m})}_{\text{increments are independent}} \\
&= M_s \E((q/p)^{X_{m+1} + \cdots + X_{n}}) \\
&= M_s \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \P(\sum_{i=1}^{n-m} X_i = k) \\
&= M_s \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \binom{n-m}{(k + (n-m))/2}p^{(k + (n-m))/2}q^{(n-m) -(k +(n+m))/2} \\
&= M_s (pq)^{(n-m)/2} \sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \binom{n-m}{(k + (n-m))/2}(p/q)^{k/2} \\
&= M_s (pq)^{(n-m)/2} \sum_{k=-(n-m),k+=2}^{n-m} \binom{n-m}{(k + (n-m))/2}(q/p)^{k/2} \\
&= M_s (pq)^{(n-m)/2} \sum_{k=0}^{n-m} \binom{n-m}{k}(q/p)^{k -(n-m)/2} \\
&= M_s p^{(n-m)} \sum_{k=0}^{n-m} \binom{n-m}{k}(q/p)^k \\
&= M_s p^{(n-m)} (1 + q/p)^{n-m} \\
&= M_s p^{(n-m)} (1/p)^{n-m} \\
&= M_s.
\end{align}
$$

$$
\qed
$$

#### (b)

Define the stopping time $\tau = \min\{n \geq 0: S_n = 200 \text{ or } S_n = 0\}$.
Then

$$
\P(\tau < \infty) = 1.
$$

Proof:


Let 
$$
E_n = \{\sum_{i=200n}^{200(n+1)} X_i = 200\}.
$$

It is clear that if any $E_n$ occurs, then $\tau < \infty$: that is,

$$
\{ \tau < \infty \} \subseteq \cap_n E_n^c.
$$
By independence,

$$
\P(E_n) = \P(E_0).
$$

So

$$
\P(E_0^c \cap \ldots \cap E_n^c) = (1- P(E_0))^n \to 0
$$
as $n \to \infty$.

The events $F_n = \cap_{i=0}^n E_n$ are decreasing so we can use continuity of probability to state that

$$
P(\cap_n F_n) = 0.
$$

The result follows by noting that $\{ \tau = \infty\} \subseteq \cap_n F_n$. $$\qed$$.

Therefore

$$
\P(\tau = \infty) \leq \P(\cap E_n^c) = 0.
$$

#### (c)

$$
\P( S_{\tau} = 200) = \frac{1-(q/p)^{100}}{1 - (q/p)^{200}}.
$$

Proof:

$\tau$ is a stopping time. $M_{\tau \wedge t}$ is bounded so Doob's optional stopping theorem applies:

$$
\E(M_\tau) = \E(M_0) = (q/p)^{S_0}.
$$


$S_{\tau}$ is a discrete random variable taking values $0$ and $200$ so it's easy to calculate the LHS of the above:

$$
\begin{align}
\E((q/p)^{S_\tau}) &= (q/p)^0 \P(S_\tau = 0) + (q/p)^{200} \P(S_\tau = 200) \\
&= (1 - P(S_\tau = 200)) + (q/p)^{200} \P(S_\tau = 200) \\
&= 1 - P(S_\tau = 200)(1 + (q/p)^{200}) = (q/p)^{100}.
\end{align}
$$

Rearranging yields the result:
$$
P(S_{\tau} = 200) = \frac{1 - (q/p)^{100}}{1 - (q/p)^{200}}.
$$

$$
\qed
$$

#### (d)

```{python}
p = 18/38
q = 1 - p

((1 - (q/p)**100)/(1 - (q/p)**200))

```

#### (e)

We seek a starting point $S_0$ for which the probability of hitting $100$ is the same
as $200$. I estimate this will be close to $200$, given the numerical experiments on
roulette.

$$
\begin{align}
\E((q/p)^{S_\tau}) &= (q/p)^{100} \P(S_\tau = 100) + (q/p)^{200} \P(S_\tau = 200) \\
&= (q/p)^{S_0}
\end{align}
$$
because $(q/p)^{S_n}$ is a martingale.


If $$\P(S_\tau = 100) = \P(S_\tau = 200) = 1/2,$$ then
$$
\begin{align}
(q/p)^{S_0 - 100} = 1/2(1 + (q/p)^{100})
\end{align}
$$

and so

$$
S_0 = \frac{\log(\frac{1 + (q/p)^{100}}{2})}{\log(q/p)} + 100
$$

```{python}
import math
S_0 = math.log((1 + (q/p)**100)/2)/math.log(q/p) + 100; S_0
```

## 4.15 La Martingale Classique

Define a process $M_n$ by

$$
M_n = \sum_{k=0}^{n-1} 2^k(S_{k+1} - S_k)
$$
where $S_0 = 0$ and

$$
S_n = \sum_k X_k
$$
where $(X_k, k \geq 1)$ are IID random variables that take value $\pm 1$ with probability $1/2$.
We are given that $M_n$ is a martingale.

#### (a)
Consider the stopping time $\tau$, the first time $m$ with $X_m = +1$.
$$
\E(M_{\tau}) = 1
$$
yet

$$
\E(M_0) = 0.
$$

Proof:

:::{.callout-note}

The way the winnings/losses $M_n$ is expressed makes this question easy as the
telescoping sum is then obvious. It could have been written as

$$
M_n = \sum_{k=0}^{n-1} 2^k X_{k+1}
$$
and the trick may not be as immediate unless you are familiar with
properties of binary expansions of integers:
$$
2^{n} = 1 + \sum_{k=0}^{n-1} 2^{k}.
$$
:::

$$
\begin{align}
\E(M_\tau) &= \E(\sum_{k=0}^{m-1} 2^k(S_{k+1} - S_k)) \\
&= \E(\sum_{k=0}^{m-2} 2^k((-1) - (-1)) + 1) \\
&= \E(1) \\
&= 1.
\end{align}
$$

$$
\begin{align}
\E(M_0) &= 0
\end{align}
$$
by definition.
$$
\qed
$$

#### (b)

Optional stopping doesn't apply here because $M_{\tau \wedge t}$ is not bounded: $M_{\tau \wedge t}$ can take
arbitrarily large negative values before reaching the first $m$ such that $X_m = +1$.

#### (c)

The weakness of this strategy is that to execute it, the player needs to have unbounded money to gamble and 
unbounded time to play.










## 4.16 A Martingale From Conditional Expectation

Let $X \in L^1(\Omega, \F, \P)$, and let $(\F_t, t \geq 0)$ be a filtration. Then the process defined by

$$
M_t = \E(X | \F_t)
$$

is a martingale.

Proof:

Clearly, $M_t$ is measurable on $\F_t$. Also, $M_t$ is integrable:

$$
\E(|M_t|) = \E(|E(X | \F_t)|) \leq \E(\E(|X||\F_t)) = \E(|X|) < \infty.
$$

For $s \leq t$ 

$$
\begin{align}
\E(M_t | \F_s) &= \E(\E(X|\F_t) |F_s) \\
&= \underbrace{\E(X| F_s)}_{\text{by the tower property}} \\
&= M_s.
\end{align}
$$

$$\qed$$

## 4.17 Joint Distribution of $(max_{t \leq T} B_t, B_T)$

$$
\P(max_{t \leq T} B_t > m, B_T \leq a) = \P(B_T > 2m -a)
$$
and the joint pdf between the two is

$$
f(m, a) = \frac{2(2m -a)}{T^{3/2}\sqrt{2 \pi}} e^{-\frac{(2m -a)^2}{2T}}.
$$

Proof:



## 4.18 Zeros of Brownian Motion

For any $t > 0$, $\P(\max_{s\leq t} B_s > 0) = \P(\min_{s \leq t} B_s < 0) = 1$


Proof:

Bachelier's formula yields the following:

$$
\P(\max_{s \leq t} B_s \leq 0) = \P(|B_t| \leq 0) = 0.
$$

:::{.callout-note collapse="true"} 
## Why is $P(|B_t| = 0)$?

We know that $B_t$ is continous: in fact, it is absolutely continous because it has a
probability density function. By definition, a random variable $X$ is continous if

$$
\P(X = x) = 0
$$
for all $x$.
:::

Therefore, 

$$
\P(\max_{s \leq t} B_s > 0) = 1.
$$

The result follows by applying the result to $-B_t$, which is also a Brownian motion.
$$\qed$$

The function $s \mapsto B_s(\omega)$ has infinitely many zeros in the interval $[0, t]$, with
probability one.

Proof:

In the interval $[0, t]$, our function takes a maximum value $M> 0$ and a minimum $m<0$, at $t_M>0$ and $t_m>0$, respectively.
Between these times, the function has a zero by continuity. There exists a $\varepsilon>0$ such that
$T = \min(t_M, t_m) - \varepsilon > 0$. Repeating this argument for $[0, T]$ we see that $s \mapsto B_s(\omega)$ has two zeros in [0, t] and we can repeat as many times as we wish to get $n$ zeros for any $n \in \mathbb{N}$ i.e. there are infinitely many zeros of $s \mapsto B_s(\omega)$ in $[0, t]$. $$\qed$$




## 4.19 Doob's Maximal Inequalities

Let $(M_k, k \geq 1)$ be a positive submartingale for the filtration $( \F_k, k \in \mathbb{N})$. Then for any
$1 \leq p < \infty$ and $a > 0$

$$
\P(\max_{k\leq n}M_k > a) \leq \frac{1}{a^p} \E(M_n^p).
$$

Proof:

#### (a)
Use Jensen's inequality to show that if $(M_k, k \geq 1)$ is a submartingale, then
so is $(M^p_k, k\geq 1)$ for $1 \leq p < \infty$. For $x \geq 0$,

$$
x \mapsto x^p
$$
is a convex function for $1 \leq p < \infty$. The process $M_n$ is positive and so by Jensen's inequality

$$
\E(M^p_n|\F_m) \geq \E(M_n|\F_m)^p = M_m^p.
$$
That is, the process $(M_k^p, k\geq 1)$ is submartingale.

If we prove the statement for $p=1$, then

$$
\frac{1}{a^p}\E(M_n^p) \geq \frac{1}{a}\E(M_n)^p \geq \P(\max_{k \leq n}M_k > a)
$$
for $1 \leq p < \infty$: moreover, we need only prove that $M_n$ is a submartingale for the theorem to apply.

#### (b) 

Consider the events

$$
B_k = \cap_{j < k} \{ \omega : M_j(\omega) \leq a\} \cap\{\omega: M_k(\omega) > a\}.
$$

The $B_k$'s are disjoint: if $m < n$ and $\omega \in B_m$, then $M_m(\omega) > a$ and so $\omega \notin B_n$.

$$
\cup_{k \leq n} B_k = \{\max_{k \leq n} M_k > a\} \triangleq B.
$$

Choose $\omega$ in the LHS set. Then $\omega$ is in exactly one $B_k$ and so
$M_k(\omega) > a$ which means that

$$
\max_{k \leq n} M_k(\omega) > a,
$$
that is

$$
\cup_{k \leq n} B_k \subseteq \{\max_{k \leq n} M_k > a\}.
$$

To prove the reverse inclusion, take $\omega \in \{\max_{k \leq n} M_k > a\}$. Then, for some $k \leq n$, $M_k(\omega) > a$ and $M_j(\omega) \leq a$ for $j < k$ and so $\omega \in \cup_{k \leq n} B_k$.

#### (c)

$$
\E(M_n) \geq \E(M_n \mathbb{1}_B) \geq a \sum_{k \leq n} \P(B_k) = a\P(B).
$$

Proof:

$$
\begin{align}
\E(M_n) &\underbrace{\geq \E(M_n \mathbb{1}_B)}_{\text{only true because } M_n >= 0} \\
&= \sum_k\E(M_n \mathbb{1}_{B_k}) \\
&= \sum_k\E(\E(M_n \mathbb{1}_{B_k}| \F_k)) \\
&= \sum_k\E(\underbrace{\mathbb{1}_{B_k}}_{\text{this is $\F_k$-measurable}} \E(M_n| \F_k)) \\
&\geq \sum_k\E(\mathbb{1}_{B_k} \underbrace{M_k}_{\text{submartingale}})) \\
&\geq a\sum_k\E( \mathbb{1}_{B_k} ) \\
&= a \sum_k \P(B_k) \\
&= a \P(B).
\end{align}
$$

$$
\qed
$$

#### (d)

For any finite set of times $0 = t_0 < t_1 < \ldots < t_n = t$, the inequality

$$
\P(\max_{k \leq n} M_{t_k} > a) \leq \frac{1}{a^p} \E(M_n^p)
$$

because

$$
S_k = M_{t_k}
$$

defines a discrete submartingale.

For each $n$, define a discretisation of $[0, t]$ by

$$
t_k = \frac{kt}{2^n}
$$
for $k =1, \ldots, 2^n$.  Define a collection of events

$$
A_n = \{\omega : \max_{k \leq 2^n} M_{t_k} > a\}.
$$

The sets are nested:

$$
A_m \subseteq A_n
$$
if $m \leq n$. For each $n$,

$$
P(A_n) \leq \frac{1}{a^p}\E(M_t^p).
$$


Using the continuity of probability

$$
\P(A) = \lim_{n\to \infty} \P(A_n)
$$
where
$$
A = \cup_{n} A_n.
$$

Therefore,

$$
\P(\max_{s \in \mathcal{D}} M_s > a) \leq \frac{1}{a^p} \E(M_t^p)
$$
where $\mathcal{D}$ is the dense subset of $[0, t]$ of real numbers of the form
$$
kt/2^n
$$
for some $k$ and $n$.

It is clear that

$$
\{\omega: \max_{s \in \mathcal{D}} M_s(\omega) > a\} \subseteq \{\omega: \max_{s \in [0, t]} M_s(\omega) > a\}.
$$

The reverse inclusion follows from the fact that

$$
s \mapsto M_s(\omega)
$$
is continous, almost surely: $M_s(\omega)$ will attain a maximum on $[0, t]$ and can be approximated 
to arbitrary precision with $M_{s'}(\omega)$ for $s' \in \mathcal{D}$.

$$
\qed
$$

## 4.20 An Application of Doob's Maximal Inequalities


#### (a)

If $(B_t, t \geq 0)$ is a Brownian motion, then

$$
\lim_{n \to \infty} \frac{B_n}{n} = 0
$$
a.s. when $n$ is an integer.


Proof:

We do the usual telescoping sum trick:

$$
\begin{align}
\frac{B_n}{n} &= \frac{\sum_{k=0}^{n-1} (B_{k+1} - B_k)}{n} \\
&= \frac{\sum_{k=0}^{n-1} X_k}{n}
\end{align}
$$

where $X_k = B_{k+1} - B_k$ are Brownian increments. Using the strong law of large numbers

$$
\lim_{n \to \infty} \frac{B_n}{n} = \lim_{n\to\infty} \bar{X_k} = \E(X_0) = 0.
$$

$$\qed$$

#### (b)

$$
\sum_{n \geq 0} \P(\max_{0 \leq s \leq 1} |B_{n+s} - B_n| > \delta n) < \infty
$$

for any $\delta > 0$.

Proof:

The process $M_t = |B_{n+t} - B_n|$ is a positive submartingale of the filteration $(\G_s = \F_{n+s}, s \geq 0)$:

$$
\begin{align}
\E(M_t | \G_s) &= \E(|B_{n+t} - B_n| | \F_{n+s}) \\
& \geq \left| \E(B_{n + t} - B_n| \F_{n +s })\right| \\
&=\left| \E(B_{n + t}|\F_{n+s}) - \E(B_n| \F_{n +s })\right | \\
&=\left | \underbrace{B_{n+s}}_{\text{martingale property}} - \E(B_n| \F_{n +s })\right| \\
&=\left | B_{n+s} - \underbrace{B_n}_{B_n \text{is } \F_{n+s} \text{ measurable}} \right| \\
&= M_s
\end{align}
$$
for $s \leq t$.

Doob's maximal inequality applies to $M_t$:

$$
\begin{align}
\P(\max_{0 \leq s \leq 1}|B_{n + s} - B_n| > \delta n) &\leq \frac{1}{\delta^2 n^2}\E((B_{n+1} - B_n)^2) \\
&= \frac{1}{\delta^2 n^2}\\
\end{align}
$$

$$
\qed
$$

#### (c)

Taking the sum
$$
\sum_n \P(\max_{0 \leq s \leq 1}|B_{n + s} - B_n| > \delta n) = \frac{\pi^2}{6 \delta^2} < \infty.
$$

As a consequence

$$
\lim_{n\to\infty} \max_{0 \leq s \leq 1} \frac{|B_{n+s} - B_n|}{n} = 0
$$
almost surely.

$$
\qed
$$

#### (d)

If $t_n \uparrow \infty$, then

$$
\lim_{n\to \infty} \frac{B_{t_n}}{t_n} = 0.
$$

Proof:

Let $k = \lfloor t_n \rfloor$ and $s = t_n - k$. Then

$$
\begin{align}
\lim_{n\to \infty} \frac{B_{t_n}}{t_n} &= \lim_{n\to\infty} \frac{B_{k+s} - B_k + B_k}{t_n} \\
&\leq \lim_{n\to\infty} \frac{B_{k+s} - B_k + B_k}{k} \\
&\leq \lim_{n\to\infty} \max_{0 \leq s \leq 1}\frac{|B_{k+s} - B_k| + |B_k|}{k} \\
& = 0.
\end{align}
$$

$$
\qed
$$


#### (e)

If
$$
X_t = t B_{1/t},
$$
then
$$
\lim_{t \to 0+} X_t = 0
$$
almost surely.

Proof:

$$
X_{\frac{1}{t}} = \frac{B_t}{t} \to 0
$$
as $t \to \infty$, almost surely.

$$
\qed
$$




















## 4.21 An Example of Fubini's Theorem


Let $(X_n, n \geq 1)$ be a sequence of random variables on $(\Omega, \F, \P)$. If

$$
\sum_n \E(|X_n|) < \infty
$$
then

$$
\E \left(\sum_{n} X_n \right ) = \sum_n \E(X_n).
$$

Proof:

Let

$$
S_n = \sum_{k=1}^n X_k.
$$

Almost surely

$$
S_n \to \sum_{k=1}^{\infty} X_k
$$
as $n \to \infty$ and

$$
| S_n | =  |\sum_{k=1}^{n} X_k| \leq \sum_{k=1}^{n} |X_k| \leq \sum_{k=1}^{\infty} |X_k|.
$$

So $S_n$ is dominated by

$$
\sum_k |X_k|
$$
and this is integrable by our assumptions:

$$
\E(\sum_{k=1}^{\infty} |X_k|)  \leq \sum_{k=1}^{\infty} \E(|X_k|) < \infty
$$

So we can invoke the dominated convergence theorem to state that

$$
\lim_{n\to \infty} \E(S_n) = \E(\lim_{n \to \infty} S_n).
$$
That is

$$
\sum_n \E(X_n) = \E\left(\sum_n X_n\right).
$$

$$
\qed
$$




































