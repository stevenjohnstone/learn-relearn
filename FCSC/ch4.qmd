---
title: "A First Course in Stochastic Calculus"
subtitle: "Chapter Four Exercises"
toc: true
toc-location: body
execute:
    freeze: true
description: "Solutions to exercises and computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---

{{< include /_macros.qmd >}}

# Notes

## Standard Brownian Motion is a Martingale

### Example 4.28 (i)
In the book, it is stated that

$$
\E(B_t | \F_s) = \E(B_s + B_t - B_s | \F_s) = B_s + E(B_t - B_s) = B_s.
$$

Let's break this down:

$$
\E(B_s | \F_s) = B_s
$$

because $B_s$ is $\F_s$ measurable.

$$
\E(B_t - B_s | \F_s) = \E(B_t - B_s) = 0.
$$
because $B_t - B_s$ is independent of $B_s$ (by independence of increments and $B_s = B_s - B_0$).

### Example 4.29 (i)

Symmetric random walk:

$(X_i, i \in \mathbb{N})$ are random IID variables with $E(X_1) = 0$ and $E(|X_1|) < \infty$. Take $\F_n = \sigma(X_i, i \leq n)$ and 
$$
S_n = X_1 + \cdots + X_n, S_0 = 0.
$$


$$
\begin{align}
E(S_{n+k} | \F_n) &= E(S_n + \sum_{i=n+1}^k S_i | \F_n) \\
&=S_n + \sum_{i = n+1}^k \E(S_i| \F_n) \\ 
&=S_n
\end{align}
$$
where $\E(S_i | \F_n) = 0$ for $i > n$ by the IID assumption.

### Example 4.36 Last Passage Time is NOT a Stopping Time
The last passage time is for $a$

$$
\rho(\omega) = \max \{ s \geq 0 : X_s(\omega) \geq a \}.
$$

Suppose that $X_t$ is a martingale with filtration $(\F_t, t \geq 0)$. Suppose that $\{ \omega : \rho(\omega) \leq t\} \in \F_t$ for all $t\geq 0$. Consider $\{ \omega : \rho(\omega) \leq 0 \} \in \F_0$. We know that $\F_0 = \{ \Omega, \emptyset \}$.

$$
\begin{align}
\{ \omega : \rho(\omega) \leq 0 \} &= \{ \omega: \max\{ s \geq : X_s(\omega) \geq a \} \leq 0\} \\
&= \{ \omega: X_s(\omega) < a,\, \forall s > 0 \}.
\end{align}
$$

We have
$$
\begin{align}
\{ \omega: X_s(\omega) < a, \, \forall s > 0 \} &= \Omega
\end{align}
$$

or 
$$
\begin{align}
\{ \omega: X_s(\omega) < a, \, \forall s > 0 \} &= \emptyset.
\end{align}
$$
If the former is true, then $X_s(\omega) < a$ for all $s > 0$ with probability one. If the latter is true, then $X_s(\omega) \geq a$ for all $s > 0$ with probability one. Therefore, if $\rho$ is a stopping time for $X_t$, then $X_t$ is bounded above
or below by $a$. By continuity of paths, $X_0 = a$ and so $\E(X_0) = a$ which in
turn implies that $\E(X_t) = a$. Suppose that $X_t \geq a$:

$$
\begin{align}
0 &= \E(X_t -a) \\
&= \int_{0}^{\infty} \P(X_t -a > x) dx \\
\end{align}
$$
and so we must have $X_t = a$, almost surely.

On the other hand, if $X_t < a$, then
$$
\begin{align}
0 &= \E(a - X_t) \\
&= \int_{0}^{\infty} \P(a - X_t > x) dx \\
\end{align}
$$
and so, again, $X_t = a$, almost surely.

So, $\rho$ is only a stopping time for $X_t$ if $X_t = a$.


### Dominated Convergence Theorem

```{mermaid}
graph TD
  A[Convergence Almost Surely] -->|Dominated Convergence| B[Convergence in Probability]
  C[L^2 Convergence] -->|Chebyshev Inequality| B
  B --> D[Convergent Sum of Probabilities]
  D -->A
```


# Numerical Projects and Exercises

## 4.1 Simulating Martingales

(a) $B_t^2 -t$, $t \in [0, 1]$

```{python}
import numpy as np
import matplotlib.pyplot as plt

N=10
steps = 100

def brownian(steps, start=0, stop=1):
    variance = (stop - start)/steps
    return np.r_[0, np.cumsum(np.random.default_rng().normal(0, np.sqrt(variance), steps-1))]

for _ in range(10):
  plt.plot(brownian(1000, 0, 10))
plt.show()
```

```{python}
t = np.linspace(0, 1, steps)
for _ in range(N):
    plt.plot(t, brownian(steps)**2 - t)

plt.title(f'{N} $B_t^2 -t$ samples')
plt.show()
```
(b) $S_t = e^{B_t - t/2}$, $t \in [0, 1]$

```{python}
for _ in range(N):
  plt.plot(t, np.exp(brownian(steps) - t/2))

plt.title(f'{N} $e^{{B_t - t/2}}$ samples')
plt.show()
```

(c) $N_t - t$, $t \in [0, 1]$ where $N_t$ is a Poisson process of rate 1

```{python}
def poisson(steps, start=0, stop=1):
    rate = (stop -start)/steps
    increments = np.random.default_rng().poisson(rate, steps)
    return np.cumsum(increments)

for _ in range(N):
    plt.plot(t, poisson(steps) - t)

plt.title(f'{N} $N_t - 1$ samples')
plt.show()
```

## 4.2 Maximum of Brownian Motion

```{python}
def max_brownian(steps, start=0, stop=1):
    return np.max(brownian(steps, start, stop))

samples = 10000
maxb_samples = [max_brownian(steps) for _ in range(samples)]

print(maxb_samples.count(0)/samples)

plt.hist(maxb_samples, density=True, bins=np.logspace(0, 1, 1000)-5)
s = np.linspace(0, 4, 1000)
plt.plot(s, (2/np.sqrt(2*np.pi))*np.exp(-(s**2)/2))
plt.show()
```

## 4.3 First Passage Time

$$
\tau = \min \{t \geq 0 : B_t \geq 1 \}.
$$

```{python}
def tau(steps):
    start, stop = 0, 10
    interval = (stop - start)/steps
    for i, v in enumerate(brownian(steps, start, stop)):
        if v >= 1:
            return i * interval
    return stop

tau_samples = [tau(1000) for _ in range(10000)]

plt.hist(tau_samples, density=True, bins=100)
average_tau = np.average(tau_samples)
plt.title(f'$E(\\tau\\wedge 10) \\approx {average_tau}$')
plt.show()
```

The expectation $\E(\tau \wedge 10)$ can be calculated from the distribution of $\tau$.
Since $\tau \wedge 10 \geq 0$, 

$$
\begin{align}
\E(\tau \wedge 10) &= \int_{0}^{\infty} \P(\tau \wedge 10 > x)\, dx \\
&= \int_{0}^{10} \P(\tau > x)\, dx \\
&= \int_0^{10} (1 - \P(\tau \leq x))\, dx \\
&= 10 - \int_0^{10} \P(\tau \leq x)\, dx\\
&= 10 - \int_0^{10} \int_0^{x} \frac{1}{\sqrt{2 \pi y^3}} e^{-1/2y} \,dy \,dx \\
\end{align}
$$


```{python}
import sympy as sp
from sympy.abc import x,y
import IPython.display as disp
sp.init_printing(use_latex='mathjax')

integrand = (1/ sp.sqrt(2 * sp.pi * y**3))*sp.exp(-1/ (2*y))

expectation = 10 - sp.integrate(integrand, (y, 0, x),(x, 0, 10)); expectation
disp.display(expectation)
disp.display(expectation.evalf())
```

What proportion of paths never reach 1? This can be expressed as

$$
\begin{align}
\P(\max_{t \in [0, 10]}B_t < 1) &= 1 - \P(\max_{t \in [0,10]} B_t \geq 1)\\
&= 1 - \P(\tau \leq 10) \\
&= 1 - \int_0^{10} \frac{1}{2 \pi y^3} e^{-1/2y}\, dy
\end{align}
$$
which can be caluclated with SymPy:

```{python}
proportion = 1 - sp.integrate(integrand, (y, 0, 10))
disp.display(proportion)
disp.display(proportion.evalf())
```

We can also retrieve an approximation from our simulation:

```{python}
sum(t == 10 for t in tau_samples)/len(tau_samples)
```

## Gambler's Ruin at the French Roulette

```{python}
import numpy as np
import matplotlib.pyplot as plt

def roulette_win(start=100, stop=200, p = 18/38):
  rolls = 2*np.random.default_rng().binomial(1, p, size=5000) - 1
  purse = np.cumsum(rolls) + start
  for p in purse:
    if p == 0:
      return False
    if p == stop:
      return True
  return purse[-1] >= start

print(f'Probability of winning $200 starting at $100 is ~ {sum([roulette_win() for _ in range(100)])/100}')

probability_estimates = [ sum([roulette_win(start=starting_purse) for _ in range(100)])/100 for starting_purse in range(1, 200)]

plt.plot(range(1, 200), probability_estimates)
plt.title('Probability of winning $200 starting with different purses')
plt.show()
```


# Exercises

## 4.1 Conditional Expectation of Continuous Random Variables

Let $(X, Y)$ be two random variables with joint density $f(x, y)$ on $\mathbb{R}^2$. Suppose that

$$
\int_{\mathbb{R}} f(x, y)\, dx > 0
$$
for every $y \in \mathbb{R}$.

Then $\E(Y | X) = h(X)$ where

$$
h(x) = \frac{\int_{\mathbb{R}} y f(x, y) dy}{\int_{\mathbb{R}} f(x, y) dy}.
$$

Proof:

The conditional expectation $E(Y | X)$ is the function $\eta : \mathbb{R} \to \mathbb{R}$ satisifying

$$
\E(g(X) Y) = \E(g(X)\eta(X))
$$ {#eq-conditional-orthogonal}

for any bounded random variable of the form $g(X)$ for some function $g$. That is,

$$
Y - \eta
$$
is othorgonal to $g(X)$.

We can show that $h(X) = \E(Y | X)$ by showing that it satisfies (@eq-conditional-orthogonal) and invoking the uniqueness of such a function.

Let $g$ be any function such that $g(X)$ is bounded and measurable. Using LOTUS

$$
\begin{align}
\E(g(X) Y) &= \int \int g(x) y f(x, y)\, dx\, dy \\
&= \int g(x) \left (\int y f(x, y) \, dy\,\right) dx \\
&= \int g(x) \left (\frac{\int y f(x, y) \, dy}{\int f(x, y)\, dy}\right )\left(\int f(x, y)\, dy\right) \, dx \\
&= \int \int g(x) h(x) f(x, y) \,dx\,dy \\
&= \int g(x) h(x) f_X(x) \,dx \\
&= \E(g(X) h(X)).
\end{align}
$$

Therefore $h= \E(Y|X)$. In particular, setting $g = 1$, we see that

$$
\E(\E(Y|X)) = \E(Y).
$$

## 4.2 Exercises on Sigma-Fields

(a) Let $A, B$ be two proper subset of $\Omega$ such that $A \cap B \neq \emptyset$.

Partition $\Omega$ into $4$ disjoint elements of $\sigma$:

$$
\Omega = (A\setminus B) \cup (B \setminus A) \cup (A \cap B) \cup (A \cup B)^c.
$$

To ease notation, define

$$
\begin{align}
S_0 &= A \setminus B, \\
S_1 &= B \setminus A, \\
S_2 &= A \cap B \\
S_3 &= (A \cup B)^c.
\end{align}
$$

Each element of $\sigma$ can be expressed as a union of at most $4$ of these sets: 
the number of elements of $\sigma$ is $2^4 = 16$. 

Enumerating these:

$$
\begin{align}
0000 &\to \emptyset, \\
0001 &\to A \setminus B, \\
0010 &\to B \setminus A, \\
0011 &\to (A \setminus B) \cup (B \setminus A) = (A \cup B) \setminus (A\cap B), \\
0100 &\to A \cap B, \\
0101 &\to (A \cap B) \cup (A \setminus B) = A, \\
0110 &\to (A \cap B) \cup (B \setminus A) = B, \\
0111 &\to (A \cap B) \cup (B \setminus B) \cup (A \setminus B) = A \cup B, \\
1000 &\to (A \cup B)^c, \\
1001 &\to (A \cup B)^c \cup (A \setminus B) = B^c,\\
1010 &\to (A \cup B)^c \cup (B \setminus A) = A^c, \\
1011 &\to (A \cup B)^c \cup (B \setminus A) \cup (A \setminus B) = (A \cap B)^c,\\
1100 &\to (A \cup B)^c \cup (A \cap B) = (A \setminus B)^c \cap (B \setminus A)^c, \\
1101 &\to (A \cup B)^c \cup (A \cap B) \cup (A \setminus B) = (B \setminus A)^c, \\
1110 &\to (A \cup B)^c \cup (A \cap B) \cup (B \setminus A) = (A \setminus B)^c, \\
1111 &\to \Omega.
\end{align}
$$

If $A \cap B = \emptyset$, then $\Omega$ can be paritioned into the union of
$A$, $B$, $(A \cup B)^c$: there are $2^3 = 8$ events in $\sigma$. Alternatively, we can just go through the list of $16$ above and cross off those which end up being empty or duplicate.

(b) Show that the singleton $\{b\} \in \mathcal{B}(\mathbb{R})$.

For $n \geq 0$, $I_n = (b - 1/n, b] \in \mathcal{B}(\mathbb{R})$. The countable
itersection of $\cap_{n \geq 1} I_n = \{b\}$ is also in the sigma-algebra.

All open intervals are in the Borel sigma-field: $(a, b) = (a, b] \cap \{b\}^c$.
All closed intervals are in the Borel sigma-field: $[a, b] = \{a\} \cup (a, b) \cup \{b\}$.

Is the subset $\mathbb{Q}$ a Borel set? That is, is $\mathcal{Q} \in \mathcal{B}(\mathbb{R})$? Yes: the rationals are countable, so they can be expressed as the union of singleton rational sets which are in the sigma-field $\mathcal{B}(\mathbb{R})$.


## 4.3 Another Look at Conditional Expectation For Gaussians

Let $(X, Y)$ be a Gaussian vector with mean $0$ and covariance matrix

$$
\begin{align}
\mathcal{C} &= \begin{bmatrix}
1 & \rho \\
\rho & 1.
\end{bmatrix}
\end{align}
$$

(a) The conditional expectation can be calculated using the $L^2$ best approximation form:

$$
\begin{align}
\E(Y|X) &= \frac{\E(XY) X}{\E(X^2)} \\
&= \rho X.
\end{align}
$$

(b) The joint pdf of $(X, Y)$ is

```{python}
import sympy as sp
from fractions import Fraction
x, y, rho = sp.symbols('x, y, rho', real=True)

C = sp.Matrix([[1, rho], [rho, 1]])
Cinv = C.inv()
xy = sp.Matrix([x, y])


pdf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/((2 * sp.pi)**(Fraction(len(xy),2)) * sp.sqrt(C.det()))
pdf[0]

```



$$
\begin{align}
\frac{\int_{\mathbb{R}} y f(x, y)\, dy}{\int_{\mathbb{R}} f (x, y) \, dy} &= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + x^2 + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&= \frac{\int_{\mathbb{R}} y e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy}{\int_{\mathbb{R}}e^{\frac{-2 \rho xy + y^2}{2(\rho -1)(\rho + 1)}}\, dy} \\
&= \frac{\int_{\mathbb{R}} (t + \rho x) e^{\frac{-t^2}{2(1-\rho)(1+\rho)}} \, dt}{\int_{\mathbb{R}} e^{\frac{-t^2}{2(1 -\rho)(1 + \rho)}} \, dt} \\
&= \sqrt{1 - \rho^2} \frac{\int_{\mathbb{R}} t e^{-t^2/2}\,dt}{\int_{\mathbb{R}} e^{-t^2/2}\, dt} + \rho x \\
&= \rho x = h(x).
\end{align}
$$

This is another way of arriving at
$$
\E(Y|X) = h(X).
$$

## 4.5 Gaussian Conditioning

$$
\mathcal{C} = \begin{bmatrix}
2 & 2 & 0 \\
2 & 4 & 0 \\
0 & 0 & 1 
\end{bmatrix}.
$$

It is easy to see that $\det{C} = 4$; non-zero determinant means that the vector
$(X_1, X_2, X_3 X_3)$ is non-degenerate.

From the covariance matrix, we see that $\E(X_1X_3) = \E(X_2 X_3) = 0$, so $X_3$ is
independent of $X_1$ and $X_2$.


$$
\begin{align}
\E(X_2 | X_1) &= \frac{\E(X_2X_1)}{\E(X_1^2)}X_1 \\
&= \frac{2}{2} X_1 \\
&= X_1.
\end{align}
$$

Therefore

$$
X_2 = X_1 + (X_2 - X_1)
$$

is a decomposition of $X_2$ into a linear combination of $X_1$ and a random variable
independent of $X_1$, namely $X_2 - X_1$.

## 4.6 Gaussian Conditioning

$(X, Y)$ is a Gaussian random vector with mean $0$ and covariance given by

$$
\mathcal{C} = \begin{bmatrix}
3/16 & 1/8 \\
1/8 & 1/4 \\
\end{bmatrix}.
$$

$(X, Y)$ is non-degenerate because $\det{C} > 0$:

```{python}
C = sp.Matrix([[Fraction(3, 16), Fraction(1, 8)], [Fraction(1,8), Fraction(1,4)]])
C.det()
```

$$
\E(Y|X) = \frac{\E(YX)}{\E(X^2)}X = (1/8) * (16/3)X =2X/3.
$$

$W = (Y - 2X/3)$ is independent of $X$ (it is orthogonal to all functions of $X$). We need

$$
\E(W^2) = \E(Y^2 - 4XY/3 + 4X^2/9) = \frac{1}{4} - \frac{4}{8.3} + \frac{4.3}{9.16} = \frac{1}{6}. 
$$



We can calculate the MGF of $Y$ conditioned on $X$:

$$
\begin{align}
\E(e^{aY} | X) &= \E(e^{a(W + 2X/3)} | X) \\
&= e^{2X/3} \E(e^{aW} X) \\
&= e^{2X/3} \E(e^{aW})  \\
&= e^{2X/3+ a^2/12}  \\
\end{align}
$$

and so the conditional distribution of $Y$ given $X$ is a Gaussian with mean $2X/3$ and variance $1/6$.

We define $Z_1 = 16X/3$ and $Z_2 = 6(Y - 2X/3)$: $Z_1$ and $Z_2$ are standard Gaussians and

$$
X = 16X/3
$$
and
$$
Y = Z_1/9 + Z_2/6.
$$



## 4.7 Gaussian Conditioning

$$
\mathcal{C} = \begin{bmatrix}
1 & -1 \\
-1 & 2 \\
\end{bmatrix}.
$$

$Z_1 = X_1$.

$$
\begin{align}
X_2 &= (X_2 - \E(X_2| Z_1)Z_1) + \E(X_2|Z_1) Z_1 \\
&= (X_2 + Z_1) - Z_1.
\end{align}
$$

So, set $Z_2 = X_2 + X_1$.

Checking:

$$
\E(Z_1) = \E(X_1) = 0
$$

and 

$$
\E(Z_1^2) = \E(X_1^2) = 1.
$$

$$
\E(Z_2) = \E(X_2) + \E(X_1) = 0
$$
and

$$
\begin{align}
\E(Z_2^2) &= \E(X_1^2 + 2X_1X_2  + X_2^2)  \\
&= 1 -2.1 + 2 \\
&= 1.
\end{align}
$$

Moreover,

$$
\begin{align}
\E(Z_2Z_1)  &= \E((X_1 + X_2) X_1) \\
&= \E(X_1^2 + X_1 X_2) \\
&= 1 - 1 \\
&= 0.
\end{align}
$$

$$
\E(X_2 | X_1) = \frac{\E(X_2 X_1)}{\E(X_1^2)}X_1 = -X_1.
$$

$$
\begin{align}
\E(e^{a X_2} | X_1) &= \E(e^{a (Z_2 - Z_1)}| Z_1) \\
&= e^{-a Z_1} \E(e^{a Z_2}| Z_1) \\
&= e^{-a Z_1} \E(e^{a Z_2}) \\
&= e^{-aZ_1 + a^2/2} \\
&= e^{-aX_1 + a^2/2}.
\end{align}
$$
using the independence of $Z_2$ and $Z_1$ and the MGF of a Gaussian of mean $0$ and
variance $1$. The conditional distribution of $X_2$ given $X_1$ is a Gaussian with mean $-X_1$ and variance $1$.

## 4.8 Gaussian Conditioning

We can use the Cholesky factorisation of the covariance matrix
to get a mapping from $(Z_1, Z_2, Z_3)$ of IID standard Gaussians to $(X_1, X_2, X_3)$:

```{python}
sp.Matrix([[2,1,1],[1,2,1],[1,1,2]]).cholesky()
```

$$
\begin{align}
X_1 &= \sqrt{2} Z_1, \\
X_2 &= \frac{Z_1}{\sqrt{2}} + \sqrt{\frac{3}{2}} Z_2, \\
X_3  &= \frac{Z_1}{\sqrt{2}} + \frac{Z_2}{\sqrt{6}} + \frac{2 Z_3}{\sqrt{3}}.
\end{align}
$$

We note that

$$
X_3 = X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}.
$$


This can be used to compute $\E(X_3 |X_2, X_1)$:

$$
\begin{align}
\E(X_3 | X_2, X_1) &= \E(X_2 - \frac{\sqrt{3}}{2}X_1 + \frac{2 Z_3}{\sqrt{3}}| X_2, X_1) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}|X_2, X_1) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1 + \E(\frac{2 Z_3}{\sqrt{3}}) \\
&=  X_2 - \frac{\sqrt{3}}{2} X_1,
\end{align}
$$

where we've used $E(X_1|X_1, X_2) = X_1$ and that $Z_3$ is independent of $X_1$ and $X_2$ (they are linear combinations of $Z_1$ and $Z_2$).

We can also compute $\E(e^{aX_3} | X_2, X_1)$:

$$
\begin{align}
\E(e^{aX_3}| X_2, X_1) &= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}|X_2, X_1) \\
&= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1)} \E(e^{\frac{2 a Z_3}{\sqrt{3}}}) \\
&= e^{a(X_2 -\frac{\sqrt{3}}{2}X_1 + \frac{2a^2}{3})}.
\end{align}
$$
The conditional distribution is a Gaussian with mean $X_2 - \frac{\sqrt{3}}{2}X_1$ and variance $4/3$.



