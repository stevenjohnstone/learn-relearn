---
title: "A First Course in Stochastic Calculus"
subtitle: "Chapter Three Exercises"
toc: true
toc-location: body
execute:
    freeze: true
description: "Solutions to exercises and computer experiments"
format:
    html:
        code-fold: false
        html-math-method: mathjax
---

{{< include /_macros.qmd >}}

# Numerical Projects and Exercises

## 3.1 Simulating Brownian Motion Using Increments

```{python}
import numpy as np
import matplotlib.pyplot as plt

def brownian(steps, time_interval):
    normal_samples = np.random.default_rng().normal(0, np.sqrt((time_interval[1] - time_interval[0])/steps), steps)
    return np.r_[0, np.cumsum(normal_samples)]
    

def plot_brownian(steps, time_interval):
    for _ in range(10):
        plt.plot(brownian(steps, time_interval))
    plt.title('Brownian Motion')
    plt.show()
    
plot_brownian(100, [0, 1])
plot_brownian(1000, [0, 1])
```

## 3.2 Simulating the Poisson Process

```{python}
def poisson(rate, steps, time_interval):
    poisson_samples = np.random.default_rng().poisson(rate*(time_interval[1] - time_interval[0])/steps, steps)
    return np.cumsum(poisson_samples)
    
def plot_poisson(samples, rate, steps, time_interval):
    for _ in range(samples):
        plt.plot(np.linspace(time_interval[0], time_interval[1], steps), poisson(rate, steps, time_interval))
    plt.title(f'Poisson Process, rate = {rate}')
    plt.show()

plot_poisson(10, 1, 100, [0,10])

```

## 3.3 Arcsine Law

```{python}

def proportion_positive(samples):
    positive = 0
    for s in samples:
        if s >= 0:
            positive+=1
    return positive/len(samples)
            

brownian_samples = [ proportion_positive(brownian(100, [0,1])) for _ in range(1000) ]

def pdf(x):
    return (1/np.pi) * 1 /(np.sqrt(x*(1-x)))

fig, ax = plt.subplots()
ax.hist(brownian_samples, label='histogram', density=True, bins=50)
x = np.linspace(0, 1, 1000)
ax.plot(x, pdf(x), label='pdf')
ax.legend(loc='upper right')
```

## 3.4 Arcsine Law for Ornstein-Uhlenbeck

```{python}
class OU:
    def __init__(self, samples):
        covariance = [ [ (1/2)*np.exp(-(1/samples) * np.abs(i -j))*(1 - np.exp(-(2/samples)*min(i, j))) for i in range(1, samples)] for j in range(1, samples)]
        self.__A = np.linalg.cholesky(covariance)
        self.__samples = samples
        return

    def path(self, rg = np.random.default_rng()):
        return np.r_[0, self.__A.dot(rg.normal(0, 1, self.__samples-1))]

ou = OU(100)

rg = np.random.default_rng()

ou_samples = [ proportion_positive(ou.path(rg)) for _ in range(10000)]

fig, ax = plt.subplots()
ax.hist(ou_samples, label='histogram', density=True, bins=100)
ax.legend(loc='upper right')
```

## Brownian Variations

```{python}
def variation(samples):
    return np.sum([ np.abs(samples[i+1] - samples[i]) for i in range(len(samples) - 1)])


def quadratic_variation(samples):
    return np.sum([ (samples[i+1] - samples[i])**2 for i in range(len(samples) - 1)])

brownian_samples = brownian(2**20, [0,1])
plt.plot(brownian_samples)
plt.show()

quadratic_variations = [ quadratic_variation(brownian_samples[::2**i]) for i in range(20, 0, -1) ]

plt.plot(quadratic_variations)
plt.title('Quadratic Variation')
plt.show()

variations = [ variation(brownian_samples[::2**i]) for i in range(20, 0, -1) ]

plt.plot(variations)
plt.title('Variation')
plt.show()
```

## 3.6 Simulating Brownian Motion Using LÃ©vy's Construction 

```{python}
        
def levy(N, sample_points):
    def counter(N):
        n = 1 
        j = 0
        while True:
            for k in range(2**j):
                n = 2**j + k
                if n >= N:
                    return
                yield j, k, n
            j+=1

    def Lambda(t):
        if 0 <= t <= 1/2:
            return t
        elif 1/2 <= t <= 1:
            return 1 - t
        return 0
    
    normal_samples = np.random.default_rng().normal(0, 1, N)
    
    def B(t):
        b = normal_samples[0] * t
        for j, k, n in counter(N):
            b+= 2**(-j/2) * Lambda((2**j) * t - k) * normal_samples[n]
        return b
        
    return [B(p) for p in sample_points]
    
"""
def test_levy(N):
    sum_t_1, sum_t_2 = 0, 0
    time_values = np.linspace(0, 1, 3)
    samples = 10000
    for _ in range(samples):
        _, t_1, t_2 = levy(N, time_values)
        sum_t_1 += t_1**2
        sum_t_2 += t_2**2
    return sum_t_1/samples, sum_t_2/samples
    
t_1_av, t_2_av = test_levy(100)
print(t_1_av, t_2_av) # we expect somewhere close to 0.5 and 1
"""
    
    
time_values = np.linspace(0, 1, 100)
for N in [5, 20, 100]:
    for _ in range(10):
        plt.plot(levy(N, time_values))
    plt.title(f'N={N}')
    plt.show()
```
:::{.callout-warning}
I think $\Lambda$ is incorrectly defined on page 61. Should read

$$
\Lambda(t) = \begin{cases}
t & \text{if } 0 \leq t \leq 1/2, \\ 
1 - t & \text{if } 1/2 < t \leq 1, \\ 
0 & \text{otherwise}.
\end{cases}
$$

The text is also inconsistent about the $2^{-j/2}$ factor.
:::

# Exercises
## 3.1 Brownian Moments

(a) $\E(B_t^6)$


We can use [Wick's/Isserlis' formula](https://en.wikipedia.org/wiki/Isserlis%27_theorem) to calculate this: there are
$(6  -1)!! = 15$ different pairings of the $6$-tuple $\{B_t, \ldots, B_t\}$ and so


$$
\begin{align}
\E(B_t^6) &= 15 \E(B_t^2)^3 \\ 
&= 15 t^3.
\end{align}
$$

::: {.callout-note}
In general, for a zero mean Gaussian variable $X$ and $n >= 1$

$$
\E(X^{2n}) = (2n -1)!!\E(X^2)^n.
$$

This can be seen with Wick's formula:

there are $(2n - 1)!!$ different pairings of the set of $2n$ elements $(X,\ldots, X)$ and each pairing results in $n$ pairs.
:::

(b) $\E((B_{t_2} - B_{t_1})(B_{t_3} - B_{t_2}))$ if $t_1 < t_2 < t_3$

$$
\begin{align}
\E((B_{t_2} - B_{t_1})(B_{t_3} - B_{t_2})) &= \E(B_{t_2} B_{t_3}) - \E(B_{t_2}^2) - \E(B_{t_1} B_{t_3}) + \E(B_{t_1}B_{t_2}) \\ 
&= t_2 - t_2 - t_1 + t_1 \\ 
&= 0.
\end{align}
$$

(c) $\E(B_s^2 B_t^2)$ if $s < t$.

The pairings of $(B_s, B_s, B_t, B_t)$ are

$((B_s, B_s),(B_t, B_t)), ((B_s, B_t), (B_s, B_t)), ((B_s, B_t),(B_s, B_t))$ and so by Wick's formula

$$
\begin{align}
\E(B_s^2B_t^2) &= \E(B_s^2)\E(B_t^2) + 2 \E(B_sB_t)^2 \\ 
&= st + 2 s^2.
\end{align}
$$

(d) $\E(B_s B_t^3)$ if $s < t$.

The pairings of $(B_s, B_t, B_t, B_t)$ are 
$((B_s, B_t),(B_t, B_t)), ((B_s, B_t), (B_t, B_t)), ((B_s, B_t),(B_t, B_t))$ and so by Wick's formula

$$
\begin{align}
\E(B_s B_t^3)  &= 3 \E(B_sB_t)\E(B_t^2) \\ 
&= 3st.
\end{align}
$$

(e) $\E(B_s^{100} B_t^{101})$.


$\E(B_s^{100} B_t^{101}) = 0$ because there are an odd number of
multiplicands.


## 3.2 Brownian Probabilities

(a) $\P(B_1 > 1, B_2 > 1)$

The integral is given by

```{python}
#| code-fold: true
import sympy as sp
from fractions import Fraction
from sympy.abc import x,y

sp.init_printing()
C=sp.Matrix([[1, 1], [1, 2]])


Cinv = C.inv()
xy = sp.Matrix([x, y])


f = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/(2 *sp.pi * sp.sqrt(C.det(), evaluate=False))
pdf = f[0]
integral = sp.Integral(pdf, (x, 1, sp.oo), (y, 1, sp.oo))
integral
```
A change of variables gives a finite domain of integration:

```{python}
#| code-fold: true
pdf = pdf.subs({x: 1/x, y: 1/y})*sp.diff(1/x, x)*sp.diff(1/y, y)

sp.Integral(pdf , (x, 0, 1), (y, 0, 1))
```

This can be calculated using a monte-carlo approximation:

```{python}
import numpy as np

def monte_carlo(integrand):
    samples = 10000
    sum = 0
    for _ in range(samples):
        x, y = np.random.default_rng().uniform(0, 1, 2)
        sum +=integrand(x, y)
    return sum/samples

probability = monte_carlo(sp.lambdify([x, y], pdf))
probability
```

Alternatively, we can evaluate the integral with Simpson's rule:

```{python}
from scipy.integrate import simps

def simpsons(integrand):
    x = np.linspace(0.01, 1, 1000)
    y=  np.linspace(0.01, 1, 1000)

    zz = integrand(x.reshape(-1, 1), y.reshape(1, -1))
    return simps([simps(zz_r, x) for zz_r in zz], y)
    
simpsons(sp.lambdify([x, y], pdf))
```

We can check this makes sense by simulating a number of Brownian
motion paths and checking the ratio of the samples satisfying $B_1 >1, B_2>1$ to the total number of samples.

```{python}
class Brownian:
    def __init__(self, C):
        self.__A = np.linalg.cholesky(C)
        return
    def path(self):
        n = len(self.__A[0])
        return self.__A.dot(np.random.default_rng().normal(0, 1, n))
        
brownian = Brownian(np.array(C.tolist()).astype(np.float64))


samples=10000
count = 0
for _ in range(samples):
    b_1, b_2 = brownian.path()
    if b_1 > 1 and b_2 > 1:
        count+=1

count/samples
```

We can also use the fact that $(B_1, B_2 - B_1)$ are independent:


$$
\begin{align}
\P(B_1 > 1, B_2> 1) &= \int \int_{\{x > 1, x + y > 1\}} \frac{e^{\frac{-1}{2}(x^2 + y^2)}}{2 \pi} dx \, dy.
\end{align}
$$

The domain of integration is:

```{python}
#| code-fold: true
from matplotlib.patches import Polygon
 
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.xlim(-1, 3)
plt.ylim(-1, 3)

for spine in ['top', 'right']:
    ax.spines[spine].set_visible(False)

ax.spines['left'].set_position('zero')
ax.spines['bottom'].set_position('zero')

axis = plt.gca()
axis.add_patch(Polygon([[1, 0], [1,3], [3, 3], [3,-2]], label='domain of integration'))
def f(x):
    return 1 - x
    
x = np.linspace(-1, 3, 1000)

plt.plot(x, f(x), 'k--', label='y= 1 - x')
plt.axvline(1, color='b', linestyle='--', label = 'x=1')
plt.legend(loc='upper left')
```

We can calculate in chunks.
```{python}
#| code-fold: true
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
 
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.xlim(-3, 3)
plt.ylim(-3, 3)

for spine in ['top', 'right']:
    ax.spines[spine].set_visible(False)

ax.spines['left'].set_position('zero')
ax.spines['bottom'].set_position('zero')

axis = plt.gca()
axis.add_patch(Polygon([[1, 0], [1,3], [3, 3], [3,0]], label='A'))
axis.add_patch(Polygon([[1, 0], [3,0], [3, -3]], label='B', color='red'))
plt.legend(loc='upper left')
```


```{python}
import sympy as sp
from fractions import Fraction
from sympy.abc import x,y

pdf = sp.exp(Fraction(-1, 2) *(x**2 + y**2))/(2*sp.pi)

A = sp.integrate(pdf, (x, 1, sp.oo), (y, 0, sp.oo));A

```

```{python}

B = sp.integrate(pdf, (x, 1-y, sp.oo), (y, -sp.oo, 0)); B
```

```{python}
(A + B).evalf()
```



(b) $\P(B_1 > 1, B_2 > 1, B_3 > 1)$

```{python}
#| code-fold: true
import sympy as sp
from fractions import Fraction
from sympy.abc import x,y,z

sp.init_printing()
C=sp.Matrix([[1, 1, 1], [1, 2, 2], [1, 2, 3]])


Cinv = C.inv()
xy = sp.Matrix([x, y, z])


f = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/((2 *sp.pi)**Fraction(3,2) * sp.sqrt(C.det(), evaluate=False))
pdf = f[0]
integral = sp.Integral(pdf, (x, 1, sp.oo), (y, 1, sp.oo), (z, 1, sp.oo))
integral
```

A change of variables gives a finite domain of integration:

```{python}
#| code-fold: true
pdf = pdf.subs({x: -1/x, y: -1/y, z: -1/z})*sp.diff(-1/x, x)*sp.diff(-1/y, y)*sp.diff(-1/z, z)

sp.Integral(pdf , (x, -1, 0), (y, -1, 0), (z, -1, 0))
```

```{python}
from scipy.integrate import tplquad

def integrate(integrand):
    return tplquad(integrand, -1, 0, -1, 0, -1, 0)
    
integrate(sp.lambdify([x, y, z], pdf))

```
```{python}
def monte_carlo(integrand):
    samples = 10000
    sum = 0
    for _ in range(samples):
        x, y, z = np.random.default_rng().uniform(-1, 0, 3)
        sum +=integrand(x, y, z)
    return sum/samples

probability = monte_carlo(sp.lambdify([x, y, z], pdf))
probability
```

```{python}
def decomposition():
    brownian = Brownian(np.array(C.tolist()).astype(np.float64))

    samples=10000
    count = 0
    for _ in range(samples):
        b_1 , b_2, b_3 = brownian.path()
        if b_1 > 1 and b_2 > 1 and b_3 > 1:
            count+=1
    return count/samples
    
decomposition()
```

## 3.3 Equivalence of Definition of Brownian Motion

If $X = (X_i)_{i=1}^n = (B_{t_1} - 0, B_{t_2} - B_{t_1}, \ldots, B_{t_n} - B_{t_{n-1}})$
are independent Gaussians with mean zero and variance $t_{j+1} -t_j$ for $j \leq n-1$, then the vector $Y = (Y_i)_{i=1}^n = (B_{t_1}, \ldots, B_{t_n} )$ is Gaussian with mean zero and covariance $\E(B_tB_s) = t \wedge s$.

Proof:


The linear transformation $A$ maps $X$ to $Y$:

$$
A = \begin{bmatrix}
1  &  0    &  \ldots  &  0 \\
-1 &  1    &  \ldots  &  0 \\
\vdots    & \ddots & &  \vdots \\
0  &     \ldots  &  -1      &  1 
\end{bmatrix}
$$

or

$$
A_{ij} = \begin{cases}
1 & \text{if } i=j, \\ 
-1 & \text{if } i = j-1, \\ 
0 & \text{otherwise}
\end{cases}
$$

The mean vector of $Y$ is $A\textbf{0} = \textbf{0}$. The covariance of $Y$, $C_Y$, is related to the covariance of $X$, $C_X$ by

$$
A C_X A^T = C_Y.
$$


This linear relationship can be expressed using cancellation of
the telescopic sum:
$$
Y_i = \sum_{k=1}^i X_k.
$$

For $i < j$,

$$
Y_j = Y_i + \sum_{k=i+1}^j X_k.
$$

So,

$$
Y_j Y_i = Y_i^2 + Y_i \sum_{k=i+1}^j X_k.
$$

Then, using the independence of $X$:
$$
\begin{align}
\E(Y_j Y_i) &= \E(Y_i^2) + \sum_{k=i+1}^j \E(Y_i X_k) \\
&= \E(Y_i^2) + \sum_{k=i+1}^j \E(\sum_{l=1}^i X_l X_k) \\ 
&= \E(Y_i^2).
\end{align}
$$

We can calculate the variance of $Y_j$:

$$
\begin{align}
\E(Y_j^2) & = \E((\sum_{k=1}^j X_k)^2) \\
&=  \E(\sum_{k=1}^j X_k^2 + 2 \sum_{k,l = 1}^j X_k X_l) \\ 
&= \sum_{k=1}^j \E(X_k^2) \\ 
&= \sum_{k=2}^j (t_{i_{k}} -t_{i_{k-1}}) + t_{i_1} \\ 
&= t_{i_j}
\end{align}
$$

This proves that

$\E(B_{t_i} B_{t_j}) = t_i \wedge t_j$.

## 3.4 Reflection at time $s$

For any $s \geq 0$, the process $(\tilde{B}_t, t\geq 0)$ defined by

$$
\tilde{B}_t = \begin{cases}
B_t & \text{if } t \leq s, \\
B_s - (B_t - B_s) & \text{otherwise}
\end{cases}
$$
is a Brownian motion.

Proof:

Clearly, $\tilde{B}_0 = 0$. For each continuous path, $B_t(\omega)$ we can have a continuous path $\tilde{B}(\omega)$:

$\tilde{B}_t(\omega)$ is piecewise two obviously continuous functions $B_t(\omega)$ and $2B_s(\omega) - B_t(\omega)$ which are
equal at $t = s$. Therefore, for a set of $\omega$ of probability one, $\tilde{B}_t(\omega)$ is continuous.

For any $t_1 < t_2 < \ldots < t_n$, the vector $(\tilde{B}_{t_i})_{i=1}^n$ is Gaussian of mean zero because it is a transformation of $(B_{t_i})_{i=1}^n$:

$$
\tilde{B} = A B + C
$$

where

$$
A_{ij} = \begin{cases}
1 & \text{if } i=j \text{ and } t_i \leq s, \\ 
-1& \text{if } i=j \text{ and } t_i > s, \\ 
0 & \text{otherwise}
\end{cases}
$$

and $C$ is the Gaussian vector defined
$$
C_i = \begin{cases}
2 B_s & \text{if } t_i > s, \\ 
0 & \text{otherwise.}
\end{cases}
$$
The linear transformation $AB$ results in a Gaussian vector; adding $C$ results in another Gaussian vector because the components of $C$ are in the span of the components of $B$. The mean of the result is zero.


If $t_1, t_2 \leq s$, then
$$
\begin{align}
\E(\tilde{B}_{t_1} \tilde{B}_{t_2}) &= \E(B_{t_1} B_{t_2}) \\ 
&= t_1 \wedge t_2.
\end{align}
$$

If $t_1 \leq s$ and $t_2 > s$, then
$$
\begin{align}
\E(\tilde{B}_{t_1} \tilde{B}_{t_2}) &= \E(B_{t_1} (2B_s - B_{t_2})) \\ 
&= 2 \E(B_{t_1} B_s) - \E(B_{t_1} B_{t_2}) \\ 
&= 2 t_1 - t_1 \\ 
&= t_1 \\
&= t_1 \wedge t_2.
\end{align}
$$

If $t_1, t_2 > s$, then
$$
\begin{align}
\E(\tilde{B}_{t_1} \tilde{B}_{t_2}) &= \E((2B_{s} - B_{t_1})(2B_s - B_{t_2})) \\ 
&= 4 \E(B_s^2) -2 \E(B_s B_{t_2}) -2 \E(B_s B_{t_1}) + \E(B_{t_1}B_{t_2}) \\ 
&= 4 s - 2 s - 2s  + t_1 \wedge t_2 \\ 
&= t_1 \wedge t_2.
\end{align}
$$

Therefore, for any $s, t$

$$
\E(\tilde{B}_s \tilde{B}_t) = s \wedge t.
$$

## 3.5 Time Reversal

Let $(B_t, t \geq 0)$ be a Brownian motion. The process
$(B_1 - B_{1-t}, t \in [0,1])$ has the distribution of a standard
Brownian motion on $[0, 1]$.

Proof:

$B_1 - B_{1-t} = 0$ when $t = 0$. If $\omega$ is such that $B_t(\omega)$ is continuous, then $B_{1-t}(\omega)$ is continuous since $t \mapsto 1-t$ is continuous. It follows that $B_1(\omega) - B_{1-t}(\omega)$ is continuous. Therefore, for $\omega$ in a set of probability one, $B_1(\omega) - B_{1-t}(\omega)$ is continuous.

Let $0 \leq t_1 < t_2 < \ldots < t_n \leq 1$. The vector $(B_1 -B_{1-t_i})_{i=1}^n$ is Gaussian: it is simply a Brownian Gaussian
vector written in reverse with $B_1$ added, which results in
a Gaussian vector. It is
easy to see that the mean is zero by linearity of expectation.


The covariance reveals the distribution:

$$
\begin{align}
\Cov((B_1 - B_{1-t_i}), (B_1 - B_{1-t_j})) &= \E((B_1 - B_{1-t_i})(B_1 - B_{1-t_j})) \\ 
&= \E(B_1^2) - \E(B_{1-t_i}B_1) - \E(B_{1 -t_j}B_1) + \E(B_{1-t_i} B_{1-t_j}) \\ 
&= 1 - (1-t_i) - (1-t_j) + (1-t_i) \wedge (1-t_j) \\ 
&= t_i + t_j -1 + 1 - t_i \vee t_j \\
&= t_i + t_j - t_i \vee t_j \\
& = t_i \wedge t_j.
\end{align}
$$

## 3.6 Time Inversion

(a) Let $(B_t, t \geq 0)$ be a standard Brownian motion. The process

$$
X_t = t B_{1/t} \text{ for } t > 0,
$$

has the distribution of a Brownian motion on $t > 0$.

Proof:

Let $0 < t_1 < \ldots < t_n$ and define $s_{n-i} = 1/t_i$, so that
$0 < s_1 < \ldots < s_n$. The vector $T=(B_{t_i})_{i=1}^n$ is Gaussian by assumption. $S=(\frac{1}{s_i} B_{s_i})_{i=1}^{n}$ is a
linear transformation of $T$ and so is also Gaussian with mean zero:

$$
S = \begin{bmatrix}
0 & \ldots & 0 & \frac{1}{s_1} \\
0 & \ldots & \frac{1}{s_2} & 0 \\ 
\vdots & \iddots & 0 & 0 \\
\frac{1}{s_n} & 0 & \ldots & 0
\end{bmatrix}\,T.
$$

The covariance, and hence the distribution, of $S$ can be found by simple calculation:

$$
\begin{align}
\E(t_i B_{1/t_i} t_jB_{1/t_j}) &= t_it_j \frac{1}{t_i} \wedge \frac{1}{t_j} \\ 
&= \frac{t_it_j}{t_i \vee t_j} \\ 
&= t_i \wedge t_j.
\end{align}
$$


(b) $X_t \to 0$ in $L^2$ as $t \to 0$.


Proof:


$$
\begin{align}
\| X_t \|_2 &= \| t B_{1/t} \|_2 \\ 
&= \E(t^2 B_{1/t}^2) \\ 
&= t^2 1/t \\ 
&= t \to 0
\end{align}
$$
as $t \to 0$.

(c)
$$
\lim_{t \to \infty} \frac{B_t}{t} = 0
$$
almost surely.


Proof:

We are allowed to use

$$
X_t \to 0
$$
as $t \to 0$ almost surely. Note: we didn't show this above.



$$
\lim_{t\to \infty} \frac{B_t}{t} = X_{1/t} \to 0
$$
as $t \to \infty$, almost surely.








