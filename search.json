[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "learn-relearn",
    "section": "",
    "text": "A First Course in Stochastic Calculus\n\n\nChapter Two Exercises\n\n\nSolutions to exercises and computer experiments\n\n\n\n\n \n\n\n\n\n\n\nA First Course in Stochastic Calculus\n\n\nChapter One Exercises\n\n\nSolutions to exercises and computer experiments\n\n\n\n\n \n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\nChapter Two Exercises\n\n\nSolutions to starred exercises\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "FCSC/ch2.html",
    "href": "FCSC/ch2.html",
    "title": "A First Course in Stochastic Calculus",
    "section": "",
    "text": "Let \\(U_1 \\sim U(0,1)\\) and \\(U_2 \\sim U(0,1)\\). Define random variables\n\\[\nZ_1 = \\sqrt{-2\\log(U_1)} \\cos(2 \\pi U_2)\n\\] and \\[\nZ_2 = \\sqrt{-2\\log(U_1)} \\sin(2 \\pi U_2).\n\\]\nGenerate \\(10000\\) samples of \\((Z_1, Z_2)\\) and plot the histograms of each random variable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrg = np.random.default_rng()\n\nN = 10000\nbins=100\n\nU1 = rg.uniform(0, 1, N)\nU2 = rg.uniform(0, 1, N)\n\nnormal = rg.normal(0, 1, N)\n\nZ1 = [np.sqrt(-2 * np.log(u[0])) * np.cos(2 * np.pi * u[1]) for u in zip(U1, U2)]\nZ2 = [np.sqrt(-2 * np.log(u[0])) * np.sin(2 * np.pi * u[1]) for u in zip(U1, U2)]\n\n\nplt.hist(normal, bins=bins, label='normal', alpha=0.5)\nplt.hist(Z1, bins=bins, label='Z1', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\n[]\n\n\n\n\n\n\nplt.hist(normal, bins=bins, label='normal', alpha=0.5)\nplt.hist(Z2, bins=bins, label='Z2', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\n[]\n\n\n\n\n\n\nplt.hist2d(Z1, Z2, bins=bins, density=True)\nplt.plot()\n\n[]\n\n\n\n\n\n\n\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\]"
  },
  {
    "objectID": "FCSC/ch2.html#an-example-of-uncorrelated-random-variables-that-are-not-independent",
    "href": "FCSC/ch2.html#an-example-of-uncorrelated-random-variables-that-are-not-independent",
    "title": "A First Course in Stochastic Calculus",
    "section": "2.1 An Example of Uncorrelated Random Variables that are not Independent",
    "text": "2.1 An Example of Uncorrelated Random Variables that are not Independent\nLet \\(X\\) be a standard Gaussian. Show that \\(\\Cov(X^2, X) = 0\\).\n\\[\n\\Cov(X^2, X) = \\E(X^3) - \\E(X^2)\\E(X)\n\\]\nThe standard Gaussian has odd moments equal to zero so\n\\[\n\\Cov(X^2, X) = 0 - \\E(X^2).0 = 0.\n\\]\nIf you don’t have the knowledge at your fingertips, there’s always direct calculation:\nWe already know that \\(\\E(X) = 0\\) for the standard Gaussian (it has mean \\(0\\)).\nUsing integration by parts: \\[\n\\begin{align}\n\\E(X^3) & = \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} x^3 e^{-x^2/2} dx \\\\\n&=  \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} x^2 \\frac{d}{dx}(-e^{-x^2/2}) dx \\\\\n&= -x^2 e^{-x^2/2} \\rvert_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} 2 x e^{-x^2/2} dx \\\\\n& = 2 \\E(X) = 0.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch2.html#sum-of-exponentials-is-gamma",
    "href": "FCSC/ch2.html#sum-of-exponentials-is-gamma",
    "title": "A First Course in Stochastic Calculus",
    "section": "2.2 Sum of Exponentials is Gamma",
    "text": "2.2 Sum of Exponentials is Gamma\nThe sum of \\(n\\) IID random variables with exponential distribution with parameter \\(\\lambda\\) is gamma with pdf\n\\[\n\\begin{align}\nf(x) = \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{-\\lambda x} &, x \\geq 0.\n\\end{align}\n\\tag{1}\\]\nProof:\nThe pdf of the sum of two IID random variables is the convolution of the pdfs of the summands.\nTherefore, \\[\n\\begin{align}\nf(x) &= \\int_{0}^{x} \\lambda^2 e^{-\\lambda(x - y)}e^{-\\lambda y} dy \\\\\n&= \\lambda^2 e^{-\\lambda x} \\int_{0}^{x} dy \\\\\n&= \\lambda^2 x e^{-\\lambda x}.\n\\end{align}\n\\]\nSo, it’s at least plausible.\nTo prove the result, we use the MGF of the exponential random variables \\(X\\) with parameter \\(\\lambda\\):\n\\[\n\\begin{align}\n\\E(e^{tX}) &= \\frac{\\lambda}{\\lambda -t} &, t < \\lambda.\n\\end{align}\n\\]\nLet \\(X_i\\) be a collection of IID exponential random variables with parameter \\(\\lambda\\). Then \\(Z = \\sum X_i\\) satisfies\n\\[\n\\E(e^{tZ}) = \\prod_{i=1}^n \\E(e^{tX_i}) = \\frac{\\lambda^n}{(\\lambda -t)^n}.\n\\]\nSuppose that \\(Y\\) has pdf (Equation 1), then for \\(t < \\lambda\\) we see that be repeated integration by parts\n\\[\n\\begin{align}\n\\E(e^{tY}) &= \\int_0^{\\infty} e^{tx} \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{-\\lambda x} dx \\\\\n&=    \\int_0^{\\infty} \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{(t -\\lambda) x} dx \\\\\n&= \\int_0^{\\infty} \\frac{\\lambda^n}{(n-1)!} x^{n-1} \\frac{(-1)^n}{(\\lambda -t)^n} \\frac{d^n}{dx^n}e^{(t-\\lambda)x} dx \\\\\n&= \\frac{\\lambda^n}{(n-1)!} \\frac{(-1)^n}{(\\lambda -t)^n} \\int_0^{\\infty} x^{n-1} \\frac{d^n}{dx^n}e^{(t-\\lambda)x} dx \\\\\n&= \\frac{\\lambda^n}{(n-1)!} \\frac{(-1)^n}{(\\lambda -t)^n} (-(n-1)\\int_0^{\\infty} x^{n-2} \\frac{d^{n-1}}{dx^{n-1}}e^{(t-\\lambda)x} dx) \\\\\n&= \\frac{\\lambda^n}{(\\lambda - t)^n} \\frac{(-1)^n}{(n-1)!}(-1)^{n-1} (n-1)!(e^{(t-\\lambda)x}\\rvert_0^{\\infty}) \\\\\n&= \\frac{\\lambda^n}{(\\lambda - t)^n} (-1)^{2n-1} (-1) \\\\\n&= \\frac{\\lambda^n}{(\\lambda -t)^n}.\n\\end{align}\n\\]\nThe MGF characterises the distribution of the random variable so the proof is complete."
  },
  {
    "objectID": "FCSC/ch2.html#why-sqrt2-pi",
    "href": "FCSC/ch2.html#why-sqrt2-pi",
    "title": "A First Course in Stochastic Calculus",
    "section": "2.3 Why \\(\\sqrt{2 \\pi}\\)?",
    "text": "2.3 Why \\(\\sqrt{2 \\pi}\\)?\nUsing polar coordinates\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-x^2 - y^2} dx dy &= \\int_0^{2 \\pi} \\int_0^{\\infty} r e^{-r^2} dr d\\theta \\\\\n&=  2 \\pi \\int_0^{\\infty} r e^{-r^2} dr \\\\\n&=  2 \\pi \\int_0^{\\infty} \\frac{-1}{2} \\frac{d}{dr}(e^{-r^2}) dr \\\\\n&=  - \\pi e^{-r^2} \\rvert_0^{\\infty} \\\\\n&= \\pi.\n\\end{align}\n\\]\nNow, \\[\n(\\int_{-\\infty}^{\\infty} e^{-x^2} dx)^2 =\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-x^2 - y^2} dx dy = \\pi.\n\\]\nTherefore \\[\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}.\n\\]\nI think the author may have meant for us to show that\n\\[\n\\int_{-\\infty}^{\\infty} e^{-x^2/2} dx = \\sqrt{2 \\pi}\n\\]\nwhich follows by a change of variables \\(x = y/\\sqrt{2}\\)\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx &= \\int_{-\\infty}^{\\infty} e^{-(y/\\sqrt{2})^2} \\frac{1}{\\sqrt{2}} dy \\\\\n&= \\frac{1}{\\sqrt{2}} \\int_{-\\infty}^{\\infty} e^{-y^2/2} dy = \\sqrt{\\pi}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch2.html#box-muller",
    "href": "FCSC/ch2.html#box-muller",
    "title": "A First Course in Stochastic Calculus",
    "section": "2.4 Box-Muller",
    "text": "2.4 Box-Muller\nLet \\(U_1 \\sim U(0,1)\\) and \\(U_2 \\sim U(0,1)\\). Define random variables\n\\[\nZ_1 = \\sqrt{-2 \\log(U_1)} \\cos(2 \\pi U_2)\n\\] and \\[\nZ_2 = \\sqrt{-2 \\log(U_1)} \\sin(2 \\pi U_2).\n\\]\nShow that \\(Z_1\\) and \\(Z_2\\) are independent standard Gaussians.\nChange to polar coordinates.\nNote that\n\\[\nR = \\sqrt{Z_1^2 + Z_2^2} = \\sqrt{-2 \\log(U_1)}\n\\]\nand\n\\[\n\\tan(\\Theta) = \\frac{Z_2}{Z_1} = \\tan(2 \\pi U_2).\n\\] so \\[\n\\Theta = 2 \\pi U_2\n\\].\nThe random variable \\(R\\) has CDF\n\\[\n\\begin{align}\nF_R(r) &= P(R \\leq r) \\\\\n&= P(\\sqrt{-2 \\log(U_2)} \\leq r) \\\\\n&= P(U_2 \\geq e^{-r^2/2}) \\\\\n&= 1 - P(U_2 < e^{-r^2/2})\n&= 1 - \\begin{cases}\n0 & \\text{if } e^{-r^2/2} <0, \\\\\ne^{-r^2/2} & \\text{for } 0 \\leq e^{-r^2/2} < 1, \\\\\n1 & \\text{if } e^{-r^2/2} \\geq 1\n\\end{cases} \\\\\n&= 1 - e^{-r^2/2}.\n\\end{align}\n\\]\nObviously, \\(\\Theta \\sim U(0, 2 \\pi)\\). Therefore, \\((Z_1, Z_2)\\) has the same distribution as \\((X, Y)\\) where \\(X, Y\\) are IID standard Gaussians."
  },
  {
    "objectID": "FCSC/ch2.html#marginally-gaussian-but-not-jointly-gaussian.",
    "href": "FCSC/ch2.html#marginally-gaussian-but-not-jointly-gaussian.",
    "title": "A First Course in Stochastic Calculus",
    "section": "2.5 Marginally Gaussian but not Jointly Gaussian.",
    "text": "2.5 Marginally Gaussian but not Jointly Gaussian.\nLet \\(X\\) be a standard Gaussian and define\n\\[\nY = \\begin{cases}\nX & \\text{if } |X| \\leq 1, \\\\\n-X & \\text{otherwise.}\n\\end{cases}\n\\]\n\\(Y\\) is also a standard Gaussian.\nProof:\nLet\n\\[\ng(x) = \\begin{cases}\nx & \\text{if } |x| \\leq 1, \\\\\n-x & \\text{otherwise.}\n\\end{cases}\n\\]\nThen the MGF of \\(Y\\) can be expressed (using LOTUS) as\n\\[\n\\begin{align}\n\\sqrt{2 \\pi} \\E(e^{tY}) &= \\sqrt{2 \\pi} \\E(e^tg(X)) \\\\\n&= \\int_{-\\infty}^{\\infty} e^{tg(x)} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{-\\infty}^{-1} + \\int_1^{\\infty}) e^{-t x} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{\\infty}^{1} + \\int_{-1}^{-\\infty}) - e^{t x} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{1}^{\\infty} + \\int_{-\\infty}^{-1} e^{t x} e^{-x^2/2} dx \\\\\n&= \\int_{-\\infty}^{\\infty} e^{tx} e^{-x^2/2} dx \\\\\n&= \\sqrt{2 \\pi} \\E(e^{tY}).\n\\end{align}\n\\]\nTherefore, \\(Y\\) and \\(X\\) are indentically distributed. They are definitely not independent as \\(Y\\) is a function of \\(X\\) and so we have no right to expect that \\(X + Y\\) is also Gaussian.\nTo see that \\(X +Y\\) is not Gaussian, note that its range is in \\([-2,2]\\); there are lower bounds on the tail of Gaussian tails which are non-zero.\n\nX = rg.normal(0,1, N)\nY = [x if np.abs(x) <=1 else -x for x in X]\nplt.hist(X+Y, bins=bins, label='X + Y', alpha=0.5)\nplt.hist(X, bins, label='X', alpha=0.5)\nplt.hist(Y, bins, label='Y', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\n[]\n\n\n\n\n\n\ndata=[X+Y, X, Y]\nax = plt.subplot()\nax.violinplot(data, range(len(data)), vert=False)\nax.set_yticks(range(len(data)))\nax.set_yticklabels(['X+Y', 'X', 'Y'])\nplt.plot()\n\n[]"
  },
  {
    "objectID": "FCSC/ch1.html",
    "href": "FCSC/ch1.html",
    "title": "A First Course in Stochastic Calculus",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# (a)\nN = 10000\nsamples = np.random.default_rng().uniform(0, 1, N)\n\n# (b)\n\nbins = 50\nplt.hist(samples, bins, label='Unform')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nUniform Distribution\n\n\n\n\n\n# (c)\nplt.hist(samples, bins, cumulative=True, label='CDF X ~ U(0,1)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n# Redo (b) and (c) for X^2\n\nsamples_squared = [ x**2 for x in samples]\n\n\nplt.hist(samples_squared, bins, label='PDF X^2')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\nplt.hist(samples_squared, bins, cumulative=True, label='CDF X^2')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n# (a)\nN = 10000\nsamples = np.random.default_rng().exponential(1, N)\naverages = [sample/(idx +1) for idx, sample in enumerate(np.cumsum(samples))]\n\nbins = np.linspace(0, 2, 100)\nplt.hist(averages, bins)\nplt.show()\n\n\n\n\n\nplt.plot(averages)\nplt.ylabel('average of N samples')\nplt.xlabel('N')\nplt.show()\n\n\n\n\n\n# (b)\n\ndef average(n):\n    return np.sum(np.random.default_rng().exponential(1, n))/n\n\nbins = np.linspace(-0, 2, 100)\nplt.hist([average(100) for _ in range(0, 10000)], bins, label='average(100)', alpha=0.5)\nplt.hist([average(10000) for _ in range(0, 10000)], bins, label='average(10000)', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nNote how the averages of 10000 samples have less variance than the averages of 100 samples.\n\n\n\n\ndef Y(N):\n    sum = np.sum(np.random.default_rng().exponential(1, N))\n    return (sum - N) /np.sqrt(N)\n\nsamples = 10000\n\nbins = np.linspace(-3, 3, 50)\n\n\nplt.hist([ Y(100) for _ in range(0, samples)], bins, label='Y(100)', alpha=0.5)\nplt.hist(np.random.default_rng().normal(0, 1, samples), bins, label='N(0,1)', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\ndef invF(y):\n    return np.tan((y - 0.5)*np.pi)\n\nN = 10000\nsamples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]\n\n\nbins = np.linspace(-10, 10, 100)\nplt.hist(samples, bins, alpha=0.5, label='Cauchy')\nplt.hist(np.random.default_rng().normal(0, 1, N), bins, alpha=0.5, label='normal')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\nfor i in range(0,4):\n    samples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]\n    plt.plot([ s/(idx + 1) for idx, s in enumerate(np.cumsum(samples)) ], label=f'average {i}')\n\nplt.legend(loc='upper right')\nplt.xlabel('N')\nplt.show()\n\n\n\n\n\ndef cauchy_empirical_mean(N):\n    return np.sum([ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ])/N\n\n\nbins = np.linspace(-40, 40, 1000)\nplt.hist([cauchy_empirical_mean(10) for _ in range(0, 10000)], bins, label='N=10', alpha=0.5)\nplt.hist([cauchy_empirical_mean(100) for _ in range(0, 10000)], bins, label='N=100', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nNote how the emperical means with 10 samples and 100 samples appear to be identically distributed. The Cauchy distribution has no defined mean (even though it is symmetrical about 0) so the Central Limit Theorem does not apply.\nIf we have two iid variables \\(X\\) and \\(Y\\) with Cauchy distribution i.e. with pdf\n\\[\nf(x) = \\frac{1}{\\pi} \\frac{1}{1 + x^2} dx.\n\\]\nWe can get the distribution of \\(\\frac{1}{2}(X + Y)\\) by considering the characteristic function of the distribution, \\(e^{-|t|}\\).\nThe characteristic function of \\(\\frac{1}{2}(X + Y)\\) is\n\\[\nE(e^{it(X + Y)/2}) = E(e^{itX/2}) E(e^{itY/2}) = e^{-2|t/2|} = e^{-|t|}.\n\\]\nThis tells us that \\(\\frac{1}{2}(X + Y)\\) has the same distribution as \\(X\\) and \\(Y\\). So, when we calculate empirical means of Cauchy distribution independent variables, the result does not converge to a constant plus a narrow Gaussian error: instead, we get a random variable with the same distribution the samples, regardless of how many samples we take!"
  },
  {
    "objectID": "FCSC/ch1.html#section",
    "href": "FCSC/ch1.html#section",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.1",
    "text": "1.1\nProve proposition 1.3.\n\nFinite additivity: if two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A \\cup B) = P(A) + P(B)\\)\n\nThis follows from Definition 2.3 (3) with \\(A = A_1\\), \\(B = A_2\\) and \\(A_i = \\emptyset\\) for \\(i > 2\\):\n\\[\n\\begin{align}\nP(A \\cup B) &= P (\\cup_{i} A_i) = P(A_1) + P(A_2) + \\cup_{i > 2} P(A_i) \\\\\n            &= P(A) + P(B) + \\cup_{i > 2} P(\\emptyset) \\\\\n            &= P(A) + P(B).\n\\end{align}\n\\]\n\nFor any event \\(A\\), \\(P(A^c) = 1 - P(A)\\):\n\n\\(\\Omega\\) is the disjoint union of \\(A\\) and \\(A^c\\). By (1) above \\[\n1 = P(\\Omega) = P(A \\cup A^c) = P(A) + P(A^c)\n\\] so \\[\nP(A) = 1 - P(A^c).\n\\]\n\nFor any events \\(A\\), \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\):\n\nExpress \\(A \\cup B = (A\\setminus B) \\cup B\\) where the union is disjoint. Using (1)\n\\[\nP(A \\cup B) = P(A\\setminus B) + P(B).\n\\] Now, \\[\nP(A\\setminus B) + P(A\\cap B) = P(A)\n\\] and so \\[\nP(A\\setminus B) = P(A) - P(A\\cap B).\n\\]\nIt follows that\n\\[\nP(A \\cup B) = P(A \\setminus B) + P(B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\nMonotonicity: If \\(A \\subseteq B\\), then \\(P(A) \\leq P(B)\\).\n\nIf \\(A \\subseteq B\\), then \\(B = A \\cup (B \\setminus A)\\) where the union is disjoint. It follows that\n\\[\nP(B) = P(A) + P(B \\setminus A) \\geq P(A).\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#equiprobability",
    "href": "FCSC/ch1.html#equiprobability",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.2 Equiprobability",
    "text": "1.2 Equiprobability\nLet \\(\\Omega\\) be a sample space with a finite number of outcomes. We define\n\\[\nP(A) = \\#A/\\#\\Omega\n\\] for \\(A \\subseteq \\Omega\\).\n\\(P\\) is a probability on \\(\\Omega\\).\nProof:\n\nSince counts are positive, \\(P >= 0\\). Since \\(\\#A \\leq \\#\\Omega\\), \\(P \\leq 1\\) and so \\(P(A) \\in [0, 1]\\) for \\(A \\subseteq \\Omega\\).\nThe empty set \\(\\emptyset\\) has no elements, so \\(P(\\emptyset) = 0\\). It is easy to see that \\[\nP(\\Omega) = \\frac{\\# \\Omega}{ \\# \\Omega} =1.\n\\]\nAdditivity: for any infinite, mutually disjoint sequence of events \\(A_1, A_2, \\ldots\\) there exists \\(N\\) such that \\(A_n = \\emptyset\\) for all \\(n \\geq N\\). In fact, \\(N\\) must be less than \\(\\# \\Omega\\).\n\nThen\n\\[\n\\begin{align}\nP(\\cup_{i=1}^{\\infty} A_n) &= P(\\cup_{i=1}^{N} A_n)\\\\\n&= \\frac{\\sum_{i=1}^N \\# A_n}{\\# \\Omega} \\\\\n&= \\sum_{i=1}^N P(A_n) \\\\\n&= \\sum_{i=1}^{\\infty} P(A_n).\n\\end{align}\n\\]."
  },
  {
    "objectID": "FCSC/ch1.html#distribution-as-a-probability-on-mathbbr",
    "href": "FCSC/ch1.html#distribution-as-a-probability-on-mathbbr",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.3 Distribution as a Probability on \\(\\mathbb{R}\\)",
    "text": "1.3 Distribution as a Probability on \\(\\mathbb{R}\\)\nLet \\(\\rho_X\\) be the distribution of random variables \\(X\\) on some probability space \\((\\Omega, \\mathcal{F}, P)\\). \\(\\rho_X\\) has the properties of a probability on \\(\\mathbb{R}\\).\nProof:\nLet \\(\\mathcal{F}_X\\) be the \\(\\sigma\\)-field of set in \\(\\mathbb{R}\\) such that \\(A \\in \\mathcal{F} \\iff \\{\\omega \\in \\Omega: X(\\omega) \\in A\\} \\in \\mathcal{F}\\).\nWe define \\(P_X\\) on \\(\\mathcal{F}_X\\) by\n\\[\nP_X(A) = P(\\{\\omega \\in \\Omega: X(\\omega) \\in A\\}).\n\\]\nWe note that \\(P_X\\) extends \\(\\rho_X\\).\nFor notational simplicity, we write\n\\[\nX^{-1}(A) = \\{\\omega \\in \\Omega: X(\\omega) \\in A\\}.\n\\]\nClearly, for any \\(A \\in \\mathcal{F}_X\\), \\(P_X(A) = P(X^{-1}(A)) \\in [0,1]\\), so satisfies (1) of Definition 1.2.\nThe pre-image of \\(\\emptyset\\), \\(X^{-1}(\\emptyset)\\) must be itself empty. Therefore,\n\\[\nP_X(\\emptyset) = P(\\emptyset) = 0.\n\\]\nSimilarly, the pre-image of \\(\\mathbb{R}\\) must be all of \\(\\Omega\\) and so\n\\[\nP_X(\\mathbb{R}) = P(\\Omega) = 1.\n\\]\nThis shows that \\(P_X\\) satisfies (2) of Definition 2.1.\nLet \\(A_1, A_2, \\ldots\\) be an infinite sequence of events in \\(\\mathcal{F}_X\\) that are mutually disjoint. Note that the sequence of pre-images \\(X^{-1}(A_1), X^{-1}(A_2), \\ldots\\) are also mutually disjoint. Since \\(P\\) is a probability we can use its additivity to prove the additivity of \\(P_X\\):\n\\[\nP_X(\\cup_{i=1}^{\\infty} A_i) = P(\\cup_{i=1}^{\\infty} X^{-1}(A_i)) = \\sum_{i=1}^{\\infty} P(X^{-1}(A_i)) = \\sum_{i=1}^{\\infty} P_X(A_i).\n\\]\nThis shows that \\(P_X\\) satisfies (3) of Definition 2.1."
  },
  {
    "objectID": "FCSC/ch1.html#distribution-of-an-indicator-function",
    "href": "FCSC/ch1.html#distribution-of-an-indicator-function",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.4 Distribution of an Indicator Function",
    "text": "1.4 Distribution of an Indicator Function\nLet \\((\\Omega, \\mathcal{F}, P)\\) and \\(A \\in \\mathcal{F}\\) with \\(0 < P(A) < 1\\). What is the distribution of \\(1_A\\)?\nObserve that \\(P(1_A \\leq x) = 0\\) when \\(x < 0\\) because the indicator is a non-negative function. When \\(0 \\leq x < 1\\), \\(1_A(\\omega) > x\\) for all \\(\\omega \\in A\\) but \\(1_A(\\omega) = 0 <= x\\) for \\(\\omega \\in A^c\\). It follows that \\[P(1_A \\leq x) = P(\\{\\omega \\in \\Omega : 1_A(\\omega) \\leq x\\}) =P(\\{\\omega \\in \\Omega : \\omega \\in A^c\\}) = P(A^c).\\] For \\(x \\geq 1\\), \\(1_A \\leq x\\) is true for all values of \\(\\omega \\in \\Omega\\) because the maximum value of the indicator function is \\(1\\). Therefore, with \\(F\\) denoting the CDF of \\(1_A\\): \\[\nF(x) = P(1_A \\leq x) = \\begin{cases}\n0 & \\text{if } x < 0,\\\\\nP(A^c) & \\text{if } 0 \\leq x < 1,\\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#events-of-probability-one",
    "href": "FCSC/ch1.html#events-of-probability-one",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.5 Events of Probability One",
    "text": "1.5 Events of Probability One\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(A_1, A_2, \\ldots\\) be a sequence of events in \\(\\mathcal{F}\\) such that \\(P(A_n) = 1\\) for all \\(n \\geq 1\\). We show that\n\\[\nP(\\cap_{i\\geq 1} A_n) = 1.\n\\]\nDefine \\(B_n = A_n^c\\) for all \\(n \\geq 1\\). We note that\n\\[\nP(B_n) = 1 - P(A_n) = 0\n\\] for all \\(n \\geq 0\\).\nDefine a sequence of events \\(C_n = \\cup_{i =1}^n B_i\\) and note that the sequence is increasing. Continuity of probability gives\n\\[\n\\lim_{n \\to \\infty} P(C_n) = P(\\cup_{i=1}^{\\infty} B_i).\n\\]\nUsing \\(P(A \\cup B) \\leq P(A) + P(B)\\), we see that\n\\[\nP(C_n) \\leq \\sum_{i = 1}^n P(B_i) = 0\n\\]\nand so \\(P(C_n) = 0\\) and \\[\\lim_{n \\to \\infty} P(C_n) = P(\\cup_{i=1}^{\\infty} B_i) = 0\\].\nTo finish the proof, we note that\n\\[\\begin{align}\nP(\\cap_{i=1}^{\\infty} A_i) &= 1 - P((\\cap_{i=1}^{\\infty} A_i)^c)\\\\\n                           &= 1 - P( \\cup_{i =1}^{\\infty} A_i^c)\\\\\n                           &= P(\\cup_{i=1}^{\\infty} B_i) = 0.\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1.html#constructing-a-random-variable-from-another",
    "href": "FCSC/ch1.html#constructing-a-random-variable-from-another",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.6 Constructing a Random Variable from Another",
    "text": "1.6 Constructing a Random Variable from Another\nLet \\(X\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\) this is uniformly distributed on \\([-1, 1]\\). Define \\(Y = X^2\\).\n\nFind the CDF of Y and plot its graph.\n\nFirst, the CDF:\nLet \\(F\\) denote the CDF of \\(Y\\) and \\(F_X\\) denote the CDF of X.\nThen \\[\nG(x) = P(Y \\leq x) = P(X^2 \\leq x) = P(X \\leq \\sqrt{x}) = F(\\sqrt{x}).\n\\]\nNow,\n\\[\nF(x) =\n\\begin{cases}\n0 & \\text{if } x \\leq 0, \\\\\nx & \\text{if } 0 < x < 1, \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nI’ve taken \\(F\\) to be different to that in the book (the first case in the book uses \\(x < 0\\)) so that the resulting \\(G\\) has a well-defined derivative at zero.\n\n\n\nand so\n\\[\nG(x) =\n\\begin{cases}\n0 & \\text{if } x <= 0, \\\\\n\\sqrt{x} & \\text{if } 0 < x < 1, \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]\nWe plot this below:\n\ndef G(x):\n    if x <= 0:\n        return 0\n    elif x > 0 and x < 1:\n        return np.sqrt(x)\n    return 1\n\nxticks = np.linspace(-1, 1.5, 100)\nplt.plot(xticks, [G(x) for x in xticks])\n\n\n\n\n\nthe PDF of Y is given by\n\n\\[\np(y) = \\frac{dG}{dy}(y) = \\begin{cases}\n0 & \\text{if } x \\leq 0, \\\\\n\\frac{1}{2\\sqrt{x}} & \\text{if } 0 < x < 1, \\\\\n0 & \\text{if } 0 \\geq 1.\n\\end{cases}\n\\]\nThe plot is below\n\n\nCode\ndef p(x):\n    if x <= 0:\n        return 0\n    elif x > 0 and x < 1:\n        return (1/( 2 * np.sqrt(x)))\n    return 1\n\nxticks = np.linspace(-1, 0, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nxticks = np.linspace(0.01, 0.99, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nxticks = np.linspace(1, 2, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nplt.show()"
  },
  {
    "objectID": "FCSC/ch1.html#sum-of-integrable-variables",
    "href": "FCSC/ch1.html#sum-of-integrable-variables",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.7 Sum of Integrable Variables",
    "text": "1.7 Sum of Integrable Variables\n\\(X\\) and \\(Y\\) are two integrable random variables on the same probability space. Then \\(aX + bY\\) is also an integrable random variable for any \\(a, b \\in \\mathbb{R}\\).\nProof:\nWe assert that \\(aX + b Y\\) is a random variable and are left to show that\n\\[\nE(\\left|aX + bY\\right|) < \\infty.\n\\]\nBy the triangle inequality,\n\\[\n\\left| aX + bY \\right| \\leq \\left| a \\right| \\left| X \\right| + \\left| b \\right| \\left| Y \\right|\n\\] and so we can conclude that\n\\[\nE(\\left|aX + bY\\right|) < \\infty.\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#jumps-and-probabilities",
    "href": "FCSC/ch1.html#jumps-and-probabilities",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.8 Jumps and Probabilities",
    "text": "1.8 Jumps and Probabilities\nLet \\(X\\) be a random variable and \\(F_X\\) be its CDF. Then\n\\[\nP(X = a) = F_X(a) - \\lim_{x \\to a-} F_X(x) = F_X(a) - F_X(a-).\n\\]\nProof:\nDefine a decreasing sequence of events \\(A_n = \\{ X \\in (a - 1/n, a]\\}\\) and note that \\(\\cap_{i=1}^{\\infty} A_n = \\{ X = a\\}\\). By continuity of probability\n\\[\n\\lim_{n \\to \\infty} P(A_n) = P(X = a).\n\\]\nNow,\n\\[\\begin{align}\nP(X = a) &= \\lim_{n \\to \\infty} P(A_n)\\\\\n         &= \\lim_{n \\to \\infty} (F_X(a) - F_X(a - 1/n))\\\\\n         &= F_X(a) - \\lim_{n \\to \\infty} (F_X(a - 1/n)) = F_X(a) - F_X(a-).\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1.html#memory-loss-property",
    "href": "FCSC/ch1.html#memory-loss-property",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.9 Memory Loss Property",
    "text": "1.9 Memory Loss Property\nLet \\(Y\\) be an exponential random variable with parameter \\(\\lambda\\). The for any \\(s, t > 0\\)\n\\[\nP(Y > t + s | Y > s) = P(Y > t).\n\\]\nProof:\nThe CDF of the exponential distribution is\n\\[\nF(t) = 1 - e^{-\\lambda t}.\n\\]\nTherefore,\n\\[\nP(Y > t) = 1 - F(t) = e^{-\\lambda t}.\n\\]\nNow\n\\[\\begin{align}\nP(Y > t + s | Y > s) & =  \\frac{P(Y > t + s \\cap Y > s)}{P(Y > s)} \\\\\n                     & =  \\frac{P(Y > t + s)}{P(Y > s)}  \\\\\n                     & =  \\frac{e^{-\\lambda(t + s)}}{e^{-\\lambda s}} \\\\\n                     & =  e^{-\\lambda t} \\\\\n                     & =  P(Y > t).\n\\end{align}\\]\nNote that \\(P(Y > t + s \\cap Y > s) = P(Y > t + s)\\) because \\(\\{ \\omega \\in \\Omega | Y(\\omega) > t + s \\} \\subseteq \\{ \\omega \\in \\Omega | Y(\\omega) > t \\}\\) and so \\(\\{ \\omega \\in \\Omega | Y(\\omega) > t + s \\} \\cap \\{ \\omega \\in \\Omega | Y(\\omega) > t \\} = \\{ \\omega \\in \\Omega | Y(\\omega) > t + s\\}\\).\n\n\n\n\n\n\nThis property characterises the exponential distribution\n\n\n\nUsing the memory property \\[\n\\begin{align}\nP(X \\leq x + h) - P(X < x) &= 1 - P(X > x + h) -1 + P(X > x) \\\\\n&= P(X > x) - P(X > x + h) \\\\\n&= P(X > x) - P(X > x + h | X > x) P(X > x) \\\\\n&= P(X > x)(1 - P(X > x + h | X > x)) \\\\\n&= P(X > x)(1 - P(X >  h)) \\\\\n&= P(X > x)(P(X \\leq h) - P( X\\leq 0)).\n\\end{align}\n\\]\nSo we can assert that the CDF \\(F\\) satisifies\n\\[\n\\begin{align}\nF'(x) &= \\lim_{h\\to 0} \\frac{P(X \\leq x +h) - P(X \\leq x)}{h} \\\\\n&= P(X > x) \\lim_{h \\to 0} \\frac{P(X \\leq h) - P(X \\leq 0)}{h} \\\\\n&= P(X > x) F'(0) \\\\\n&= (1 - F(x)) F'(0).\n\\end{align}\n\\]\nThe family of solutions to this differential equation is\n\\[\nF(x) = 1 - e^{-\\lambda x}\n\\] for \\(\\lambda > 0\\)."
  },
  {
    "objectID": "FCSC/ch1.html#gaussian-integration-by-parts",
    "href": "FCSC/ch1.html#gaussian-integration-by-parts",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.10 Gaussian Integration by Parts",
    "text": "1.10 Gaussian Integration by Parts\n\nLet \\(Z\\) be a standard Gaussian random variable. Then\n\n\\[\nE(Zg(Z)) = E(g'(Z))\n\\]\nwhen both expectations are well-defined.\nProof:\nWe can use LOTUS and integration by parts:\n\\[\\begin{align}\nE(Z g(Z)) = \\int_{-\\infty}^{\\infty} z g(z) p(z) dz  &=  \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} z g(z) e^{-z^2/2} dz \\\\\n&= \\int_{-\\infty}^{\\infty} g(z) (-\\frac{dp}{dz}) dz \\\\\n&= -g(z) p(z) \\rvert_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} g'(z) p(z) dz \\\\\n&= E(g'(Z)).\n\\end{align}\\]\nNote that\n\\[\n-g(z) p(z) \\rvert_{-\\infty}^{\\infty} = 0\n\\]\nfollows from \\(zg(z) p(z)\\) being integrable.\n\nIn particular, if \\(g(z) = z^{n + 1}\\), then \\(g'(z) = (n +1) z^{n}\\) so\n\n\\[\nE( Z g(Z)) = E(Z^{n+2}) = E((n + 1) Z^{n}).\n\\]\nWe see that\n\\[\n\\begin{align}\nE(Z^{2n}) &= (2n -1) E(Z^{2n -2})\\\\\n          &= (2n -1) (2n -3) E(Z^{2n - 4})\\\\\n          &= (2n - 1) (2n -3) \\ldots 1 E(Z^2)\\\\\n          &= (2n -1) (2n -3) \\ldots 1.\n\\end{align}\n\\]\nSimilarly,\n\\[\n\\begin{align}\nE(Z^{2n + 1}) &= (2n) E(Z^{2n -1})\\\\\n          &= (2n) (2n -2) E(Z^{2n - 3})\\\\\n          &= (2n) (2n -2) \\ldots 1 E(Z)\\\\\n          &= 0.\n\\end{align}\n\\]\nSetting \\(X = \\sigma Z\\), we find that\n\\[\nE(X^{2n}) = \\sigma^{2n} (2n -1)(2n -3) \\ldots 1.\n\\]\nWe can’t have a mean different from zero: without symmetry about zero the special structure of odd and even functions would be lost."
  },
  {
    "objectID": "FCSC/ch1.html#mgf-of-exponential-random-variables",
    "href": "FCSC/ch1.html#mgf-of-exponential-random-variables",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.11 MGF of Exponential Random Variables",
    "text": "1.11 MGF of Exponential Random Variables\nShow that for a random variable \\(X \\sim exp(\\lambda)\\)\n\\[\nE(e^{t X}) = \\frac{ \\lambda}{\\lambda - t }, t < \\lambda.\n\\]\nUsing LOTUS and choosing \\(t < \\lambda\\)\n\\[\\begin{align}\nE(e^{tX}) &= \\int_{0}^{\\infty} e^{tx} \\lambda e^{-\\lambda x} dx \\\\\n          &= \\int_0^{\\infty} e^{(t - \\lambda) x} dx \\label{exp:integral} \\tag{*} \\\\\n          &= \\frac{1}{t - \\lambda} e^{(t - \\lambda) x} \\rvert_{0}^{\\infty} \\\\\n          &= \\frac{ \\lambda}{\\lambda - t }.\n\\end{align}\\]\nNote that if \\(t \\geq \\lambda\\), then the integral \\(\\eqref{exp:integral}\\) is not well-defined.\nWe can calculate \\(E(X)\\):\n\\[\nE(X) = \\frac{d}{dt} E(e^{tX}) \\rvert_{t=0} = \\frac{\\lambda}{(\\lambda -t)^2}\\rvert_{t=0} = \\frac{1}{\\lambda}.\n\\]\nWe can also calculate \\(Var(X)\\):\n\\[\nE(X^2) = \\frac{d^2}{dt^2} E(e^{tX}) \\rvert_{t=0} = \\frac{2\\lambda}{(\\lambda -t)^3} \\rvert_{t=0} = \\frac{2}{\\lambda^2}.\n\\]\nand so\n\\[\nVar(X) = E(X^2) - E(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} =\\frac{1}{\\lambda^2}.\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#gaussian-tail",
    "href": "FCSC/ch1.html#gaussian-tail",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.12 Gaussian Tail",
    "text": "1.12 Gaussian Tail\nConsider a random variable \\(X\\) with finite MGF such that\n\\[\\begin{equation}\nE(e^{\\lambda X}) \\leq e^{\\lambda^2/2}\n\\label{gtail} \\tag{1}\n\\end{equation}\\]\nfor all \\(\\lambda \\in \\mathbb{R}\\).\nProve that for \\(a > 0\\)\n\\[\nP(X > a) \\leq e^{-a^2/2}.\n\\]\nFor \\(\\lambda > 0\\) \\[\nP(X > a) = P(e^{\\lambda X} > e^{\\lambda a})\n\\] by the monitonicity of the exponential. By Markov’s inequality and \\(\\eqref{gtail}\\) \\[\nP(e^{\\lambda X} > e^{\\lambda a}) \\leq \\frac{E(e^{\\lambda X})}{e^{\\lambda a}} \\leq e^{\\lambda^2/2 - \\lambda a}.\n\\]\nLet \\(f(\\lambda) = e^{\\lambda^2/2 - \\lambda a}\\). The minimum for \\(f\\) is found by differentiation:\n\\[\nf'(\\lambda) = (\\lambda - a) e^{\\lambda^2/2 - \\lambda a}\n\\]\nand \\(f'(\\lambda) = 0\\) is solved for \\(\\lambda = a\\). Therefore,\n\\[\nP(X > a) \\leq e^{a^2/2 - a^2} = e^{-a^2/2}.\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#expectation-from-cdf",
    "href": "FCSC/ch1.html#expectation-from-cdf",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.13 Expectation from CDF",
    "text": "1.13 Expectation from CDF\nLet \\(X\\) be a random variable such that \\(X >= 0\\). Then\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx.\n\\]\nProof:\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} (1 - F(x)) dx\n\\]\nwhere \\(F(x) = P(X <= x)\\). We can write\n\\[\n1 - F(x) = \\lim_{x \\to \\infty} F(x) - F(x) = \\int_x^{\\infty} dF(t)\n\\]\nwhere the integral is understood in the Lebesgue sense.\nNow,\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} \\int_x^{\\infty} dF(t) dx\n\\]\nand by changing the order of integration (and appealing to Fubini’s Theorem)\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} \\int_0^t dx dF(t) = \\int_0^{\\infty} t dF(t) = E(X).\n\\]\nSubtle point: we’re using the fact that X >= 0 to arrive at zero for the lower limit of the inner integral.\n\n\n\n\n\n\nConsequence: LOTUS (Law Of The Unconcious Statistician)\n\n\n\n\n\nLet \\(g: \\mathbb{R} \\mapsto \\mathbb{R}\\) be measurable. Then \\(E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) dF(x)\\).\nFirst suppose that \\(g >= 0\\). Then\n\\[\nE(g(X)) = \\int_0^{\\infty} P(g(X) > x) dx\n\\] by the result above.\nNow,\n\\[\n\\int_0^{\\infty} P(g(X) > x) dx = \\int_0^{\\infty} \\int_{\\{z: g(z) > x\\}} dF(z) dx.\n\\]\nChanging the order of integration, we see get\n\\[\n\\int_{-\\infty}^{\\infty} \\int_0^{g(z)} dx dF(z) = \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\]\nNow suppose we have general \\(g\\). Split \\(g\\) into the sum of non-negative and negative components\n\\[\ng = g_+ - g_{-}\n\\]\n\\[\n\\begin{align}\nE(g(X)) &= E(g_+(X)) - E(g_{-}(X)) \\\\\n&= \\int_{-\\infty}^{\\infty} g_+(x) dF(x) - \\int_{-\\infty}^{\\infty} g_{-}(x) dF(x) \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nConsequence: Limit of xP(X>x)\n\n\n\n\n\nIf \\(E(X_+) < \\infty\\), then \\(\\lim_{x \\to \\infty} x P(X > x) = 0\\).\nWe prove this by contradiction. Suppose that there exists \\(c > 0\\) such that \\(lim_{x \\to \\infty} xP(X >x) \\geq c\\). Then for some \\(x(c)\\), \\(xP(X>x) \\geq c\\) for all \\(x > x(c)\\) and so\n\\[\nE(X+) = \\int_0^{\\infty} P(X > x) dx \\geq \\int_{x(c)}^{\\infty} P(X > x) dx \\geq \\int_{x(c)}^{\\infty} \\frac{c}{x} dx = \\infty.\n\\]\nThis contradicts \\(E(X_+) < \\infty\\).\nContrast this with the Markov inequality which states that\n\\[\nP(X > x) <= \\frac{E(X)}{x}\n\\]\nwhich puts a bound on \\(x P(X > x)\\):\n\\[\nx P(X > x) <= E(X).\n\\]\n\n\n\nTake a random variable \\(X\\) such the \\(E(|X|) < \\infty\\). Prove that\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X \\leq x) dx.\n\\]\nDefine \\(X_+ = X 1_{X \\geq 0}\\) and \\(X_{-} = X 1_{X < 0}\\) and note that \\(X = X_+ + X_{-}\\). Then\n\\[\nE(X) = E(X_+) + E (X_{-})\n\\] by linearity of expectation. From above,\n\\[\nE(X_+) = \\int_0^{\\infty} P(X_+ > x) dx = \\int_0 ^{\\infty} P(X > x) dx.\n\\]\nSet \\(Y = -X_{-}\\). Then \\(Y > 0\\) and\n\\[\nE(Y) = \\int_0^{\\infty} P(Y > y) dy.\n\\]\nNow,\n\\[\\begin{align}\nE(X 1_{X < 0}) &= - E(- X_{-}) \\\\\n               &= - E(Y) \\\\\n               &= -\\int_0^{\\infty} P(-X_{-} >y) dy \\\\\n               &= \\int_{0}^{-\\infty} P(X_{-} < x) dx \\\\\n               &= \\int_0^{-\\infty} P(X < x) dx\n\\end{align}\\]\nThen\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X < x) dx.\n\\]\n\n\n\n\n\n\nThe result we have to prove is not generally true\n\n\n\n\\[\n\\int (P(X <= x) -P(X<x)) dx = \\int P(X = x) dx = 0\n\\]\nwhen \\(X\\) has a PDF (the CDF is at least continuous). In this case, we can state\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X \\leq x) dx.\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#characteristic-function",
    "href": "FCSC/ch1.html#characteristic-function",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.14 Characteristic Function",
    "text": "1.14 Characteristic Function\n\nNote that the expectation of a complex-valued random variable (or indeed the complex-valued random variable) is defined in Chapter 1.\n\nA good definition is\n\\[\nE(Z) = E(\\Re(Z)) + i E(\\Im(Z)).\n\\]\nNote that if \\(E(|Z|) < \\infty\\), then the real and imaginary parts of \\(Z\\) are also integrable.\nWe can extend LOTUS to complex valued functions. Suppose that \\(g: \\mathbb{R} \\to \\mathbb{C}\\) and \\(E(|g(X)|) < \\infty\\). We can decompose \\(g\\) as\n\\[\ng = g_1 + i g_2.\n\\]\nNote that \\(g_1(X)\\) and \\(g_2(X)\\) are integrable.\nwhere \\(g_1, g_2 : \\mathbb{R} \\to \\mathbb{R}\\). Then by linearity of expectation\n\\[\\begin{align}\nE(g(X)) &= E(g_1(X)) + i E(g_2(X))\\\\\n        &= \\int_{-\\infty}^{\\infty} g_1(x) dF(x) + i \\int_{-\\infty}^{\\infty} g_2(x) dF(x)\\\\\n        &= \\int_{-\\infty}^{\\infty} (g_1(x) + i g_2(x)) dF(x)\\\\\n        &= \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\end{align}\\]\nUsing this result, we see that\n\\[\nE(e^{itX}) = E(\\cos(tX)) + i E(\\sin(tX))\n\\]\nfrom the familiar Euler identity \\(e^{itx} = \\cos(tx) + i \\sin(tx)\\).\nThe same result can be argued using the Taylor series:\n\\[\\begin{align}\nE(e^{itX}) &= \\sum_{n=0}^{\\infty} E( \\frac{(itX)^n}{n!})\\\\\n           &= \\sum_{n=0}^{\\infty} E( \\frac{((-1)^{n} t^{2n} X^{2n}}{(2n)!}) + i \\sum_{i=0}^{\\infty} E( \\frac{((-1)^{n} t^{2n +1} X^{2n + 1}}{(2n + 1)!})\\\\\n           &= E(\\cos(tX)) + iE(\\sin(tX).\n\\end{align}\\]\n\nLet \\(X \\sim \\mathcal{N}(0, 1)\\). Then\n\n\\[\nE(e^{itX}) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} e^{itx} e^{-\\frac{1}{2}x^2} dx.\n\\]\nDefine \\(g(t) = E(e^{itX})\\) and try to build a differential equation we can solve which is hopefully equal to the desired result.\nUsing the Dominated Convergence Theorem, we can take differentiation inside the expectation integral (the derivative of the integrand is dominated by \\(e^{-\\frac{1}{2}x^2}\\) which is integrable).\nWe get\n\\[\\begin{align}\ng'(t) &= \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} i x e^{itx} e^{-\\frac{1}{2}x^2} dx.\n\\end{align}\\]\nUse integration by parts:\nSet \\(u = ie^{itx}\\) so \\(du = -t e^{itx}\\) and\n\\[\\begin{align}\ndv &= \\frac{1}{\\sqrt{2 \\pi}} x e^{-\\frac{1}{2}x^2}\\\\\n   &= -\\frac{1}{\\sqrt{2 \\pi}} \\frac{d}{dx}(e^{-\\frac{1}{2}x^2}).\n\\end{align}\\]\nPerforming the integration by parts we see that \\[\n\\begin{align}\ng'(t) &= \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} i x e^{itx} e^{-\\frac{1}{2}x^2} dx \\\\\n      &= -\\frac{1}{\\sqrt{2 \\pi}} ie^{itx} e^{-\\frac{1}{2}x^2} \\rvert_{-\\infty}^{\\infty} -  \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} te^{itx}  e^{-\\frac{1}{2}x^2} dx \\\\\n      &= 0 + iE(e^{itX})\\\\\n      &= -t g(t). \\label{characteristic:diffeqn} \\tag{*}\n\\end{align}\n\\] Moreover,\n\\[\ng(0) = E(e^{0}) = E(1) = 1.\n\\]\nThe unique solution to \\(\\eqref{characteristic:diffeqn}\\) is\n\\[\ng(t) = \\frac{1}{2}e^{-t^2/2}\n\\]\nand so\n\\[\nE(e^{itX}) = \\frac{1}{2}e^{-t^2/2}.\n\\]\nLet \\(Z = \\sigma X + \\mu\\) for \\(\\sigma > 0\\) and \\(\\mu \\in \\mathbb{R}\\). Then\n\\[\\begin{align}\nE(e^{itZ}) &= E(e^{it(\\sigma X + \\mu)}) \\\\\n           &= E(e^{\\sigma X} e^{it\\mu}) \\\\\n           &= e^{it\\mu}  E(e^{it\\sigma X})\\\\\n           &= e^{it\\mu}e^{-(t\\sigma)^2/2}\\\\\n           &= e^{it \\mu - \\sigma^2t^2/2}.\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1.html#when-ex-infty",
    "href": "FCSC/ch1.html#when-ex-infty",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.15 When \\(E(X) < \\infty\\)",
    "text": "1.15 When \\(E(X) < \\infty\\)\nLet \\(X \\geq 0\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\). If \\(E(X) < \\infty\\), then \\(P(X = \\infty) = 0\\).\nProof:\nLet \\(A_n = \\{\\omega \\in \\Omega: X(\\omega) > n \\}\\) for \\(n > 0\\).\nWe note that the sequence of events \\(A_n\\) is decreasing and so by continuity of probability\n\\[\n\\lim_{n\\to \\infty} P(A_n) = P(\\cap A_n) = P(X = \\infty).\n\\]\nThe proof is complete with an application of Markov’s inequality: \\[\nP(X = \\infty) = \\lim_{n \\to \\infty} P(A_n) \\leq \\lim_{n\\to\\infty} \\frac{1}{n} E(X) = 0.\n\\]"
  },
  {
    "objectID": "FCSC/ch1.html#when-ex-0",
    "href": "FCSC/ch1.html#when-ex-0",
    "title": "A First Course in Stochastic Calculus",
    "section": "1.16 When \\(E(X) = 0\\)",
    "text": "1.16 When \\(E(X) = 0\\)\nLet \\(X \\geq 0\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\). If \\(E(X) = 0\\), then \\(P(X = 0) = 1\\).\nProof:\nBy Markov’s inequality\n\\[\nP(X > 1/n) \\leq n E(X) = 0\n\\]\nand so \\(P(X > 1/n) = 0\\) for \\(n = 1, 2, \\ldots\\). It follows that\n\\[\nP(X \\leq 1/n) = 1 - P(X > 1/n) = 1.\n\\]\nThe sequence of events \\(\\{ X \\leq 1/n \\}\\) is decreasing and\n\\[\n\\{ X = 0 \\} = \\cap \\{ X \\leq 1/n \\}.\n\\]\nBy the continuity of probability\n\\[\nP(X = 0) = \\lim_{n \\to \\infty} P(X \\leq 1/n) = 1.\n\\]"
  },
  {
    "objectID": "PML/ch2.html",
    "href": "PML/ch2.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "I only cover starred exercises here\n\n\n\nFor solutions to non-starred exercises see https://probml.github.io/pml-book/solns-public.pdf.\n\n\n\nExercise 2.1\nLet \\(H \\in \\{1, \\ldots, K\\}\\) be a discrete random variable, and let \\(e_1\\) and \\(e_2\\) be the observed values of two other random variables \\(E_1\\) and \\(E_2\\).\nWe wish to calculate\n\\[\n\\vec{P}(H | e_1, e_2) = (P(H=1|e_1, e_2), \\ldots, P(H= K)).\n\\]\nFor \\(i \\in \\{1, \\ldots, K\\}\\),\n\\[\n\\begin{align}\nP(H=i|e_1, e_2) &= \\frac{P(H=i,e_1, e_2)}{P(e_1, e_2)} \\\\\n                &= \\frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)}.\n\\end{align}\n\\]\nSo, to perform the caculation, we need y - \\(P(e_1, e_2 |H = i)\\) for all \\(i \\in \\{1, \\ldots K\\}\\) i.e. \\(P(e_1, e_2 |H)\\) - \\(P(H=i)\\) for all \\(i \\in \\{1, \\ldots, K\\}\\) i.e. \\(P(H)\\) - \\(P(e_1, e_2)\\)\nSo the solution to (a) is (ii).\nNow we assume that \\(E_1 \\perp E_2 |H\\). This means that\n\\[\nP(e_1, e_2 |H=i) = P(e_1|H=i) P(e_2|H=i)\n\\] for all \\(i \\in \\{1, \\ldots, K\\}\\). With this assumption \\(\\{P(e_1, e_2), P(H), P(e_1|H), P(e_2|H)\\}\\), set (i), is now sufficient to perform the calculation:\n\\[\n\\begin{align}\nP(H=i|e_1, e_2) &= \\frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)} \\\\\n&= \\frac{P(e_1 |H=i) P(e_2|H=i) P(H=i)}{P(e_1, e_2)}\n\\end{align}\n\\] for all \\(i \\in \\{1, \\ldots, K\\}\\).\n\n\nExercise 2.3\nThe question is unclear about notation. We’ll assume that \\(p\\) will denote probability density functions i.e.\n\n\\(p(x,y|z)\\) denotes the joint distribution of \\(X\\) and \\(Y\\) conditioned on \\(Z\\)\n\\(p(x|z)\\) denotes the conditional distribution of \\(X\\) given \\(Z\\)\n\\(p(y|z)\\) denotes the conditional distribution of \\(Y\\) given \\(Z\\)\n\nSuppose there exists \\(g\\) and \\(h\\) such that\n\\[\np(x, y |z) = g(x, z) h(y, z).\n\\]\nBy definition \\[\n\\begin{align}\np(x | z) &= \\int p(x, y |z) dy \\\\\n         &= \\int g(x, z) h(y, z) dy \\\\\n         &= g(x, z) \\int h(y, z) dy.\n\\end{align}\n\\] Similarly, \\[\n\\begin{align}\np(y | z) &= \\int p(x, y |z) dx \\\\\n         &= \\int g(x, z) h(y, z) dx \\\\\n         &= h(y, z) \\int g(x, z) dx.\n\\end{align}\n\\]\nTherefore,\n\\[\n\\begin{align}\np(x |z)p (y | z) &= g(x, z) h(y, z) \\int \\int g(x, z) h(y,z) dx dy \\\\\n&= p(x, y| z) \\int \\int p(x, y |z) dx dy \\\\\n&= p(x, y | z)\n\\end{align}\n\\]\ni.e. \\(X \\perp Y | Z\\).\nThe other direction is trivial: if \\(X \\perp Y|Z\\) then we can set \\(g(x, z) = p(x | z)\\) and \\(h(y, z) = p(y |z)\\).\n\n\nExercise 2.5\nSuppose that \\(X, Y\\) are two points sampled indpendently and uniformly at random from the interval \\([0, 1]\\). What is the expected location of the leftmost point?\nBy independence, we can express the joint CDF of \\(X\\) and \\(Y\\) as follows: \\[\n\\begin{align}\nF_{X,Y}(x, y) &= P(X \\leq x, Y \\leq y) \\\\\n&= P(X \\leq x) P(Y \\leq y) \\\\\n& = \\begin{cases}\n1  & \\text{for } x, y > 1,\\\\\nx & \\text{for } x  \\in [0,1], y > 1,\\\\\ny & \\text{for } y \\in [0,1], x > 1,\\\\\nxy & \\text{for } x, y \\in [0,1],\\\\\n0  & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nThe joint probability density is then \\[\n\\begin{align}\np(x, y) &= \\frac{\\partial^2}{\\partial_x \\partial_y}F_{X,Y}(x, y) \\\\\n&= \\begin{cases}\n1 & \\text{for } x,y \\in [0,1], \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nUsing LOTUS,\n\\[\n\\begin{align}\nE(\\min(X, Y)) &= \\int_0^1 \\int_0^1 \\min(x, y) dx dy \\\\\n&= \\int_0^1 (\\int_0^y x dx + \\int_y^1 y dx) dy \\\\\n&= \\int_0^1 (y - \\frac{y^2}{2}) dy \\\\\n&= \\frac{y^2}{2} -\\frac{y^3}{6} \\rvert_0^1 \\\\\n&= \\frac{1}{2} - \\frac{1}{6} \\\\\n&=  \\frac{1}{3}.\n\\end{align}\n\\]\nAnother approach is to use the fact that for \\(Z \\geq 0\\)\n\\[\nE(Z) = \\int_0^{\\infty} P(Z > z) dz.\n\\]\nIf we set \\(Z = \\min(X, Y)\\), then\n\\[\n\\begin{align}\nE(Z) &= \\int_0^{\\infty} P(\\min(X,Y) > z) dz \\\\\n&= \\int_0^1 P(\\min(X, Y) > z) dz \\\\\n&= \\int_0^1 P(X > z, Y > z) dz \\\\\n&= \\int_0^1 P(X > z) P(Y > z) dz \\\\\n&= \\int_0^1 (1 - z)^2 dz \\\\\n&= \\frac{-(1 - z)^3}{3} \\rvert_0^1 \\\\\n&= \\frac{1}{3}.\n\\end{align}\n\\]\n\n\nExercise 2.7\nLet \\(X \\sim Ga(a, b)\\) and \\(Y = 1/X\\). Derive the distribution of \\(Y\\).\nBy definition\n\\[\nGa(x | a, b) =  = \\frac{b^a}{\\Gamma(a)} x^{a-1}e^{-xb}.\n\\]\n\\[\nP(Y \\leq y) = P(1/X \\leq y) = P(X \\geq 1/y)\n\\]\n\\[\nP(X \\geq 1/y) = 1 - P( X < 1/y) = 1 - \\int_0^{1/y} Ga(x |a, b) dx\n\\]\nTaking the derivative with respect to \\(y\\)\n\\[\n\\begin{align}\n\\frac{d}{dy} P(Y \\leq y) &=  - \\frac{d}{dy} \\int_0^{1/y} Ga(x |a, b) dx \\\\\n&= -Ga(1/y |a, b) \\frac{d}{dy}(1/y) \\\\\n&= \\frac{1}{y^2} Ga(1/y |a, b).\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nI’ve used the second fundamental theorem of calculus and chain rule\n\\[\n\\frac{d}{dx} \\int_{g(x)}^{f(x)}  h(t) dt = h(f(x))f'(x) - h(g(x))g'(x).\n\\]\n\n\n\nTherefore, the pdf of \\(Y\\) is \\[\n\\frac{b^a}{\\Gamma(a)} (1/y)^{a-1}(1/y)^2e^{-b/y} = \\frac{b^a}{\\Gamma(a)} y^{-(a+1)}e^{-b/y}\n\\]\n\n\nExercise 2.9\nLet \\(D\\) be the event that you have a disease. Let \\(+\\) be the event that you test positive for this disease and let \\(-\\) be the event that you test negative for the same disease.\nWe are told that \\(P(+|D) = P(- | D^c) = 0.99\\). It follows that\n\\[\nP(+ | D^c) = 1 - P(-| D^C) = 10^{-2}.\n\\] We are also told that \\[\nP(D) = 10^{-4}.\n\\]\nThe probability of having the disease given a positive test can be expressed using Bayes’ Theorem:\n\\[\n\\begin{align}\nP(D | +)  &= \\frac{P(+ | D) P(D)}{P(+)} \\\\\n&= \\frac{P(+ | D) P(D)}{P(+| D)P(D) + P(+|D^c)P(D^c)} \\\\\n&= \\frac{0.99.10^{-4}}{0.99.10^{-4} + 10^{-2}(1 - 10^{-4})}.\n\\end{align}\n\\]\nUsing python as a calculator\n\naccuracy= 99/100 \nprevalance=1/10000\n\nanswer= accuracy*prevalance/(accuracy*prevalance + (1-accuracy)*(1 -prevalance))\nprint(answer)\n\n0.009803921568627442\n\n\nSo, with the test for this disease, a positive test means you have about 1% chance of having the disease.\n\n\nExercise 2.11\nLet’s phrase the problem in terms of a game with coins:\nI toss two fairs coins without showing you the result.\n\nYou ask if any of the coins are heads and I respond, truthfully, “yes”: what is the probability that one coin is a tail?\nI ask you to choose a coin to be revealed: you choose and it is a head. What is the probability that the other coin is a tail?\n\nLet \\(A\\) be the event that there is at least one head and \\(B\\) denote the event that one head has been revealed. When the event \\(B\\) occurs we know that there is at least one head and so \\(B\\) also occurs i.e. \\(B \\subseteq A\\). It follows that\n\\[\nP(B) \\leq P(A).\n\\]\nThe two events do not convey the same information: \\(B\\) tells you that after random sampling, the revealed coin is heads whereas \\(A\\) tells you that after looking at both coins, I reveal that at least one is heads. So before doing any calculation I’d guess that\n\\[\nP(B) < P(A).\n\\]\nLet \\(T\\) be the event of at least one tail, \\(H\\) be the event of at least one head and subscript these to indicate if a specific coin is a head or tail e.g. \\(T_1\\) is the event that coin \\(1\\) is a tail. Denote by \\(C_i\\) the event that coin \\(i\\) is chosen.\nWe can write \\[\n\\begin{align}\nP(A) &= P(T | H) \\\\\n     &= \\frac{P(T \\cap H)}{P(H)} \\\\\n     &= \\frac{P((T_1 \\cup T_2) \\cap (H_1 \\cup H_2))}{P(H_1 \\cup H_2)} \\\\\n     &= \\frac{P((T_1 \\cap H_2) \\cup (T_2 \\cap H_1))}{P(H_1 \\cup H_2)}.\n\\end{align}\n\\]\nNow,\n\\[\nP(H_1 \\cup H_2) = 3/4\n\\] because there are \\(3\\) out of \\(4\\) equally likely outcomes that have at least one head,\nand\n\\[\n\\begin{align}\nP((T_1 \\cap H_2) \\cup (T_2 \\cap H_1)) &= P(T_1 \\cap H_2) + P(T_2 \\cap H_1) \\\\\n&= P(T_1)P(H_2) + P(T_2)P(H_1) \\\\\n&= 1/4 + 1/4 \\\\\n& = 1/2.\n\\end{align}\n\\]\nTherefore, \\[\nP(A) = \\frac{1/2}{3/4} = 2/3.\n\\]\nSimilarly,\n\\[\n\\begin{align}\nP(B) &= P(T | (H_1 \\cap C_1) \\cup (H_2 \\cap C_2)) \\\\\n     &= \\frac{P(T \\cap ((H_1 \\cap C_1) \\cup (H_2 \\cap C_2)))}{P((H_1 \\cap C_1) \\cup (H_2 \\cap C_2))} \\\\\n     &= \\frac{P((H_1 \\cap T_2 \\cap C_1) \\cup (H_2 \\cap T_1 \\cap C_2))}{P((H_1 \\cap C_1) \\cup (H_2 \\cap C_2))}.\n\\end{align}\n\\]\nWe know that the events \\(C_1\\) and \\(C_2\\) are disjoint. So\n\\[\n\\begin{align}\nP((H_1 \\cap T_2 \\cap C_1) \\cup (H_2 \\cap T_1 \\cap C_2)) &= P(H_1 \\cap T_2 \\cap C_1) + P(H_2 \\cap T_1 \\cap C_2) \\\\\n&= P(C_1 | H_1 \\cap T_2)P(H_1 \\cap T_2) + P(C_2 |H_2 \\cap T_1) P(H_2 \\cap T_1) \\\\\n&= \\frac{1}{4}  (P(C_1|H_1 \\cap T_2) + P(C_2| H_2 \\cap T_1))\n\\end{align}\n\\]\nSimilarly,\n\\[\n\\begin{align}\nP((H_1 \\cap C_1) \\cup (H_2 \\cap C_2)) &= P(H_1 \\cap C_1) + P(H_2 \\cap C_2) \\\\\n&= P(C_1 | H_1)P(H_1) + P(C_2| H_2)P(H_2) \\\\\n&= \\frac{1}{2} (P(C_1 |H_1) + P(C_2| H_2)).\n\\end{align}\n\\]\nTherefore, \\[\n\\begin{equation}\nP(B) = \\frac{1}{2} \\frac{P(C_1 | H_1 \\cap T_2) + P(C_2|H_2 \\cap T_1)}{P(C_1|H_1) + P(C_2|H_2))}\n\\end{equation}\n\\tag{1}\\] Since the choice of coin is independent of the results of coin tosses (it must be because you know nothing about the results of the coin tosses)\n\\[\nP(B) = 1/2.\n\\]\n\n\n\n\n\n\nWhat happens if I pick the coins for you?\n\n\n\nThen the choice of coin is not necessarily independent: I could, for example, only show a head if both coin tosses result in a head. Then\n\\[\nP(C_1|H_1 \\cap T_2) = P(C_2|H_2 \\cap T_1) = 0\n\\] and so, by Equation 1, \\(P(B) = 0\\).\nOn the other hand, I could always show a head if it’s available and then \\[\nP(B) = 2/3\n\\] because this is essentially the same as answering the question “are there heads”.\nNow suppose that \\(0 \\leq \\alpha \\leq 1\\) and I adopt the following scheme: with probability \\(\\alpha\\) I show you a head when one is available and with probability \\((1 - \\alpha)\\) I only show you a head if both tosses were heads. Then\n\\[\nP(B) = \\frac{2 \\alpha}{3}.\n\\]\nIn summary, when I’m free to look at the coins and decide which coin to show, I can come up with a process with results in \\(P(B)\\) being any chosen value in \\([0, 2/3]\\).\n\n\nWe can use a simulation to support our conclusion and highlight the difference between the two scenarios:\n\nimport random\n\ndef head(coin):\n    return coin == 0\n\ndef tail(coin):\n    return coin == 1\n\ndef has_heads(coin):\n    return head(coin[0]) or head(coin[1])\n\ndef has_tails(coin):\n    return tail(coin[0]) or tail(coin[1])\n\niterations = 10000\n\ntails = 0\nfor i in range(0, iterations):\n    coins = []\n    while True:\n        # keep trying until we have at least one head\n        coins= [random.randint(0,1), random.randint(0,1)]\n        if has_heads(coins):\n            break\n    if has_tails(coins):\n        tails += 1\n\nprint(tails/iterations)\n\ntails = 0\n\nfor i in range(0, iterations):\n    def head_revealed():\n        while True:\n            # keep trying tosses and random coin picks until\n            # the revealed coin is a head\n            coins= [random.randint(0,1), random.randint(0,1)]\n            coin_choice = random.randint(0,1)\n            revealed_coin = coins[coin_choice]\n            remaining_coin = coins[(coin_choice+1)%2]\n            if head(revealed_coin):\n                return remaining_coin\n    if tail(head_revealed()):\n        tails +=1\n\nprint(tails/iterations)\n\n0.6744\n\n\n0.497"
  }
]