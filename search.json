[
  {
    "objectID": "FCSC/ch5/ch5_exercises.html",
    "href": "FCSC/ch5/ch5_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\sgn}{\\operatorname{sgn}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\G}{\\mathcal{G}}\n\\newcommand{\\qed}{\\tag*{$\\square$}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#stopped-martingales-are-martingales",
    "href": "FCSC/ch5/ch5_exercises.html#stopped-martingales-are-martingales",
    "title": "Exercises",
    "section": "5.1 Stopped Martingales are Martingales",
    "text": "5.1 Stopped Martingales are Martingales\n\\((M_n, n \\in \\mathbb{N}_0)\\) is a martingale in discrete time for a filtration \\((\\F_n, n \\in \\mathbb{N}_0)\\). Let \\(\\tau\\) be a stopping time for the same filtration and let\n\\[\nX_n(\\omega) = \\begin{cases}\n+1 & \\text{if } n < \\tau(\\omega) \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nbe a martingale transform: use it to show that \\((M_{n \\wedge \\tau}, n \\in \\mathbb{N}_0)\\) is a Martingale.\nDenote the transform of \\(M\\) by \\(X.M\\):\n\\[\nX.M_n = \\sum_{k = 0}^{n-1} X_k (M_{k+1} - M_k).\n\\]\nIt is clear that \\(X.M_n\\) is measurable on \\(\\F_n\\):\n\\[\nX^{-1}\\{1\\} = \\{ \\omega: n < \\tau(\\omega) \\} \\in \\cap \\F_k\n\\] and\n\\[\nX^{-1}\\{0\\} = \\{ \\omega: n \\geq \\tau(\\omega) \\} \\in \\cap \\F_k\n\\]\nby the measurability requirements of a stopping time.\nThe random variable \\(X.M_n\\) must be in \\(L^1\\) to be a martingale:\n\\[\n\\begin{align}\n\\E(| X.M_n|) &\\leq \\sum{k=0}^{n-1} \\E(|X_k(M_{k+1} - M_k)|) \\\\\n&\\leq \\sum_{k=0}^{n-1} (\\E(|M_{k+1}|) + \\E(|M_k|)) \\\\\n< \\infty\n\\end{align}\n\\] because \\(M\\) is a martingale.\nThe martingale property of \\(X.M_n\\) can be demonstrated as follows:\nlet \\(m \\leq n\\), then\n\\[\n\\begin{align}\n\\E(X.M_n | \\F_m) &= \\sum_{k=1}^{n-1} \\E(X_k(M_{k+1} - M_k)| \\F_m) \\\\\n&= \\sum_{k = 0}^{m-1} \\E(X_k(M_{k+1} - M_k)|\\F_m) + \\sum_{k = m}^{n-1} \\E(X_k(M_{k+1} - M_k) | \\F_m) \\\\\n&= \\sum_{k = 0}^{m-1} X_k(M_{k+1} - M_k) + \\sum_{k = m}^{n-1} \\E(X_k(M_{k+1} - M_k) | \\F_m) \\\\\n&= X.M_m + \\sum_{k = m}^{n=1} (\\E(X_k M_{k+1} | \\F_m) - \\E(X_k M_k |\\F_m)) \\\\\n&= X.M_m + \\sum_{k = m}^{n=1} (\\E(X_k M_{k+1} | \\F_m) - \\E(X_k M_k |\\F_m)) \\\\\n&= X.M_m + \\sum_{k = m}^{n=1} \\underbrace{(\\E(\\E(X_k M_{k+1}|\\F_k) | \\F_m)}_{\\text{tower property}} - \\E(X_k M_k |\\F_m)) \\\\\n&= X.M_m + \\sum_{k = m}^{n=1} (\\E(\\underbrace{X_k\\E( M_{k+1}|\\F_k)}_{\\text{$X_k$ is $\\F_k$-measurable}} | \\F_m) - \\E(X_k M_k |\\F_m)) \\\\\n&= X.M_m + \\sum_{k = m}^{n=1} (\\E(\\underbrace{X_kM_k}_{\\text{martingale property of $M$}}| \\F_m) - \\E(X_k M_k |\\F_m)) \\\\\n&= X.M_m.  \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe haven’t used any specific properties of \\(X\\) here, other than the fact that it’s measurable on the filtration.\n\n\nWe need to make a connection between \\(X.M\\) and \\(M_{n \\wedge \\tau}\\):\n\\[\n\\begin{align}\nX.M_n &= \\sum_{k=0}^{n-1} X_k(M_{k+1} - M_k) \\\\\n&= \\begin{cases}\nM_{\\tau} - M_0 & \\text{for } n \\geq \\tau, \\\\\nM_n - M_0 & \\text{otherwise}\n\\end{cases} \\\\\n&= M_{n \\wedge \\tau} - M_0.\n\\end{align}\n\\]\nTherefore, \\((M_{n \\wedge \\tau}, n \\in \\mathbb{N}_0)\\) is a martingale since it is equal to a martingale plus a constant."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#itô-integral-of-a-simple-process",
    "href": "FCSC/ch5/ch5_exercises.html#itô-integral-of-a-simple-process",
    "title": "Exercises",
    "section": "5.2 Itô Integral of a Simple Process",
    "text": "5.2 Itô Integral of a Simple Process\n\\[\nI_s = \\begin{cases}\n10 B_s & s \\in [0, 1/3], \\\\\n10 B_{1/3} + 5(B_s - B_{1/3}) & s \\in (1/3, 2/3] \\\\\n10 B_{1/3} + 5(B_{2/3} - B_{1/3}) + 2( B_t - B_{2/3}) & s \\in (2/3, 1]. \\\\\n\\end{cases}\n\\]\n\n(a)\n\\(X = (I_{1/3}, I_{2/3}, I_1)\\) is a Gaussian vector.\nProof:\nWe show that \\(X\\) is a linear transformation of \\(Y = (B_{1/3}, B_{2/3}, B_1)\\) and the result follows, since \\(Y\\) is a Gaussian vector.\nBy expanding terms, we see that\n\\[\nX = A Y = \\begin{bmatrix}\n10 & 0 & 0 \\\\\n10 & 5 & 0 \\\\\n5 & 3 & 2 \\\\\n\\end{bmatrix} Y.\n\\]\n\\[\\qed\\]\n\n\n(b)\nThe covariance matrix of \\(Y\\) is\n\\[\n\\Cov(Y) = \\begin{bmatrix}\n1/2 & 1/2 & 1/2 \\\\\n1/2 & 2/3 & 2/3 \\\\\n1/2 & 2/3 & 1 \\\\\n\\end{bmatrix}.\n\\]\nThe covariance matrix of \\(X\\) is given by\n\\[\n\\Cov(X) = A \\Cov(Y) A^{T}\n\\]\nand we can offload the drudgery to sympy:\n\nimport sympy as sp\nfrom fractions import Fraction\n\na, b = Fraction(1, 3), Fraction(2, 3)\n\ncovY = sp.Matrix([[a, a, a], [ a, b, b], [a, b, 1]])\n\nA = sp.Matrix([[10, 0, 0], [5, 5, 0], [5, 3, 2]])\n\ncovX = sp.MatMul(A, covY, A.transpose())\ncovX.doit()\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{100}{3} & \\frac{100}{3} & \\frac{100}{3}\\\\\\frac{100}{3} & \\frac{125}{3} & \\frac{125}{3}\\\\\\frac{100}{3} & \\frac{125}{3} & 43\\end{matrix}\\right]\\)\n\n\n\n\n(c)\n\\[\n\\begin{align}\n\\E(I_1 B_1) &= 2 \\E(B_1^2) + 3 \\E(B_{2/3}B_1) + 5 \\E(B_{1/3}B_1) \\\\\n&= 2 + 2 + 5/3 \\\\\n&= 17/3 \\\\\n&\\neq 0 \\\\\n&= \\E(I_1) \\E(B_1).\n\\end{align}\n\\]\n\\(I_1\\) and \\(B_1\\) are not independent as they are not independent in the mean."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#convergence-in-l2-implies-covergence-of-first-and-second-moments",
    "href": "FCSC/ch5/ch5_exercises.html#convergence-in-l2-implies-covergence-of-first-and-second-moments",
    "title": "Exercises",
    "section": "5.3 Convergence in \\(L^2\\) Implies Covergence of First and Second Moments",
    "text": "5.3 Convergence in \\(L^2\\) Implies Covergence of First and Second Moments\nIf \\(X_n \\to X\\) in \\(L^2(\\Omega, \\F, \\P)\\) as \\(n \\to \\infty\\), then\n\\[\n\\E(X_n) \\to \\E(X)\n\\] and\n\\[\n\\E(X_n^2) \\to \\E(X^2)\n\\] as \\(n \\to \\infty\\).\nProof:\n\\[\n\\begin{align}\n\\E(X_n^2) &= \\E((X_n - X + X)^2) \\\\\n&= \\underbrace{\\E((X_n - X)^2)}_{\\to 0 \\text{ by assumption}} + 2 \\underbrace{\\E(X(X_n - X))}_{\\leq \\E(X^2) \\E((X_n - X)^2)} + \\E(X^2) \\\\\n&\\to \\E(X^2)\n\\end{align}\n\\] as \\(n \\to 0\\).\nUsing Jensen’s inequality and the triangle inequality\n\\[\n\\begin{align}\n|\\E(X_n) - \\E(X)| &= |\\E(X_n - X)| \\\\\n&\\leq \\E(|X_n - X|). \\\\\n\\end{align}\n\\]\nApply Jensen’s inequality again\n\\[\n\\E(|X_n - X|)^2 \\leq \\E((X_n - X)^2) \\to 0\n\\] as \\(n \\to \\infty\\). Therefore,\n\\[\n|\\E(X_n) - \\E(X)| \\to 0\n\\] as \\(n \\to \\infty\\).\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#increments-of-martingales-are-uncorrelated",
    "href": "FCSC/ch5/ch5_exercises.html#increments-of-martingales-are-uncorrelated",
    "title": "Exercises",
    "section": "5.4 Increments of Martingales are Uncorrelated",
    "text": "5.4 Increments of Martingales are Uncorrelated\n\n(a)\nFor \\(t_1 \\leq t_2 \\leq t_3 \\leq t_4\\),\n\\[\n\\E((M_{t_4} - M_{t_3})(M_{t_2} - M_{t_1})) = 0.\n\\]\nProof:\nMultiplying out brackets:\n\\[\n(M_{t_4} - M_{t_3})(M_{t_2} - M_{t_1}) = \\underbrace{M_{t_2}M_{t_4} - M_{t_2}M_{t_3}}_{A} -\\underbrace{M_{t_1}M_{t_4}  + M_{t_1}M_{t_3}}_{B}.\n\\]\nTaking each group in turn: \\[\n\\begin{align}\n\\E(A) &= \\E(M_{t_2}M_{t_4} - M_{t_2}M_{t_3}) \\\\\n&= \\E(\\E(M_{t_2}M_{t_4} - M_{t_2}M_{t_3}| \\F_{t_2})) \\\\\n&= \\E(\\underbrace{M_{t_2}}_{\\F_{t_2}-\\text{measurable}}\\E(M_{t_4} - M_{t_3}| \\F_{t_2})) \\\\\n&= \\E(M_{t_2}\\underbrace{(M_{t_2} - M_{t_2})}_{\\text{martingale property}}) \\\\\n&= 0\n\\end{align}\n\\] and\n\\[\n\\begin{align}\n\\E(B) &= \\E(M_{t_1}M_{t_4} - M_{t_1}M_{t_3}) \\\\\n&= \\E(\\E(M_{t_1}M_{t_4} - M_{t_1}M_{t_3}| \\F_{t_1})) \\\\\n&= \\E(\\underbrace{M_{t_1}}_{\\F_{t_1}-\\text{measurable}}\\E(M_{t_4} - M_{t_3}| \\F_{t_1})) \\\\\n&= \\E(M_{t_1}\\underbrace{(M_{t_1} - M_{t_1})}_{\\text{martingale property}}) \\\\\n&= 0.\n\\end{align}\n\\]\n\\[\\qed\\]\n\n\n(b)\nLet \\((B_t, t \\geq 0)\\) be a standard Brownian motion, and let \\((X_t, t \\geq 0)\\) be a process in \\(L^2_c(T)\\). For \\(t < t'\\)\n\\[\n\\E\\left( \\int_0^t X_s dB_s \\int_0^{t'} X_s dB_s\\right) = \\int_0^t \\E(X_s^2) ds.\n\\]\nProof:\nLet \\((X^{(n)}_s, s \\geq 0) \\in S(T)\\), with \\(Y_i = X_{t_i}\\) and let \\(t = t_k\\) and \\(t' = t_{k + m}\\) to make the notation easier.\n\\[\n\\begin{align}\n\\E\\left( \\int_0^t X^{(n)}_s dB_s \\int_0^{t'} X^{(n)}_s dB_s\\right) &= \\E(\\sum_{i=0}^{k-1} Y_i(B_{t_{i+1}} - B_{t_i})\\sum_{i=0}^{k+m -1} Y_i (B_{t_{i+1}} - B_{t_i})) \\\\\n&= \\E(\\sum_{i=0}^{k-1} Y_i^2 (B_{t_{i+1}} - B_{t_i})^2) \\\\\n&= \\sum_{i=0}^{k-1} \\E(\\E(Y_i^2(B_{t_{i+1}} - B_{t_i})^2|\\F_{t_i})) \\\\\n&= \\sum_{i=0}^{k-1} \\E(Y_i^2\\E((B_{t_{i+1}} - B_{t_i})^2|\\F_{t_i})) \\\\\n&= \\sum_{i=0}^{k-1} (t_{i+1} - t_i)\\E(Y_i^2) \\\\\n&= \\int_0^t \\E((X^{(n)}_s)^2) ds\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor \\(i < j\\)\n\\[\n\\begin{align}\n\\E(Y_i Y_j(M_{t_{i+1}} - M_{t_i})(M_{t_{j+1}} - M_{t_j})) &= \\E(\\E(Y_i Y_j(M_{t_{i+1}} - M_{t_i})(M_{t_{j+1}} - M_{t_j})|\\F_{t_j})) \\\\\n&= \\E(\\underbrace{Y_i Y_j(M_{t_{i+1}} - M_{t_i})}_{\\F_{t_j}\\text{-measurable}}\\underbrace{\\E(M_{t_{j+1}} - M_{t_j}|\\F_{t_j})}_{= 0 \\text{ by martingale property}}) \\\\\n&= 0.\n\\end{align}\n\\]\nThis explains why the cross-terms vanish above.\n\n\nEvery process in \\(L^2_c(T)\\) is the limit of a sequence of processes in \\(S(T)\\):\nif \\(X^{(n)}_s \\to X_s\\) in \\(L^2(\\Omega, \\F, \\P)\\) as \\(n \\to \\infty\\) for \\(s \\leq t\\), then\n\\[\n\\E\\left( \\int_0^t X^{(n)}_s dB_s \\int_0^{t'} X^{(n)}_s dB_s\\right)\n\\to \\E\\left( \\int_0^t X_s dB_s \\int_0^{t'} X_s dB_s\\right)\n\\] by definition of the Itô integral.\nMoreover, \\[\n\\sum_{i=0}^{k-1} (t_{i+1} - t_i)\\E(Y_i^2) =\n\\sum_{i=0}^{k-1} (t_{i+1} - t_i)\\E(X_{t_i}^2) \\to \\int_0^t \\E(X_s^2) ds\n\\] by definition of the Riemann integral.\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#mean-and-variance-of-martingale-transforms",
    "href": "FCSC/ch5/ch5_exercises.html#mean-and-variance-of-martingale-transforms",
    "title": "Exercises",
    "section": "5.5 Mean and Variance of Martingale Transforms",
    "text": "5.5 Mean and Variance of Martingale Transforms\n\\[\n\\begin{align}\n\\E(X.M_n) &= \\sum_{k=0}^{n-1} \\E(Y_k(M_{t_{k+1}} - M_{t_k})) \\\\\n&= \\sum_{k=0}^{n-1} \\E(\\E(Y_k(M_{t_{k+1}} - M_{t_k})|\\F_{t_k})) \\\\\n&= \\sum_{k=0}^{n-1} \\E(\\overbrace{Y_k}^{\\F_{t_k}\\text{-measurable}} \\underbrace{\\E(M_{t_{k+1}} - M_{t_k}|\\F_{t_k}}_{= 0 \\text{ by martingale property}})) \\\\\n&= 0.\n\\end{align}\n\\]\nAs in previous questions, cross terms cancel because increments are uncorrelated giving\n\\[\n\\E(X.M_n^2) = \\sum_{k=0}^{n-1} E(Y_k^2 (M_{t_{k+1}} - M_{t_k})^2).\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#geometric-brownian-motion-is-in-l2_ct",
    "href": "FCSC/ch5/ch5_exercises.html#geometric-brownian-motion-is-in-l2_ct",
    "title": "Exercises",
    "section": "5.6 Geometric Brownian Motion is in \\(L^2_c(T)\\)",
    "text": "5.6 Geometric Brownian Motion is in \\(L^2_c(T)\\)\nLet\n\\[\nM_t = \\exp(\\sigma B_t - \\sigma^2 t/2)\n\\] for \\(t \\geq 0\\) where \\((B_t)\\) is a standard Brownian motion with filtration \\((\\F_t)\\)\nThen \\(M = (M_t, t \\leq T) \\in L^2_c(T)\\).\nProof:\nWe must show that\n\n\\(M_t\\) is \\(\\F_t\\)-measurable for \\(t \\in [0, T]\\)\n\\(\\E(\\int_0^T M_t^2 dt) = \\int_0^T \\E(M_t^2) dt < \\infty\\)\n\\(t \\mapsto M_t(\\omega)\\) is continuous on \\([0, T]\\) for \\(\\omega\\) is a set of probability one.\n\n\n1.\n\\(M_t\\) is \\(\\F_t\\)-measurable because it is a continuous function of an \\(\\F_t\\)-measurable random variable, \\(B_t\\).\n\n\n2.\n\\[\n\\begin{align}\n\\int_0^T \\E(\\exp(\\sigma B_t - \\sigma^2 t/2)^2) dt &= \\int_0^T \\E(\\exp(2\\sigma B_t - \\sigma^2 t)) dt \\\\\n&= \\int_0^T \\exp(-\\sigma^2 t) \\E(\\exp(2 \\sigma B_t)) dt \\\\\n&= \\int_0^T \\exp(-\\sigma^2 t) \\underbrace{\\exp(\\sigma^2 t)}_{\\text{MGF}} dt \\\\\n&= \\int_0^T dt \\\\\n&= T < \\infty.\n\\end{align}\n\\]\n\n\n3.\nThere exists an event \\(A \\subseteq \\Omega\\) with \\(\\P(A) = 1\\), such that\n\\[\nt \\mapsto B_t(\\omega)\n\\] is continuous on \\([0, T]\\) for each \\(\\omega \\in A\\). Since \\(\\exp\\) is continuous,\n\\[\nt \\mapsto \\exp(\\sigma B_t(\\omega) -\\sigma^2 t/2)\n\\] is continuous for each \\(\\omega \\in A\\).\n\\[\\qed\\]."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#a-process-that-is-not-in-l2_ct",
    "href": "FCSC/ch5/ch5_exercises.html#a-process-that-is-not-in-l2_ct",
    "title": "Exercises",
    "section": "5.7 A Process that is not in \\(L^2_c(T)\\)",
    "text": "5.7 A Process that is not in \\(L^2_c(T)\\)\n\\((\\exp(B_t^2), t \\geq 0)\\) is not in \\(L^2_c(T)\\) for \\(T > 1/4\\).\nProof:\nIf \\(T > 1/4\\), then\n\\[\n\\int_0^T \\E(\\exp(B_t^2)^2) dt = \\infty\n\\] because\n\\[\n\\begin{align}\n\\int_0^T \\E(\\exp(B_t^2)^2) dt &= \\int_0^T \\exp(2B_t^2) dt \\\\\n&= \\int_0^T \\int_{-\\infty}^{\\infty} \\exp(2x^2) \\frac{\\exp(-x^2/2t)}{\\sqrt{2 \\pi t}} dx dt \\\\\n&= \\int_0^T \\int_{-\\infty}^{\\infty} \\frac{\\exp(x^2(2 - 1/2t))}{\\sqrt{2 \\pi t}} dx dt \\\\\n\\end{align}\n\\]\nwhere the inner intergral is infinite if\n\\[\n2 - 1/2t > 0\n\\]\ni.e.\nwhen\n\\[\nt > 1/4.\n\\]\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals",
    "href": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals",
    "title": "Exercises",
    "section": "5.8 Practice on Itô Integrals",
    "text": "5.8 Practice on Itô Integrals\n\\[\nX_t = \\int_0^t (1 -s) dB_s,\n\\]\n\\[\nY_t = \\int_0^t (1 + s) dB_s\n\\]\nfor \\(t \\geq 0\\).\n\n(a)\n\\(X_t\\) is a Wiener integral, so has mean zero and is Gaussian.\nBy the Itô isometry, \\[\n\\begin{align}\n\\E(X_tX_{t'}) &= \\int_0^{t\\wedge t'} (1 -s)^2 ds \\\\\n&= \\int_0^{t \\wedge t'} (1 - 2s + s^2) ds \\\\\n&= (t \\wedge t') - (t \\wedge t')^2 + \\frac{1}{3} (t \\wedge t')^3.\n\\end{align}\n\\]\nIn particular,\n\\[\n\\E(X_t^2) = t - t^2 + \\frac{1}{3}t^3.\n\\]\n\n\n(b)\n\\(Y_t\\) is a Wiener integral, so has mean zero and is Gaussian.\nBy the Itô isometry, \\[\n\\begin{align}\n\\E(Y_tY_{t'}) &= \\int_0^{t\\wedge t'} (1 + s)^2 ds\\\\\n&= \\int_0^{t \\wedge t'} (1 + 2s + s^2) ds \\\\\n&= (t\\wedge t') + (t \\wedge t')2 +\\frac{1}{3} (t \\wedge t')^3.\n\\end{align}\n\\]\nIn particular,\n\\[\n\\E(Y_t^2) = t + t^2 + \\frac{1}{3}t^3.\n\\]\n\n\n(c)\nBy the Itô isometry, \\[\n\\begin{align}\n\\E(X_t Y_t) &= \\int_0^t \\E((1 -s )(1 + s)) ds \\\\\n&= \\int_0^t (1 - s^2) ds \\\\\n&= t - \\frac{1}{3} t^3.\n\\end{align}\n\\]\n\\(X_t\\) and \\(Y_t\\) are uncorrelated at\n\\[\nt = \\sqrt{3}.\n\\]\nAny linear combination of \\(X_t\\) and \\(Y_t\\) is another Gaussian random variable. Observe that\n\\[\n\\begin{align}\n\\alpha X_t + \\beta Y_t = \\int_0^t \\left((\\alpha + \\beta) + (\\beta - \\alpha)s\\right) dB_s\n\\end{align}\n\\] by linearity of the Itô integrals for \\(X_t\\) and \\(Y_t\\) and any \\(\\alpha, \\beta \\in \\mathbb{R}\\). Since \\(\\alpha X_t + \\beta Y_t\\) can be expressed as a Wiener integral, it is Gaussian.\nTherefore, \\((X_t, Y_t)\\) is a Gaussian vector and so if \\(X_t\\) and \\(Y_t\\) are uncorrelated, they are independent. At \\(t = \\sqrt{3}\\), \\(\\E(X_t Y_t) = 0\\) and so \\(X_t\\) and \\(Y_t\\) are independent."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals-1",
    "href": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals-1",
    "title": "Exercises",
    "section": "5.9 Practice on Itô Integrals",
    "text": "5.9 Practice on Itô Integrals\nThe process \\((X_t, t \\geq 0)\\) is given by\n\\[\nX_t = \\int_0^t \\sin{s} d B_s.\n\\]\n\n(a)\nThis process is Gaussian.\nProof:\nThis is a Wiener process so it is Gaussian. \\[\\qed\\]\n\n\n(b)\nBy the Itô isometry \\[\n\\begin{align}\n\\E(X_t X_{t'}) &= \\int_0^{t \\wedge t'} \\E(\\sin^2 s) ds \\\\\n&= \\int_0^{t \\wedge t'} \\sin^2 s ds \\\\\n&= \\frac{s}{2} - \\frac{1}{4}\\sin{2s} \\lvert_0^{t\\wedge t'} \\\\\n&= \\frac{t \\wedge t'}{2} - \\frac{1}{4} \\sin{2(t\\wedge t')}.\n\\end{align}\n\\]\nIn particular,\n\\[\n\\E(X_{\\pi/2}^2) = \\frac{\\pi}{4},\n\\]\n\\[\n\\E(X_{\\pi}^2) = \\frac{\\pi}{2},\n\\]\nand\n\\[\nE(X_{\\pi/2} E_{\\pi}) = \\frac{\\pi}{4}.\n\\]\nThe covariance matrix for \\((X_{\\pi/2}, X_{\\pi})\\) is\n\\[\n\\frac{\\pi}{4} \\begin{bmatrix}\n1 & 1 \\\\\n1 & 2 \\\\\n\\end{bmatrix}.\n\\]\n\n\nCode\nfrom fractions import Fraction\nfrom sympy.abc import x,y\n\nC = (sp.pi/4) * sp.Matrix([[1, 1], [1, 2]])\n\nCinv = C.inv()\nxy = sp.Matrix([x, y])\n\n\nf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/(2 *sp.pi * sp.sqrt(C.det(), evaluate=False))\npdf = f[0]\nsp.Integral(pdf, (x, 1, sp.oo), (y, 1, sp.oo))\n\n\n\\(\\displaystyle \\int\\limits_{1}^{\\infty}\\int\\limits_{1}^{\\infty} \\frac{2 e^{- \\frac{2 \\cdot \\left(2 x^{2} - 2 x y + y^{2}\\right)}{\\pi}}}{\\pi^{2}}\\, dx\\, dy\\)\n\n\n\n\n(c)\nDefine a process by \\[\nY_t = \\int_0^t \\cos{s} dB_s.\n\\]\nThe covariance of \\(X_t\\) and \\(Y_t\\) can be calculated using the Itô isometry:\n\\[\n\\begin{align}\n\\E\\left(X_t Y_t \\right) &= \\int_0^t \\E(\\sin{s} \\cos{s}) ds \\\\\n&= \\int_0^t \\sin{s} \\cos{s} ds \\\\\n&= \\frac{\\sin^2(t)}{2}.\n\\end{align}\n\\]\n\\((X_t, Y_t)\\) is a Gaussian vector so \\(X_t\\) and \\(Y_t\\) are independent if and only if\n\\[\n\\E(X_t Y_t) = \\frac{\\sin^2(t)}{2} = 0.\n\\]\nTherefore, \\(X_t\\) and \\(Y_t\\) are independent for \\(t = k \\pi\\), \\(k \\in \\mathbb{N}_0\\)."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#not-everything-is-a-martingale",
    "href": "FCSC/ch5/ch5_exercises.html#not-everything-is-a-martingale",
    "title": "Exercises",
    "section": "5.10 Not Everything is a Martingale",
    "text": "5.10 Not Everything is a Martingale\n\n(a)\nThe Ornstein-Uhlenbeck process\n\\[\nY_t = e^{-t} \\int_0^t e^s dB_s\n\\] is not a martingale.\nProof:\nThe process\n\\[\nX_t = \\int_0^t e^s dB_s\n\\] is a continuous martingale (because it is defined as an Itô integral).\nFor \\(s \\leq t\\)\n\\[\n\\begin{align}\n\\E(Y_t | \\F_s) &= e^{-t} \\E(X_t | \\F_s) \\\\\n&= e^{-t} X_s \\\\\n&= e^{-t} e^{s} Y_s \\\\\n&= e^{s - t} Y_s.\n\\end{align}\n\\]\nIt is clear that \\(Y_t\\) is not a martingale because\n\\[\n\\E(Y_t | \\F_s) \\neq Y_s.\n\\]\n\\[\n\\qed\n\\]\n\n\n(b)\nThe Brownian bridge process\n\\[\nZ_t = (1 - t) \\int_0^t \\frac{1}{1-s} dB_s\n\\] for \\(t <1\\) is not a martingale.\nProof:\nThe process\n\\[\nX_t = \\int_0^t \\frac{1}{1-s} dB_s\n\\] is a martingale.\nFor \\(s \\leq t\\)\n\\[\n\\begin{align}\n\\E(Z_t |\\F_s) &= (1-t) \\E(X_t | \\F_s) \\\\\n&= (1-t) X_s \\\\\n&= \\frac{(1-t)}{(1-s)} Z_s.\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(c)\nThe process \\[\n\\left(\\int_0^t B_s ds, t \\geq 0 \\right)\n\\] is not a martingale.\nProof:\nUsing Itô’s fromula with \\(f(x) = x^3/3\\)\n\\[\n\\int_0^t B_s ds = B_t^3/3 - \\int_0^t B_s^2 dB_s = B_t^3/3 - X_t\n\\] where \\[\nX_t = \\int_0^t B_s^2 dB_s.\n\\]\n\\(X_t\\) is a martingale, so for \\(s \\leq t\\)\n\\[\n\\begin{align}\n\\E(\\int_0^t B_s ds| \\F_s) &= \\frac{1}{3}\\E(B_t^3 | \\F_s) - \\E(X_t|\\F_s) \\\\\n&= \\E((B_t - B_s + B_s)^3 | \\F_s) - X_s \\\\\n&= \\E((B_t - B_s)^3 + 3 (B_t - B_s)^2 B_s + 3(B_t - B_s) B_s^2 + B_s^3 | \\F_s) - X_s \\\\\n&= 3(t - s) B_s - X_s \\\\\n&\\neq B_s^3/3 - Xs.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals-2",
    "href": "FCSC/ch5/ch5_exercises.html#practice-on-itô-integrals-2",
    "title": "Exercises",
    "section": "5.11 Practice on Itô Integrals",
    "text": "5.11 Practice on Itô Integrals\nDefine \\[\nX_t = \\int_0^t \\sgn(B_s) dB_s\n\\] for \\(t \\geq 0\\).\n\n(a)\nThe mean of \\(X_t\\) is zero: it is a limit in \\(L^2\\) of random variables with mean zero.\nLet \\(t < t'\\). Divide the interval \\([0, t']\\) so that \\(t_0 = 0\\), \\(t = t_m\\) and \\(t' = t_n\\) and approximate \\(X_t\\) and \\(X_{t'}\\) by\n\\[\nY_t = \\sum_{j=0}^{m-1}\\sgn(B_j)(B_{j+1} - B_j)\n\\]\nand\n\\[\nY_{t'} = \\sum_{j=0}^{n-1}\\sgn(B_j)(B_{j+1} - B_j),\n\\] respectively.\nUsing the normal trick with uncorrelated increments, cross terms vanish leaving\n\\[\n\\begin{align}\n\\E(Y_t Y_{t'}) &= \\sum_{j=0}^{m - 1} \\E(\\sgn(B_i)^2(B_{j+1} - B_j)) \\\\\n&= \\sum_{j=0}^{m-1} \\E((B_{j+1} - B_j)^2) \\\\\n&= \\sum_{j=0}^{m-1} (t_{j+1} - t_j) \\\\\n&= t_{m} - t_0 \\\\\n&= t.\n\\end{align}\n\\]\nTaking limits, we see that \\[\n\\E(X_tX_{t'}) = t \\wedge t'.\n\\]\n\n\n(b)\n\\(X_t\\) and \\(B_t\\) are uncorrelated for all \\(t\\geq 0\\).\nProof:\nAssuming we’re permitted to use the Itô isometry\n\\[\n\\begin{align}\n\\E(X_t B_t) &= \\E(\\left(\\int_0^t \\sgn(B_s) dB_s\\right)\\left(\\int_0^t dB_s\\right))\\\\\n&= \\int_0^t \\underbrace{\\E(\\sgn(B_s))}_{=0 \\text{ by symmetry}} ds \\\\\n&= 0.\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(c)\n\\(X_t\\) and \\(B_t\\) are not independent.\nProof:\nWe show that \\(\\E(B_t^2 X_t) \\neq 0\\), proving that \\(X_t\\) and \\(B_t\\) are not independent:\n\\[\n\\begin{align}\n\\E(B_t^2 X_t) &= \\E(\\left(2\\int_0^t B_s dB_s +t \\right) X_t) \\\\\n&= 2 \\E((\\int_0^t B_s dB_s)( \\int_0^t \\sgn(B_s) dB_s)) \\\\\n&= \\underbrace{2 \\int_0^t \\E(B_s \\sgn(B_s)) ds}_{\\text{Itô isometry}} \\\\\n&= 2 \\int_0^t \\E(|B_s|) ds \\\\\n&= 2 \\int_0^t \\sqrt{ \\frac{2 s}{\\pi}} ds \\\\\n&= \\frac{4}{3}\\sqrt{\\frac{2}{\\pi}} t^{3/2} > 0\n\\end{align}\n\\] for \\(t > 0\\) (\\(t = 0\\) is the trivial case when \\(X_t = B_t = 0\\)).\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#integration-by-parts-from-some-itô-integrals",
    "href": "FCSC/ch5/ch5_exercises.html#integration-by-parts-from-some-itô-integrals",
    "title": "Exercises",
    "section": "5.12 Integration By Parts from Some Itô integrals",
    "text": "5.12 Integration By Parts from Some Itô integrals\nLet \\(g \\in C^2(\\mathbb{R})\\) and \\((B_t, t \\geq 0)\\) be a standard Brownian motion.\n\n(a)\nFor any \\(t \\geq 0\\)\n\\[\n\\int_0^t g(s) dB_s = g(t) B_t - \\int_0^tB_s g'(s) ds.\n\\]\nProof:\nDefine\n\\[\nf(t, x) = g(t) x.\n\\]\nNote that\n\\[\n\\partial_0f(t, x) = g'(t) x,\n\\] \\[\n\\partial_1 f(t, x) = g(t),\n\\] and \\[\n\\frac{1}{2}\\partial_1^2 f(t,x) = 0.\n\\] Apply Itô’s fromula:\n\\[\n\\begin{align}\nf(t, B_t) -f(0, B_0) &= \\int_0^t \\partial_1f(s, B_s) dB_s + \\int_0^t \\left(\\partial_0 f(s, B_s) + \\frac{1}{2} \\partial_1^2 f(s, B_s)\\right) ds \\\\\n&\\implies \\\\\ng(t) B_t &= \\int_0^t g(s) dB_s + \\int_0^t B_s g'(s) ds.\n\\end{align}\n\\]\n\\[\\qed\\]\n\n\n(b)\nThe process given by\n\\[\nX_t = t^2B_t - 2 \\int_0^ts B_s ds\n\\] is Gaussian.\nProof:\nUsing integration by parts with\n\\[\ng(t) = t^2\n\\] we have\n\\[\n\\int_0^t s^2 dB_s = t^2 B_t - 2 \\int_0^t s B_s ds = X_t.\n\\]\nTherefore, \\(X_t\\) is a Wiener integral and so the process is Gaussian.\n\\[\\qed\\]\nThe mean of \\(X_t\\) is zero. The covariance is given by\n\\[\n\\begin{align}\n\\E(X_t X_{t'}) &= \\int_0^{t\\wedge t'} s^4 ds \\\\\n&= \\frac{1}{4} (t \\wedge t')^5.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#some-practice-with-itôs-formula",
    "href": "FCSC/ch5/ch5_exercises.html#some-practice-with-itôs-formula",
    "title": "Exercises",
    "section": "5.13 Some Practice with Itô’s Formula",
    "text": "5.13 Some Practice with Itô’s Formula\n\n(a)\nThe process defined by \\[\nX_t = \\int_0^t \\cos{s} dB_s\n\\] is Gaussian and a martingale because it is Wiener. The mean is zero.\nThe covariance is given by\n\\[\n\\begin{align}\n\\E(X_t X_{t'}) &= \\int_0^{t \\wedge t'} \\cos^2(s) ds \\\\\n&= \\frac{1}{2}((t \\wedge t') + \\sin(t \\wedge t') \\cos(t \\wedge t')).\n\\end{align}\n\\]\n\n\n(b)\nThe process defined by\n\\[\nX_t = B_t^4\n\\]\nis not a martingale. Using Itô’s formula\n\\[\nB_t^4 = 4 \\underbrace{\\int_0^t B_s^3 dB_s}_{\\text{martingale}}+ 6 \\int_0^t B_s^2 ds.\n\\]\nTherefore,\n\\[\nY_t = B_t^4 - 6 \\int_0^t B_s^2 ds\n\\] is a martingale.\nThe mean is zero and the covariance is given by\n\\[\n\\begin{align}\n\\E(Y_t Y_{t'}) &= 16 \\int_0^{t \\wedge t'} \\underbrace{\\E(B_s^6)}_{E(X^{2n}) = (2n -1)!! E(X^2)^n} ds \\\\\n&= 16 \\int_0^{t \\wedge t'} 5!! s^3 ds \\\\\n&= \\frac{15.16}{4} (t \\wedge t')^4 \\\\\n&= 60(t \\wedge t')^4.\n\\end{align}\n\\]\nThe process \\(X_t\\) is not Gaussian.\n\n\n(c)\nThe process defined by \\[\nX_t = e^{t/2} \\cos{B_t}\n\\]\nis a martingale.\nProof:\nDefine \\[\nf(t, x) = e^{t/2} \\cos{x}\n\\] and note that \\[\n\\partial_0 f(t, x) = \\frac{1}{2} f(t, x) = -\\frac{1}{2} \\partial^2_1f(t, x)\n\\] and so \\(f(t, B_t)\\) is a martingale. \\[\\qed\\].\nThe mean is\n\\[\n\\E(X_t) = \\E(X_0) = 1.\n\\]\nFor \\(s \\leq t\\), the covariance is given by\n\\[\n\\begin{align}\nE(X_s X_t) &= \\E(X_s^2) \\\\\n&= \\E(e^{s}\\cos^2{B_s}) \\\\\n&= \\frac{e^s}{2} \\E(\\cos{2 B_s} + 1). \\\\\n\\end{align}\n\\]\nUsing power series expansion of \\(\\cos\\):\n\\[\n\\begin{align}\n\\E(\\cos(2 B_s)) & = \\sum_{k = 0}^{\\infty} \\frac{(-1)^k\\E((2B_s)^{2k})}{(2k)!} \\\\\n&= \\sum_{k=0}^{\\infty} \\frac{(-1)^k(2k - 1)!!(4s)^k}{(2k)!} \\\\\n&= \\sum_{k=0}^{\\infty} \\frac{(-1)^k(4s)^k}{(2k)!!} \\\\\n&= \\sum_{k=0}^{\\infty} \\frac{(-1)^k(4s)^k}{2^k(k)!} \\\\\n&= \\sum_{k=0}^{\\infty} \\frac{(-2s)^k}{k!} \\\\\n&= \\exp(-2s).\n\\end{align}\n\\]\nTherefore,\n\\[\n\\E(X_s X_t) = \\frac{\\exp(-s) + \\exp(s)}{2} = \\cosh{s}\n\\] for \\(s \\leq t\\).\nThe process cannot be Gaussian: for each \\(t\\), \\(X_t\\) takes values in \\([0, e^{t/2}]\\) with probability one.\n\n\n(d)\nThe process defined by\n\\[\nZ_t = (B_t + t) \\exp(-B_t - t/2)\n\\] is a martingale.\nProof:\nLet\n\\[\nf(t, x) = (x + t) \\exp(-x - t/2).\n\\]\nThen\n\\[\n\\partial_0 f(t, x) = \\exp(-x - t/2) -\\frac{1}{2} (x + t) \\exp(-x -t/2)\n\\] and \\[\n\\begin{align}\n\\frac{1}{2} \\partial_1^2f(t, x) &= -\\exp(-x -t /2) + \\frac{1}{2}\\exp(-x -t/2)\n\\end{align}\n\\]\nand so \\[\nZ_t = f(t, B_t)\n\\] is a martingale.\n\\[\\qed\\]\nThe mean of the process is zero.\nFor \\(s \\leq t\\), the covariance is given by\n\\[\n\\begin{align}\n\\E(Z_s Z_t) &= \\E(Z_s^2) \\\\\n&= \\E((B_s + s)^2 \\exp(-2B_s - s)) \\\\\n&= \\exp(-s) (\\E( B_s^2 \\exp(-2B_s)) + 2s\\E(B_s \\exp(-2B_s)) + s^2 \\E(\\exp(-2B_s))) \\\\\n&= s(1 + s) \\exp(s).\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\begin{align}\n\\E(B_s^2 \\exp(-2 B_s)) &= s \\E(Z \\exp(-2 \\sqrt{s} Z)) \\\\\n&= s \\E(\\exp(-2B_s) - 2 \\sqrt{s} Z \\exp(-2 B_s)) \\\\\n&= s \\E(\\exp(-2B_s)) -2s\\E(B_s \\exp(-2B_s))\n\\end{align}\n\\] where \\(Z\\) is a standard Gaussian and we’ve used integration by parts:\n\\[\n\\E(Zg(Z)) = \\E(g'(Z)).\n\\]\nUsing the MFG of the Gaussian \\[\n\\E(\\exp(-2B_s)) = \\exp(2s).\n\\]\n\n\nIs this Gaussian? Let’s experiment:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnormal_samples = np.random.default_rng().normal(0, 1, 10000)\n\nZ = (normal_samples + 1) * np.exp(-normal_samples -1/2)\n\nplt.hist(Z, density=True, bins=1000)\nplt.show()\n\n\n\n\nClearly not!"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#gaussian-moments-using-itô",
    "href": "FCSC/ch5/ch5_exercises.html#gaussian-moments-using-itô",
    "title": "Exercises",
    "section": "5.14 Gaussian Moments Using Itô",
    "text": "5.14 Gaussian Moments Using Itô\nLet \\((B_t, t \\in [0, 1])\\) be a Brownian motion. Then\n\\[\n\\E(B_t^k) = \\frac{1}{2}k(k-1)\\int_0^t \\E(B_s^{k-2}) ds.\n\\]\nProof:\nUse Itô’s formula with\n\\[\nf(x) = x^{k}\n\\] and \\(k\\geq 2\\):\n\\[\n\\begin{align}\nf(B_t) - f(B_0) &= \\int_0^t f'(B_s) dB_s + \\frac{1}{2}\\int_0^t f''(B_s) ds \\\\\n&\\implies \\\\\nB_t^k &=\\underbrace{ k \\int_0^t B_s^{k-1} dB_s }_{\\text{martingale with mean zero}}+ \\frac{1}{2} \\int_0^t k(k-1)B_s^{k-2} ds \\\\\n&\\implies \\\\\n\\E(B_t^k) &= \\frac{k(k-1)}{2} \\int_0^t \\E(B_s^{k-2}) ds.\n\\end{align}\n\\]\n\\[\\qed\\]\nIn particular,\n\\[\n\\begin{align}\n\\E(B_t^4) &= \\frac{4.3}{2} \\int_0^t \\E(B_s^2) ds \\\\\n&= 6 \\int_0^t s ds \\\\\n&= 3 t^2\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\E(B_t^6) &= \\frac{6.5}{2} \\int_0^t \\E(B_s^4) ds \\\\\n&= 15 \\int_0^t 3 s^2 ds \\\\\n&= 15 t^3.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#cross-variation-of-t-and-b_t",
    "href": "FCSC/ch5/ch5_exercises.html#cross-variation-of-t-and-b_t",
    "title": "Exercises",
    "section": "5.15 Cross-variation of \\(t\\) and \\(B_t\\)",
    "text": "5.15 Cross-variation of \\(t\\) and \\(B_t\\)\nLet \\((t_j, j \\leq n)\\) be a sequence of partitions of \\([0, t]\\) such that\n\\[\n\\max_j |t_{j+1} - t_j| \\to 0\n\\] as \\(n \\to \\infty\\). Then\n\\[\n\\lim_{n\\to \\infty} \\sum_{j=0}^{n-1} (t_{j+1} -t_j)(B_{t_{j+1}} - B_{t_j}) = 0\n\\] in \\(L^2\\).\nProof:\nDefine\n\\[\nS_n = \\sum_{j=0}^{n-1} (t_{j+1} -t_j)(B_{t_{j+1}} - B_{t_j}) = 0.\n\\]\nFor \\(j < k\\)\n\\[\n\\begin{align}\n\\E((t_{k+1}- t_k)(t_{j+1} -t_j)(B_{t_{k+1}}- B_{t_k})(B_{t_{j+1}} - B_{t_j})) &=\\\\\n(t_{k+1}- t_k)(t_{j+1} -t_j)\\E(\\E((B_{t_{k+1}}- B_{t_k})(B_{t_{j+1}} - B_{t_j})|\\F_{t_k}))  &= \\\\\n(t_{k+1}- t_k)(t_{j+1} -t_j)\\E((\\underbrace{B_{t_{k+1}}- B_{t_k}}_{\\text{independent of } \\F_{t_k}})(B_{t_{j+1}} - B_{t_j})|\\F_{t_k})  &= \\\\\n(t_{k+1}- t_k)(t_{j+1} -t_j)\\E((B_{t_{k+1}}- B_{t_k})\\underbrace{\\E(B_{t_{j+1}} - B_{t_j}|\\F_{t_k})}_{=0}) &= 0.\n\\end{align}\n\\]\nTherefore,\n\\[\n\\begin{align}\n\\E(S_n^2) &= \\E(\\sum_{j=0}^{n-1}(t_{j+1} - t_j)^2(B_{t_{j+1}}-B_{t_j})^2) \\\\\n&= \\sum_{j=0}^{n-1}(t_{j+1} - t_j)^3 \\\\\n&\\leq \\max_j|t_{j+1} -t_j|^2 \\sum_{j=0}^{n-1}  (t_{j+1} - t_j) \\\\\n&\\leq \\max_j|t_{j+1} -t_j|^2 t \\\\\n&\\to 0\n\\end{align}\n\\] as \\(n\\to \\infty\\)."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#exercise-on-itôs-formula",
    "href": "FCSC/ch5/ch5_exercises.html#exercise-on-itôs-formula",
    "title": "Exercises",
    "section": "5.16 Exercise on Itô’s Formula",
    "text": "5.16 Exercise on Itô’s Formula\nConsider for \\(t \\geq 0\\) the process \\[\nX_t=\\exp(t B_t).\n\\]\n\n(a)\nWe can use the MGF of the Gaussian to calculate the mean and variance of \\(X_t\\):\n\\[\n\\begin{align}\n\\E(X_t) &= \\E(\\exp(t B_t))\\\\\n&= \\exp(t^3/2),\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\E(X_t^2) &= \\E(\\exp(2t B_t)) \\\\\n&= \\exp((2t)^2t/2) \\\\\n&= \\exp(2t^3).\n\\end{align}\n\\].\nNote that \\(X_t\\) cannot be a martingale because\n\\[\nX_0 = 1 \\neq \\E(X_t)\n\\] for \\(t > 0\\).\n\n\n(b)\nUsing Itô’s formula with\n\\[\nf(t, x) = \\exp(t x)\n\\]\nwe see that\n\\[\n\\begin{align}\nf(t, B_t) - f(0, B_0) &= \\int_0^t \\partial_1 f(s, B_s) dB_s + \\int_0^t \\{\\partial_0 f(s, B_s) + \\frac{1}{2}\\partial^2_1 f(s, B_s)\\} ds \\\\\n&\\implies \\\\\n\\exp(t B_t) - 1 &= \\underbrace{\\int_0^t s \\exp(s B_s) dB_s}_{\\text{martingale}} + \\int_0^t \\{ B_s \\exp(s B_s) + \\frac{1}{2}s^2 \\exp(s B_s)\\} ds \\\\\n&\\implies \\\\\n\\exp(t B_t) - 1 - \\int_0^t (s^2/2 + B_s)\\exp(s B_s) ds &= \\int_0^t s \\exp(s B_s) d B_s. \\\\\n\\end{align}\n\\]\nSo\n\\[\nC_t = 1 +  \\int_0^t (s^2/2 + B_s)\\exp(s B_s) ds.\n\\]\n\n\n(c)\n\\((\\exp(t B_t), t \\leq T)\\) is in \\(L_c^2(T)\\)\nProof:\nFor almost all \\(\\omega\\) \\[\nt \\mapsto \\exp(t B_t(\\omega))\n\\] is a composition of a continuous function (\\(B_t(\\omega)\\)) with another continuous function. Therefore, the measurability and continuity requirements are met.\nThe integrability requirement is also met: \\[\n\\begin{align}\n\\E(\\exp(2t B_t)) &= \\int_0^T \\exp(2 t^3)  dt \\\\\n&\\leq T \\exp(2 T^3) < \\infty.\n\\end{align}\n\\]\n\\[\\qed\\]\n\n\n(d)\nThe covriance between \\(B_t\\) and \\(\\int_0^t \\exp(s B_s) ds\\) is\n\\[\n\\int_0^t \\exp(s^3/2) ds.\n\\]\nProof:\nUsing the Itô isometry:\n\\[\n\\begin{align}\n\\E(B_t(\\int_0^t \\exp(s B_s) ds)) &= \\E((\\int_0^t dB_s) (\\int_0^t \\exp(s B_s) ds)) \\\\\n&= \\int_0^t \\E(\\exp(s B_s)) ds \\\\\n&= \\int_0^t \\exp(s^3/2) ds.\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\int_0^t d B_s = B_t.\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#itôs-formula-and-optional-stopping",
    "href": "FCSC/ch5/ch5_exercises.html#itôs-formula-and-optional-stopping",
    "title": "Exercises",
    "section": "5.17 Itô’s Formula and Optional Stopping",
    "text": "5.17 Itô’s Formula and Optional Stopping\n\n(a)\nDefine\n\\[\nf(t, x) = tx + g(x)\n\\] and find an ODE for g(x) such that\n\\[\n\\partial_0f(t,x) = -\\frac{1}{2} \\partial_1^2 f(t, x).\n\\]\n\\[\ng''(x) = -2x\n\\]\nso\n\\[\ng(x) = -\\frac{1}{3}x^3 + Cx + D\n\\] where \\(C\\) and \\(D\\) are constants we can choose.\nWe have chosen \\(g\\) so that \\(f(t, B_t)\\) is a martingale. We cannot apply Doob’s optional stopping theorem directly as the stopped process \\(f(\\tau \\wedge t, B_{\\tau \\wedge t})\\) is not bounded. However, the stopped martingale satisfies \\[\n\\E(f(\\tau \\wedge t, B_{\\tau \\wedge t})) = f(0, B_0) = D\n\\] and we show below that taking the limit as \\(t \\to \\infty\\) \\[\n\\E(f(\\tau, B_\\tau)) = D.\n\\]\n\n\n\n\n\n\nFiddly convergence proof\n\n\n\nFor \\(k \\geq 1\\)\n\\[\n\\lim_{t\\to\\infty} \\E((\\tau \\wedge t)^k) =  \\E(\\tau^k)\n\\] by the monotone convergence theorem and \\[\n\\lim_{t\\to\\infty} \\E(B_{\\tau\\wedge t}^k) = \\E(B_{\\tau}^k)\n\\] by the dominated convergence theorem. In particular, \\(\\tau \\wedge t \\to \\tau\\) and \\(B_{\\tau \\wedge t} \\to B_\\tau\\) in \\(L^2\\) and\n\\[\n\\lim_{t \\to \\infty} \\E((\\tau \\wedge t - B_{\\tau \\wedge t})^2) = \\E((\\tau - B_{\\tau})^2).\n\\] Creatively developing the square shows\n\\[\n\\begin{align}\n2\\E(\\tau \\wedge t B_{\\tau \\wedge t}) &= \\E((\\tau\\wedge t - B_{\\tau \\wedge t})^2) - \\E((\\tau \\wedge t)^2) - E(B_{\\tau \\wedge t}^2)\\\\\n&\\to 2\\E(\\tau B_{\\tau}).\n\\end{align}\n\\]\n\n\nMoreover, since \\(B_{\\tau}\\) can only take one of two values\n\\[\n\\begin{align}\n\\E(f(\\tau, B_\\tau)) &= \\E(\\tau B_\\tau) + \\E(g(B_\\tau)) \\\\\n&= \\E(\\tau B_\\tau) + \\P(B_\\tau = a) g(a) + (1 - P(B_\\tau = a)) g(-b).\n\\end{align}\n\\]\nLet \\(p = \\P(B_\\tau =a)\\) and choose boundary conditions\n\\[\ng(a) = 1/p\n\\] and \\[\ng(-b) = 0.\n\\]\nThen \\[\n\\E(\\tau B_\\tau) = D - 1.\n\\]\n\\[\n\\begin{align}\nbg(a) + ag(-b) &= b(-a^3/3 + Ca + D) + a(b^3/3 - Cb + D) \\\\\n&= ab(b^2 - a^2)/3 + (a + b)D \\\\\n&= b/p.\n\\end{align}\n\\] Rearranging yields\n\\[\nD = ab(a -b)/3 + \\frac{b}{p(a+b)}\n\\]\nand\n\\[\n\\E(\\tau B_\\tau) = \\frac{ab}{3}(a -b) + \\frac{b}{p(a+b)} - 1.\n\\]\nFrom symmetry, the process \\((-B_t, t \\geq 0)\\) satisfies the above with the roles of \\(a\\) and \\(b\\) swapped:\n\\[\n\\begin{align}\n\\E(-\\tau B_\\tau) &= \\frac{ab}{3}(b - a) + \\frac{a}{\\P(-B_\\tau = b)(a+b)} - 1 \\\\\n&= \\frac{ab}{3}(b - a) + \\frac{a}{\\P(B_\\tau = -b)(a+b)} - 1 \\\\\n&= \\frac{ab}{3}(b - a) + \\frac{a}{(1 - p)(a+b)} - 1. \\\\\n\\end{align}\n\\]\nUsing the above with linearity of expectation\n\\[\n\\E(\\tau B_\\tau) = \\frac{ab}{3}(a - b) + (1 - \\frac{a}{(1-p)(a+b)})\n\\]\nand so\n\\[\n\\frac{b}{p(a+b)} - 1 = 1 - \\frac{a}{(1-p)(a+b)}\n\\]\nwith solution\n\\[\np = \\frac{b}{a+b}.\n\\]\nTherefore,\n\\[\n\\E(\\tau B_\\tau) = \\frac{ab}{3}(a-b).\n\\]\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#a-strange-martingale",
    "href": "FCSC/ch5/ch5_exercises.html#a-strange-martingale",
    "title": "Exercises",
    "section": "5.18 A Strange Martingale",
    "text": "5.18 A Strange Martingale\nLet \\((B_t, t \\geq 0)\\) be a standard Brownian motion. Consider the process\n\\[\nM_t = \\frac{1}{\\sqrt{1 -t}}\\exp(\\frac{-B_t^2}{2(1-t)}),\n\\] for \\(t \\in [0,1)\\).\n\n(a)\n\\[\nM_t = 1  + \\int_0^t \\frac{-B_sM_s}{1 -s}d B_s\n\\] for \\(t \\in [0,1)\\).\nProof:\nUse \\[\nf(t, x) = \\frac{1}{\\sqrt{1-t}}\\exp(\\frac{-x^2}{2(1-t)})\n\\] and Itô’s formula to show that \\(f(t, B_t)\\) is a martingale:\n\nimport sympy as sp\nfrom sympy.abc import t, s, x\n\nf = (1/sp.sqrt(1 -s)) * sp.exp((-x**2)/(2*(1-s)))\n\n(sp.diff(f, s) + (1/2)*sp.diff(f, x, 2)).simplify()\n\n\\(\\displaystyle 0\\)\n\n\nThe Itô integral for the martingale is\n\nintegrand=sp.diff(f, x)\n\nM_s = sp.Symbol('M_s')\nB_s = sp.Symbol('B_s')\n\nsp.Integral(integrand.subs({f:M_s, x: B_s}).simplify(),(s, 0, t))\n\n\\(\\displaystyle \\int\\limits_{0}^{t} \\frac{B_{s} M_{s}}{s - 1}\\, ds\\)\n\n\n\n\n(b)\n\\((M_s, s \\leq t)\\) is a martingale for \\(t < 1\\) because it can be expressed as an Itô integral (plus a constant).\n\n\n(c)\nGiven (b), we can state that\n\\[\n\\E(M_t) = M_0 = 1\n\\] for \\(t <1\\).\n\n\n(d)\n\\[\n\\lim_{t \\to 1^-} M_t = 0\n\\] almost surely.\nProof:\nThere exists \\(A \\subseteq \\Omega\\) with \\(\\P(A) = 1\\), such that\n\\[\ng: t \\mapsto B_t(\\omega)\n\\] is continuous. Let \\(B = \\{\\omega: B_t(\\omega) \\neq 0 \\}\\); clearly, \\(\\P(B) =1\\) and \\(\\P(A \\cap B) = 1\\). Choose \\(\\omega \\in A \\cap B\\) and consider the corresponding \\(g\\): on the interval \\([0, 1]\\), \\(g^2\\) attains a minimum \\(C \\geq 0\\) and so\n\\[\n\\frac{1}{\\sqrt{1 -t}} \\exp(\\frac{-B_t^2}{2(1 -t)}) \\leq\n\\frac{1}{\\sqrt{1 -t}} \\exp(\\frac{-C}{2(1 -t)})\n\\] for \\(0 \\leq t < 1\\).\nFor this fixed \\(\\omega\\) \\[\n\\begin{align}\n\\frac{1}{\\sqrt{1-t}} \\exp(\\frac{-C}{2(1-t)}) &= \\sqrt{\\frac{1}{1-t}\\exp(\\frac{-C}{1-t})} \\\\\n&\\to 0\n\\end{align}\n\\] as \\(t\\to 1^-\\); this is true for \\(\\omega \\in A\\cap B\\), so almost surely.\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\exp(C/(1-t)) > \\frac{C^{2}}{2(1-t)^2}\n\\] so \\[\n\\exp(-C/(1-t)) < \\frac{2(1-t)^2}{C^2}\n\\] and \\[\n\\frac{1}{1-t}\\exp(-C/(1-t)) <  \\frac{2(1-t)}{C^2} \\to 0\n\\] as \\(t \\to 1^-\\).\n\n\n\\[\\qed\\]\n\n\n(e)\n\\[\n\\E(\\sup_{0 \\leq t < 1} M_t) = + \\infty.\n\\]\nProof:\nSuppose that\n\\[\n\\E(\\sup_{0 \\leq t < 1} M_t) < \\infty\n\\] i.e. \\(\\exists C > 0\\) such that \\[\n\\E(\\sup_{0 \\leq t < 1} M_t) < C.\n\\]\nThen\n\\[\n\\P(\\{ \\omega: \\exists t \\in [0,1) \\text{ such that } M_t(\\omega) \\geq C \\}) = 0.\n\\]\nAlmost surely, \\(M_t \\leq C\\) for \\(t \\in [0,1)\\) and so since\n\\[\nM_t \\to 0\n\\] almost surely, the dominated convergence theorem would imply that\n\\[\n\\lim_{t \\to 1^-} \\E(M_t) = 0 \\Rightarrow\\Leftarrow\n\\]\n\\[\\qed\\].\n\n\n5.19 \\(L^2\\)-limit of Gaussians is Gaussian\nLet \\((X_n, n \\geq 0)\\) be a sequence of Gaussian random variables that converge to \\(X\\) in \\(L^2(\\Omega, \\F, \\P)\\).\n\\(X\\) is Gaussian.\nProof:\nLet \\(m_n = \\E(X_n)\\) and \\(\\sigma_n^2 = \\E(X_n^2) - \\E(X_n)^2\\). We know that\n\\[\nm_n \\to \\E(X)\n\\] and \\[\n\\sigma_n^2 \\to \\sigma^2 = \\E(X^2) - \\E(X)^2\n\\] as \\(n \\to \\infty\\). Therefore,\n\\[\n\\E(\\exp(i t X_n)) \\to \\exp(itm - \\sigma^2t^2/2).\n\\]\nConvergence of \\(X_n \\to X\\) in \\(L^2\\) implies that there is a subsequence such that \\[\nX_{n_k}(\\omega) \\to X(\\omega)\n\\] almost surely. Using continuity of the exponential function \\[\n\\exp(itX_{n_k}(\\omega)) \\to \\exp(it X(\\omega))\n\\] almost surely. The sequence is bounded by \\(1\\):\n\\[\n|\\exp(itX_{n_k}(\\omega))| \\leq 1.\n\\]\nTherefore, by the dominated convergence theorem\n\\[\n\\begin{align}\n\\lim \\E(\\exp(itX_{n_k}))) &= \\E(\\lim \\exp(it X_{n_k})) \\\\\n&= \\E(\\exp(itX)) \\\\\n&= \\exp(itm - \\sigma^2t^2/2).\n\\end{align}\n\\]\nHaving this characteristic function shows that \\(X\\) is Gaussian with mean \\(m\\) and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#l2-is-complete",
    "href": "FCSC/ch5/ch5_exercises.html#l2-is-complete",
    "title": "Exercises",
    "section": "5.20 \\(L^2\\) is Complete",
    "text": "5.20 \\(L^2\\) is Complete\n\n(a)\nLet \\((X_n)\\) be a Cauchy sequence in \\(L^2(\\Omega, \\F, \\P)\\). There exists a subsequence \\((X_{n_k})\\) such that \\[\n\\|X_m - X_{n_k} \\| \\leq 2^{-k}\n\\] for all \\(m > n_k\\).\nProof:\nFor each \\(k\\), there exists \\(N\\) such that\n\\[\n\\| X_n - X_m \\| < 2^{-k}\n\\] for \\(n, m > N\\) . Choose \\(n_k = N + 1\\) to construct the desired sequence.\n\\[ \\qed \\]\n\n\n(b)\nConsider the candidate limit\n\\[\n\\sum_{j=0}^{\\infty} (X_{n_{j+1}} - X_{n_j})\n\\] where \\(X_{n_0} = 0\\). This sum converges almost surely.\nProof:\n\\[\n\\begin{align}\n\\sum_{j=0}^{k} \\E(| X_{n_{j+1}} - X_{n_j}|) &< \\underbrace{\\sum_{j=0}^{k} 2^{-(j+1)}}_{\\|\\cdot\\|_1 \\leq \\|\\cdot\\|_2} \\\\\n& \\to 1\n\\end{align}\n\\] as \\(k \\to \\infty\\).\nBy Markov’s inequality\n\\[\n\\begin{align}\n\\P(\\sum_{j=0}^{k} |X_{n_{j+1}} - X_{n_j}| > c) &\\leq \\frac{1}{c} \\E(\\sum_{j=0}^k |X_{n_{j+1}} - X_{n_j}|) \\\\\n&\\leq \\frac{1}{c}.\n\\end{align}\n\\]\nLet \\[\nB_k = \\{ \\omega : \\sum_{j=0}^{k} |X_{n_{j+1}} - X_{n_j}| >c) \\}\n\\] increasing, nested events such that \\[\n\\cup B_k = \\{ \\omega : \\sum_{j=0}^{\\infty} |X_{n_{j+1}} - X_{n_j}| >c \\}.\n\\]\nBy continuity of probability\n\\[\n\\P(\\sum_{j=0}^{\\infty} |X_{n_{j+1}} - X_{n_j}| >c) = \\lim_{k \\to \\infty} \\P(B_k) \\leq \\frac{1}{c}.\n\\]\nTherefore, \\[\n\\P(\\sum_{j=0}^{\\infty} |X_{n_{j+1}} - X_{n_j}| \\leq c) \\geq 1 - \\frac{1}{c}.\n\\]\nDefine\n\\[\nA_n = \\{ \\omega: \\sum_{j=0}^{\\infty} |X_{n_{j+1}}(\\omega) - X_{n_j}(\\omega)| \\leq n \\}.\n\\]\nThen \\((A_n)\\) is an increasing sequence of events and\n\\[\nA = \\cup A_n = \\{ \\omega: \\sum_{j=0}^{\\infty} |X_{n_{j+1}}(\\omega) - X_{n_j}(\\omega)| < \\infty \\}.\n\\] By continuity of probability \\[\n\\P(A) = \\lim_{n\\to\\infty} \\P(A_n) = 1.\n\\] For any \\(\\omega \\in A\\), the sum converges. \\[\\qed\\]\n\n\n(c)\nDefine \\[\nX = \\sum_{j=0}^{\\infty} (X_{n_{j+1}} - X_{n_j})\n\\] which is finite, almost surely.\n\\[\n\\| X - X_{n_k}\\| \\to 0\n\\] as \\(k\\to \\infty\\) and \\(\\| X\\| < \\infty\\).\nProof:\n\\[\nX_{n_k} = \\sum_{j=0}^{k-1} (X_{n_{k+1}} - X_{n_k})\n\\] and so \\(X_{n_k} \\to X\\) almost surely.\n\\[\n\\begin{align}\n\\| X - X_{n_k} \\| &= \\| \\sum_{j= k}^{\\infty} (X_{n_{k+1}} - X_{n_k}) \\| \\\\\n&\\leq \\sum_{j=k}^{\\infty} \\|X_{n_{k+1}} - X_{n_k}\\| \\\\\n&\\leq 2^{-k} \\\\\n&\\to 0\n\\end{align}\n\\] as \\(k \\to \\infty\\).\nNote that\n\\[\n\\|X\\| \\leq \\| X - X_{n_k} \\| + \\| X_{n_k} \\| < \\infty.\n\\]\n\\[\\qed\\]\n\n\n(d)\nFor \\(\\varepsilon > 0\\) there exists \\(N\\) such that\n\\[\n\\| X_{n_k} - X \\| < \\varepsilon/2.\n\\] for \\(n_k > N\\).\nSince the sequence is Cauchy, there exists \\(M\\) such that for \\(n_k, m > M\\), such that\n\\[\n\\| X_{n_k} - X_m \\| < \\varepsilon/2.\n\\] Choose \\(m, n_k > \\max{M, N}\\). Then\n\\[\n\\begin{align}\n\\|X - X_m \\| &= \\| X - X_{n_k} + X_{n_k} - X_m \\| \\\\\n&\\leq \\| X - X_{n_k} \\| + \\| X_{n_k} - X_m \\| \\\\\n& < \\varepsilon.\n\\end{align}\n\\] That is, \\(X_m \\to X\\) in \\(L^2\\) as \\(m \\to \\infty\\)."
  },
  {
    "objectID": "FCSC/ch5/ch5_exercises.html#another-application-of-doobs-maximal-inequality",
    "href": "FCSC/ch5/ch5_exercises.html#another-application-of-doobs-maximal-inequality",
    "title": "Exercises",
    "section": "5.21 Another Application of Doob’s Maximal Inequality",
    "text": "5.21 Another Application of Doob’s Maximal Inequality\nLet \\((B_t, t \\in [0,1])\\) be a Brownian motion defined on \\((\\Omega, \\F, \\P)\\).\nThe process\n\\[\nZ_t = (1-t) \\int_0^t \\frac{1}{1-s}ds\n\\]\nhas the distribution of the Brownian bridge on \\([0, 1)\\).\n\\[\n\\lim_{t \\to 1^-} Z_t = 0\n\\] almost surely.\nProof:\nFirst, show that \\(Z_t \\to 0\\) in \\(L^2\\) as \\(t \\to 1^-\\).\n\\[\n\\begin{align}\n\\E(Z_t^2) &= (1-t)^2 \\int_0^t \\frac{1}{(1-s)^2} ds \\\\\n&= (1-t)^2 (\\frac{t}{1-t}) \\\\\n&= t(1-t) \\to 0\n\\end{align}\n\\] as \\(t\\to 1^-\\).\nThen, using Doob’s maximal inequality show that\n\\[\n\\P(\\max_{t \\in [ 1- 1/2^n, 1 -1/2^{n+1}]} |Z_t| > \\delta) \\leq \\frac{1}{\\delta^2} \\frac{1}{2^{n-1}}.\n\\]\nBy definition of \\(Z_t\\) and simple properties of maxima,\n\\[\n\\begin{align}\n\\max_{t \\in [1 - 1/2^n, 1 - 1/2^{n+1}]} |Z_t| &= \\max_{t  \\in [1 - 1/2^n, 1 - 1/2^{n+1} ]} |(1 - t) \\int_0^t \\frac{1}{1-s} ds | \\\\\n&\\leq (1/2^n) \\max_{t \\in [1 - 1/2^n, 1 - 1/2^{n+1}]} |\\int_0^t \\frac{1}{1-s} ds| \\\\\n\\end{align}\n\\] Using Doob’s maximal inequality and \\(\\E(Z_t) = t(1-t)\\) gives \\[\n\\begin{align}\n\\P(\\max_{t \\in [1 - 1/2^n, 1 - 1/2^{n+1}]} |Z_t| > \\delta) &\\leq \\P((1/2^n) \\max_{t \\in [1 - 1/2^n, 1 - 1/2^{n+1}]} |\\int_0^t \\frac{1}{1-s} ds| > \\delta) \\\\\n&= \\P(\\max_{t \\in [1 - 1/2^n, 1 - 1/2^{n+1}]} |\\int_0^t \\frac{1}{1-s} ds| > \\delta 2^{n}) \\\\\n&\\leq \\frac{1}{\\delta^2 2^{2n}} \\E(\\frac{1}{(1/2^{n+1})^2} Z_{1-1/2^{n+1}}^2) \\\\\n&\\leq \\frac{1}{\\delta^2 2^{2n}} \\frac{1}{(1/2^{n+1})^2} \\E(Z_{1-1/2^{n+1}}^2) \\\\\n&\\leq \\frac{1}{\\delta^2 2^{2n}} \\frac{1}{(1/2^{n+1})^2} (1/2^{n+1})(1-1/2^{n+1}) \\\\\n&= \\frac{2^{n+1} - 1}{\\delta^2 2^{2n}} \\\\\n&\\leq \\frac{1}{\\delta^2 2^{n-1}}.\n\\end{align}\n\\]\nLet\n\\[\nE_n = \\{ \\omega: \\max_{t \\in [1- 1/2^n, 1- 1/2^{n+1}]} |Z_t| > \\delta \\}.\n\\]\nBy the above\n\\[\n\\begin{align}\n\\sum_{n=1}^{\\infty} \\P(E_n) &\\leq \\sum_{n=1}^{\\infty} \\frac{1}{\\delta^2 2^{n-1}} \\\\\n&= \\frac{2}{\\delta^2}\\\\\n\\end{align}\n\\] and so by the Borel-Cantelli lemma, \\[\n\\P(\\lim\\sup_{n\\to \\infty} E_n) = 0.\n\\] We can see that \\[\n\\cup_{n=k}^{\\infty} E_n = \\{ \\omega: \\exists j \\geq k \\max_{t \\in [1-1/2^j, 1 -1/2^{j+1})} |Z_t(\\omega)| > \\delta\\}\n\\] and\n\\[\n\\begin{align}\n\\cap_{k=1}^{\\infty} \\cup_{n=k}^{\\infty} E_n &= \\{\\omega: \\forall k\\, \\exists j \\geq k \\text{ such that }  \\max_{t \\in [1-2^{j}, 1-1/2^{j+1})}|Z_t| > \\delta \\}\\\\\n& \\supseteq \\{ \\omega : \\lim_{t \\to 1^{-}} |Z_t(\\omega)| > \\delta \\}.\n\\end{align}\n\\] Note: \\[\nt \\mapsto Z_t(\\omega)\n\\] is continuous almost surely.\nTherefore, \\[\n\\P(\\lim_{t \\to 1^{-1}} |Z_t| > \\delta) = 0\n\\] for all \\(\\delta > 0\\). That is, \\(Z_t \\to 0\\) as \\(t \\to 1^-\\) almost surely."
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html",
    "href": "FCSC/ch5/ch5_experiments.html",
    "title": "Computer Experiments",
    "section": "",
    "text": "import numpy as np\n\nincrements = 1000\nsamples = 100\n\ndef ito(integrand, inc=increments, rng=np.random.default_rng()):\n  delta = 1/inc\n  bp = np.cumsum(np.r_[0, rng.normal(0, np.sqrt(1/inc), inc)])\n  return np.cumsum([ integrand(i*delta, bp[i])*(bp[i+1] - bp[i]) for i in range(len(bp)-1)])\n\ndef t(path):\n  return np.linspace(0, 1, len(path))\n\n\nimport matplotlib.pyplot as plt\n\n\n#### (a)\nfor _ in range(samples):\n  path = ito(lambda t, x: 4*(x**3))\n  plt.plot(t(path), path)\n\nplt.title(\"(a) $\\int_0^t 4 B_s^3 d B_s$\")\nplt.show()\n\n#### (b)\nfor _ in range(samples):\n  path = ito(lambda t, x: np.cos(x))\n  plt.plot(t(path), path)\n\nplt.title(\"(b) $\\int_0^t \\cos{B_s} d B_s$\")\nplt.show()\n\n#### (c)\nfor _ in range(samples):\n  path = ito(lambda t, x: np.exp(t/2)*np.sin(x))\n  plt.plot(t(path), path)\n\nplt.title(\"(c) $1 - \\int_0^t e^{s/2} \\sin{B_s} d B_s$\")\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#ornstein-uhlenbeck-process-and-brownian-bridge-revisited",
    "href": "FCSC/ch5/ch5_experiments.html#ornstein-uhlenbeck-process-and-brownian-bridge-revisited",
    "title": "Computer Experiments",
    "section": "5.2 Ornstein-Uhlenbeck Process and Brownian Bridge Revisited",
    "text": "5.2 Ornstein-Uhlenbeck Process and Brownian Bridge Revisited\n\nfor _ in range(samples):\n  path = ito(lambda t, x: np.exp(t))\n  ticks = t(path)\n  plt.plot(ticks, np.exp(-ticks)*path)\nplt.title(\"Ornstein-Uhlenbeck\")\nplt.show()\n\nfor _ in range(samples):\n  path = ito(lambda t, x: 1/(1 - t))\n  ticks = t(path)\n  plt.plot(ticks, (1-ticks)*path)\nplt.title(\"Brownian-Bridge\")\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#itôs-formula",
    "href": "FCSC/ch5/ch5_experiments.html#itôs-formula",
    "title": "Computer Experiments",
    "section": "5.3 Itô’s Formula",
    "text": "5.3 Itô’s Formula\n\n(a)\n\nincrements = 1000\ndelta=1/increments\n\ndef seeded_rng():\n  return np.random.default_rng(1337)\n\nbrownian_path = np.cumsum(np.r_[0, seeded_rng().normal(0, np.sqrt(delta), increments-1)])\n\n  \npath = ito(lambda t, x: 4*(x**3), rng=seeded_rng(), inc=increments)\nplt.plot(t(path), path, label='$\\int_0^t 4 B_s^3$')\n\nriemann = np.cumsum(6*delta*(brownian_path**2))\nplt.plot(t(brownian_path), brownian_path**4 - riemann, label='$B_t^4 - 6 \\int_0^t B_s^2 ds$')\n\nplt.title(\"(a)\")\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n(b)\n\nriemann = np.cumsum(delta*np.sin(brownian_path))\nplt.plot(t(riemann), np.sin(brownian_path) + (1/2)*riemann, label='$\\sin{B_t} + \\\\frac{1}{2}\\int_0^2 \\sin{B_s} ds$')\nito_path = ito(lambda t, x: np.cos(x), rng=seeded_rng(), inc=increments)\nplt.plot(t(ito_path), ito_path, label='$\\int_0^t \\cos{B_s} dB_s$')\n\nplt.title(\"(b)\")\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n(c)\n\npath = [ np.exp(i*delta/2)*np.cos(b) for i, b in enumerate(brownian_path) ]\nplt.plot(t(path), path, label='$e^{t/2}\\cos{B_t}$')\n\nito_path = ito(lambda t, x: np.exp(t/2)*np.sin(x), rng=seeded_rng(), inc=increments)\nplt.plot(t(ito_path), 1 - ito_path, label='$1 - \\int_0^t \\sin{B_s} dB_s$')\n\nplt.title(\"(c)\")\nplt.legend(loc='upper left')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#a-path-dependent-integrand",
    "href": "FCSC/ch5/ch5_experiments.html#a-path-dependent-integrand",
    "title": "Computer Experiments",
    "section": "5.4 A Path-Dependent Integrand",
    "text": "5.4 A Path-Dependent Integrand\n\nfor i in range(10):\n  rng = np.random.default_rng(i)\n  # feed the same rng into each ito integration step because the\n  # brownian path in each integral is the same?\n  integrand = iter(ito(lambda t, x: x, rng=rng))\n  rng = np.random.default_rng(i)\n  integral = ito(lambda t, x: next(integrand), rng=rng)\n  plt.plot(t(integral), integral)\n\nplt.title(\"5.4\")\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#convergence-of-itôs-formula",
    "href": "FCSC/ch5/ch5_experiments.html#convergence-of-itôs-formula",
    "title": "Computer Experiments",
    "section": "5.5 Convergence of Itô’s Formula",
    "text": "5.5 Convergence of Itô’s Formula\n\ndef rhs(seed, inc):\n    delta = 1/inc\n    brownian_path = np.cumsum(np.r_[0, np.random.default_rng(seed).normal(0, np.sqrt(delta), inc-1)])\n    riemann = np.cumsum(3*delta*brownian_path)\n    return ito(lambda t, x: 3*(x**2), rng=np.random.default_rng(seed), inc=inc) + riemann\n\ndef lhs(seed, inc):\n    delta = 1/inc\n    brownian_path = np.cumsum(np.r_[0, np.random.default_rng(seed).normal(0, np.sqrt(delta), inc-1)])\n    return brownian_path**3\n\n\ndiscretization = [10, 100, 1000, 10000]\nsamples = 100\nerrors = []\n\nfor inc in discretization:\n  error = 0\n  for i in range(samples):\n    error += np.abs(lhs(i, inc)[-1] - rhs(i, inc)[-1])\n  errors.append(error/samples)\n\nplt.plot(discretization, errors, '-xk')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#testing-the-solution-to-gamblers-ruin",
    "href": "FCSC/ch5/ch5_experiments.html#testing-the-solution-to-gamblers-ruin",
    "title": "Computer Experiments",
    "section": "5.6 Testing the Solution to Gambler’s Ruin",
    "text": "5.6 Testing the Solution to Gambler’s Ruin\n\nN = 10000\ndrift = 1\nvolatility = 1\nstart, end = 0, 5\n\ndef hit(target, increments):\n  brownian_motion = np.cumsum(np.r_[0, np.random.default_rng().normal(0, np.sqrt((end - start)/increments), increments - 1)])\n  process = volatility*brownian_motion + drift* np.linspace(start, end, increments)\n  for v in process:\n    if v <= target:\n      return 1\n  return 0\n\nprobability = np.exp(-2*drift/(volatility**2))\n\nincrements = 100\nfor inc in [100, 1000]:\n  hits = sum((hit(-1, inc) for _ in range(N)))\n  print(inc, np.abs(hits/N - probability))\n\n100 0.033835283236612695\n\n\n1000 0.013735283236612703"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#the-integral-of-a-process-not-in-mathcall_ct",
    "href": "FCSC/ch5/ch5_experiments.html#the-integral-of-a-process-not-in-mathcall_ct",
    "title": "Computer Experiments",
    "section": "5.7 The Integral of a Process not in \\(\\mathcal{L}_c(T)\\)",
    "text": "5.7 The Integral of a Process not in \\(\\mathcal{L}_c(T)\\)\n\nN = 100\nstart, end = 0, 10\n\ndef ito():\n  increments = 1000\n  delta = (end - start)/increments\n  B =  np.cumsum(np.r_[0, np.random.default_rng().normal(0, np.sqrt(delta), increments-1)])\n  Z = np.exp(B**2)\n  return np.cumsum([Z[i]*(B[i+1] -B[i]) for i in range(len(B) -1)])\n\nfor _ in range(N):\n  I = ito()\n  plt.plot(np.linspace(start, end, len(I)), I)\n\nplt.show()"
  },
  {
    "objectID": "FCSC/ch5/ch5_experiments.html#tanakas-formula",
    "href": "FCSC/ch5/ch5_experiments.html#tanakas-formula",
    "title": "Computer Experiments",
    "section": "5.8 Tanaka’s Formula",
    "text": "5.8 Tanaka’s Formula\n\nincrements = 1000000\nstart, end = 0, 1\ndelta = (end - start)/increments\nbrownian_motions = [ np.cumsum(np.r_[0, np.random.default_rng().normal(0, np.sqrt(delta), increments-1)]) for _ in range(10)]\n\n\ndef sgn_ito(B):\n  def sgn(x):\n    return 1 if x >= 0 else -1\n  return np.cumsum([ sgn(B[i])*(B[i+1]- B[i]) for i in range(len(B) - 1)])\n\nfor B in brownian_motions:\n  plt.plot(np.linspace(start, end, increments-1), np.abs(B[:-1]) - sgn_ito(B))\n\n\n\n\n\ndef L(epsilon, B):\n  return np.cumsum([b < epsilon for b in np.abs(B)])*(delta/(2*epsilon))\n\nepsilon = 0.001\nfor B in brownian_motions:\n  plt.plot(np.linspace(start, end, increments), L(epsilon, B))\nplt.show()"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html",
    "href": "FCSC/ch1/ch1_exercises.html",
    "title": "Chapter One: Exercises",
    "section": "",
    "text": "Prove proposition 1.3.\n\nFinite additivity: if two events \\(A\\) and \\(B\\) are disjoint, then \\(P(A \\cup B) = P(A) + P(B)\\)\n\nThis follows from Definition 2.3 (3) with \\(A = A_1\\), \\(B = A_2\\) and \\(A_i = \\emptyset\\) for \\(i > 2\\):\n\\[\n\\begin{align}\nP(A \\cup B) &= P (\\cup_{i} A_i) = P(A_1) + P(A_2) + \\cup_{i > 2} P(A_i) \\\\\n            &= P(A) + P(B) + \\cup_{i > 2} P(\\emptyset) \\\\\n            &= P(A) + P(B).\n\\end{align}\n\\]\n\nFor any event \\(A\\), \\(P(A^c) = 1 - P(A)\\):\n\n\\(\\Omega\\) is the disjoint union of \\(A\\) and \\(A^c\\). By (1) above \\[\n1 = P(\\Omega) = P(A \\cup A^c) = P(A) + P(A^c)\n\\] so \\[\nP(A) = 1 - P(A^c).\n\\]\n\nFor any events \\(A\\), \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\):\n\nExpress \\(A \\cup B = (A\\setminus B) \\cup B\\) where the union is disjoint. Using (1)\n\\[\nP(A \\cup B) = P(A\\setminus B) + P(B).\n\\] Now, \\[\nP(A\\setminus B) + P(A\\cap B) = P(A)\n\\] and so \\[\nP(A\\setminus B) = P(A) - P(A\\cap B).\n\\]\nIt follows that\n\\[\nP(A \\cup B) = P(A \\setminus B) + P(B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\nMonotonicity: If \\(A \\subseteq B\\), then \\(P(A) \\leq P(B)\\).\n\nIf \\(A \\subseteq B\\), then \\(B = A \\cup (B \\setminus A)\\) where the union is disjoint. It follows that\n\\[\nP(B) = P(A) + P(B \\setminus A) \\geq P(A).\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#equiprobability",
    "href": "FCSC/ch1/ch1_exercises.html#equiprobability",
    "title": "Chapter One: Exercises",
    "section": "1.2 Equiprobability",
    "text": "1.2 Equiprobability\nLet \\(\\Omega\\) be a sample space with a finite number of outcomes. We define\n\\[\nP(A) = \\#A/\\#\\Omega\n\\] for \\(A \\subseteq \\Omega\\).\n\\(P\\) is a probability on \\(\\Omega\\).\nProof:\n\nSince counts are positive, \\(P >= 0\\). Since \\(\\#A \\leq \\#\\Omega\\), \\(P \\leq 1\\) and so \\(P(A) \\in [0, 1]\\) for \\(A \\subseteq \\Omega\\).\nThe empty set \\(\\emptyset\\) has no elements, so \\(P(\\emptyset) = 0\\). It is easy to see that \\[\nP(\\Omega) = \\frac{\\# \\Omega}{ \\# \\Omega} =1.\n\\]\nAdditivity: for any infinite, mutually disjoint sequence of events \\(A_1, A_2, \\ldots\\) there exists \\(N\\) such that \\(A_n = \\emptyset\\) for all \\(n \\geq N\\). In fact, \\(N\\) must be less than \\(\\# \\Omega\\).\n\nThen\n\\[\n\\begin{align}\nP(\\cup_{i=1}^{\\infty} A_n) &= P(\\cup_{i=1}^{N} A_n)\\\\\n&= \\frac{\\sum_{i=1}^N \\# A_n}{\\# \\Omega} \\\\\n&= \\sum_{i=1}^N P(A_n) \\\\\n&= \\sum_{i=1}^{\\infty} P(A_n).\n\\end{align}\n\\]."
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#distribution-as-a-probability-on-mathbbr",
    "href": "FCSC/ch1/ch1_exercises.html#distribution-as-a-probability-on-mathbbr",
    "title": "Chapter One: Exercises",
    "section": "1.3 Distribution as a Probability on \\(\\mathbb{R}\\)",
    "text": "1.3 Distribution as a Probability on \\(\\mathbb{R}\\)\nLet \\(\\rho_X\\) be the distribution of random variables \\(X\\) on some probability space \\((\\Omega, \\mathcal{F}, P)\\). \\(\\rho_X\\) has the properties of a probability on \\(\\mathbb{R}\\).\nProof:\nLet \\(\\mathcal{F}_X\\) be the \\(\\sigma\\)-field of set in \\(\\mathbb{R}\\) such that \\(A \\in \\mathcal{F} \\iff \\{\\omega \\in \\Omega: X(\\omega) \\in A\\} \\in \\mathcal{F}\\).\nWe define \\(P_X\\) on \\(\\mathcal{F}_X\\) by\n\\[\nP_X(A) = P(\\{\\omega \\in \\Omega: X(\\omega) \\in A\\}).\n\\]\nWe note that \\(P_X\\) extends \\(\\rho_X\\).\nFor notational simplicity, we write\n\\[\nX^{-1}(A) = \\{\\omega \\in \\Omega: X(\\omega) \\in A\\}.\n\\]\nClearly, for any \\(A \\in \\mathcal{F}_X\\), \\(P_X(A) = P(X^{-1}(A)) \\in [0,1]\\), so satisfies (1) of Definition 1.2.\nThe pre-image of \\(\\emptyset\\), \\(X^{-1}(\\emptyset)\\) must be itself empty. Therefore,\n\\[\nP_X(\\emptyset) = P(\\emptyset) = 0.\n\\]\nSimilarly, the pre-image of \\(\\mathbb{R}\\) must be all of \\(\\Omega\\) and so\n\\[\nP_X(\\mathbb{R}) = P(\\Omega) = 1.\n\\]\nThis shows that \\(P_X\\) satisfies (2) of Definition 2.1.\nLet \\(A_1, A_2, \\ldots\\) be an infinite sequence of events in \\(\\mathcal{F}_X\\) that are mutually disjoint. Note that the sequence of pre-images \\(X^{-1}(A_1), X^{-1}(A_2), \\ldots\\) are also mutually disjoint. Since \\(P\\) is a probability we can use its additivity to prove the additivity of \\(P_X\\):\n\\[\nP_X(\\cup_{i=1}^{\\infty} A_i) = P(\\cup_{i=1}^{\\infty} X^{-1}(A_i)) = \\sum_{i=1}^{\\infty} P(X^{-1}(A_i)) = \\sum_{i=1}^{\\infty} P_X(A_i).\n\\]\nThis shows that \\(P_X\\) satisfies (3) of Definition 2.1."
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#distribution-of-an-indicator-function",
    "href": "FCSC/ch1/ch1_exercises.html#distribution-of-an-indicator-function",
    "title": "Chapter One: Exercises",
    "section": "1.4 Distribution of an Indicator Function",
    "text": "1.4 Distribution of an Indicator Function\nLet \\((\\Omega, \\mathcal{F}, P)\\) and \\(A \\in \\mathcal{F}\\) with \\(0 < P(A) < 1\\). What is the distribution of \\(1_A\\)?\nObserve that \\(P(1_A \\leq x) = 0\\) when \\(x < 0\\) because the indicator is a non-negative function. When \\(0 \\leq x < 1\\), \\(1_A(\\omega) > x\\) for all \\(\\omega \\in A\\) but \\(1_A(\\omega) = 0 <= x\\) for \\(\\omega \\in A^c\\). It follows that \\[P(1_A \\leq x) = P(\\{\\omega \\in \\Omega : 1_A(\\omega) \\leq x\\}) =P(\\{\\omega \\in \\Omega : \\omega \\in A^c\\}) = P(A^c).\\] For \\(x \\geq 1\\), \\(1_A \\leq x\\) is true for all values of \\(\\omega \\in \\Omega\\) because the maximum value of the indicator function is \\(1\\). Therefore, with \\(F\\) denoting the CDF of \\(1_A\\): \\[\nF(x) = P(1_A \\leq x) = \\begin{cases}\n0 & \\text{if } x < 0,\\\\\nP(A^c) & \\text{if } 0 \\leq x < 1,\\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#events-of-probability-one",
    "href": "FCSC/ch1/ch1_exercises.html#events-of-probability-one",
    "title": "Chapter One: Exercises",
    "section": "1.5 Events of Probability One",
    "text": "1.5 Events of Probability One\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(A_1, A_2, \\ldots\\) be a sequence of events in \\(\\mathcal{F}\\) such that \\(P(A_n) = 1\\) for all \\(n \\geq 1\\). We show that\n\\[\nP(\\cap_{i\\geq 1} A_n) = 1.\n\\]\nDefine \\(B_n = A_n^c\\) for all \\(n \\geq 1\\). We note that\n\\[\nP(B_n) = 1 - P(A_n) = 0\n\\] for all \\(n \\geq 0\\).\nDefine a sequence of events \\(C_n = \\cup_{i =1}^n B_i\\) and note that the sequence is increasing. Continuity of probability gives\n\\[\n\\lim_{n \\to \\infty} P(C_n) = P(\\cup_{i=1}^{\\infty} B_i).\n\\]\nUsing \\(P(A \\cup B) \\leq P(A) + P(B)\\), we see that\n\\[\nP(C_n) \\leq \\sum_{i = 1}^n P(B_i) = 0\n\\]\nand so \\(P(C_n) = 0\\) and \\[\\lim_{n \\to \\infty} P(C_n) = P(\\cup_{i=1}^{\\infty} B_i) = 0\\].\nTo finish the proof, we note that\n\\[\\begin{align}\nP(\\cap_{i=1}^{\\infty} A_i) &= 1 - P((\\cap_{i=1}^{\\infty} A_i)^c)\\\\\n                           &= 1 - P( \\cup_{i =1}^{\\infty} A_i^c)\\\\\n                           &= P(\\cup_{i=1}^{\\infty} B_i) = 0.\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#constructing-a-random-variable-from-another",
    "href": "FCSC/ch1/ch1_exercises.html#constructing-a-random-variable-from-another",
    "title": "Chapter One: Exercises",
    "section": "1.6 Constructing a Random Variable from Another",
    "text": "1.6 Constructing a Random Variable from Another\nLet \\(X\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\) this is uniformly distributed on \\([-1, 1]\\). Define \\(Y = X^2\\).\n\nFind the CDF of Y and plot its graph.\n\nFirst, the CDF:\nLet \\(F\\) denote the CDF of \\(Y\\) and \\(F_X\\) denote the CDF of X.\nThen \\[\nG(x) = P(Y \\leq x) = P(X^2 \\leq x) = P(X \\leq \\sqrt{x}) = F(\\sqrt{x}).\n\\]\nNow,\n\\[\nF(x) =\n\\begin{cases}\n0 & \\text{if } x \\leq 0, \\\\\nx & \\text{if } 0 < x < 1, \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nI’ve taken \\(F\\) to be different to that in the book (the first case in the book uses \\(x < 0\\)) so that the resulting \\(G\\) has a well-defined derivative at zero.\n\n\n\nand so\n\\[\nG(x) =\n\\begin{cases}\n0 & \\text{if } x <= 0, \\\\\n\\sqrt{x} & \\text{if } 0 < x < 1, \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}\n\\]\nWe plot this below:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef G(x):\n    if x <= 0:\n        return 0\n    elif x > 0 and x < 1:\n        return np.sqrt(x)\n    return 1\n\nxticks = np.linspace(-1, 1.5, 100)\nplt.plot(xticks, [G(x) for x in xticks])\n\n\n\n\n\nthe PDF of Y is given by\n\n\\[\np(y) = \\frac{dG}{dy}(y) = \\begin{cases}\n0 & \\text{if } x \\leq 0, \\\\\n\\frac{1}{2\\sqrt{x}} & \\text{if } 0 < x < 1, \\\\\n0 & \\text{if } 0 \\geq 1.\n\\end{cases}\n\\]\nThe plot is below\n\n\nCode\ndef p(x):\n    if x <= 0:\n        return 0\n    elif x > 0 and x < 1:\n        return (1/( 2 * np.sqrt(x)))\n    return 1\n\nxticks = np.linspace(-1, 0, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nxticks = np.linspace(0.01, 0.99, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nxticks = np.linspace(1, 2, 50)\nplt.plot(xticks, [p(x) for x in xticks])\nplt.show()"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#sum-of-integrable-variables",
    "href": "FCSC/ch1/ch1_exercises.html#sum-of-integrable-variables",
    "title": "Chapter One: Exercises",
    "section": "1.7 Sum of Integrable Variables",
    "text": "1.7 Sum of Integrable Variables\n\\(X\\) and \\(Y\\) are two integrable random variables on the same probability space. Then \\(aX + bY\\) is also an integrable random variable for any \\(a, b \\in \\mathbb{R}\\).\nProof:\nWe assert that \\(aX + b Y\\) is a random variable and are left to show that\n\\[\nE(\\left|aX + bY\\right|) < \\infty.\n\\]\nBy the triangle inequality,\n\\[\n\\left| aX + bY \\right| \\leq \\left| a \\right| \\left| X \\right| + \\left| b \\right| \\left| Y \\right|\n\\] and so we can conclude that\n\\[\nE(\\left|aX + bY\\right|) < \\infty.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#jumps-and-probabilities",
    "href": "FCSC/ch1/ch1_exercises.html#jumps-and-probabilities",
    "title": "Chapter One: Exercises",
    "section": "1.8 Jumps and Probabilities",
    "text": "1.8 Jumps and Probabilities\nLet \\(X\\) be a random variable and \\(F_X\\) be its CDF. Then\n\\[\nP(X = a) = F_X(a) - \\lim_{x \\to a-} F_X(x) = F_X(a) - F_X(a-).\n\\]\nProof:\nDefine a decreasing sequence of events \\(A_n = \\{ X \\in (a - 1/n, a]\\}\\) and note that \\(\\cap_{i=1}^{\\infty} A_n = \\{ X = a\\}\\). By continuity of probability\n\\[\n\\lim_{n \\to \\infty} P(A_n) = P(X = a).\n\\]\nNow,\n\\[\\begin{align}\nP(X = a) &= \\lim_{n \\to \\infty} P(A_n)\\\\\n         &= \\lim_{n \\to \\infty} (F_X(a) - F_X(a - 1/n))\\\\\n         &= F_X(a) - \\lim_{n \\to \\infty} (F_X(a - 1/n)) = F_X(a) - F_X(a-).\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#memory-loss-property",
    "href": "FCSC/ch1/ch1_exercises.html#memory-loss-property",
    "title": "Chapter One: Exercises",
    "section": "1.9 Memory Loss Property",
    "text": "1.9 Memory Loss Property\nLet \\(Y\\) be an exponential random variable with parameter \\(\\lambda\\). The for any \\(s, t > 0\\)\n\\[\nP(Y > t + s | Y > s) = P(Y > t).\n\\]\nProof:\nThe CDF of the exponential distribution is\n\\[\nF(t) = 1 - e^{-\\lambda t}.\n\\]\nTherefore,\n\\[\nP(Y > t) = 1 - F(t) = e^{-\\lambda t}.\n\\]\nNow\n\\[\\begin{align}\nP(Y > t + s | Y > s) & =  \\frac{P(Y > t + s \\cap Y > s)}{P(Y > s)} \\\\\n                     & =  \\frac{P(Y > t + s)}{P(Y > s)}  \\\\\n                     & =  \\frac{e^{-\\lambda(t + s)}}{e^{-\\lambda s}} \\\\\n                     & =  e^{-\\lambda t} \\\\\n                     & =  P(Y > t).\n\\end{align}\\]\nNote that \\(P(Y > t + s \\cap Y > s) = P(Y > t + s)\\) because \\(\\{ \\omega \\in \\Omega | Y(\\omega) > t + s \\} \\subseteq \\{ \\omega \\in \\Omega | Y(\\omega) > t \\}\\) and so \\(\\{ \\omega \\in \\Omega | Y(\\omega) > t + s \\} \\cap \\{ \\omega \\in \\Omega | Y(\\omega) > t \\} = \\{ \\omega \\in \\Omega | Y(\\omega) > t + s\\}\\).\n\n\n\n\n\n\nThis property characterises the exponential distribution\n\n\n\nUsing the memory property \\[\n\\begin{align}\nP(X \\leq x + h) - P(X < x) &= 1 - P(X > x + h) -1 + P(X > x) \\\\\n&= P(X > x) - P(X > x + h) \\\\\n&= P(X > x) - P(X > x + h | X > x) P(X > x) \\\\\n&= P(X > x)(1 - P(X > x + h | X > x)) \\\\\n&= P(X > x)(1 - P(X >  h)) \\\\\n&= P(X > x)(P(X \\leq h) - P( X\\leq 0)).\n\\end{align}\n\\]\nSo we can assert that the CDF \\(F\\) satisfies\n\\[\n\\begin{align}\nF'(x) &= \\lim_{h\\to 0} \\frac{P(X \\leq x +h) - P(X \\leq x)}{h} \\\\\n&= P(X > x) \\lim_{h \\to 0} \\frac{P(X \\leq h) - P(X \\leq 0)}{h} \\\\\n&= P(X > x) F'(0) \\\\\n&= (1 - F(x)) F'(0).\n\\end{align}\n\\]\nThe family of solutions to this differential equation is\n\\[\nF(x) = 1 - e^{-\\lambda x}\n\\] for \\(\\lambda > 0\\)."
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#gaussian-integration-by-parts",
    "href": "FCSC/ch1/ch1_exercises.html#gaussian-integration-by-parts",
    "title": "Chapter One: Exercises",
    "section": "1.10 Gaussian Integration by Parts",
    "text": "1.10 Gaussian Integration by Parts\n\nLet \\(Z\\) be a standard Gaussian random variable. Then\n\n\\[\nE(Zg(Z)) = E(g'(Z))\n\\]\nwhen both expectations are well-defined.\nProof:\nWe can use LOTUS and integration by parts:\n\\[\\begin{align}\nE(Z g(Z)) = \\int_{-\\infty}^{\\infty} z g(z) p(z) dz  &=  \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} z g(z) e^{-z^2/2} dz \\\\\n&= \\int_{-\\infty}^{\\infty} g(z) (-\\frac{dp}{dz}) dz \\\\\n&= -g(z) p(z) \\rvert_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} g'(z) p(z) dz \\\\\n&= E(g'(Z)).\n\\end{align}\\]\nNote that\n\\[\n-g(z) p(z) \\rvert_{-\\infty}^{\\infty} = 0\n\\]\nfollows from \\(zg(z) p(z)\\) being integrable.\n\nIn particular, if \\(g(z) = z^{n + 1}\\), then \\(g'(z) = (n +1) z^{n}\\) so\n\n\\[\nE( Z g(Z)) = E(Z^{n+2}) = E((n + 1) Z^{n}).\n\\]\nWe see that\n\\[\n\\begin{align}\nE(Z^{2n}) &= (2n -1) E(Z^{2n -2})\\\\\n          &= (2n -1) (2n -3) E(Z^{2n - 4})\\\\\n          &= (2n - 1) (2n -3) \\ldots 1 E(Z^2)\\\\\n          &= (2n -1) (2n -3) \\ldots 1.\n\\end{align}\n\\]\nSimilarly,\n\\[\n\\begin{align}\nE(Z^{2n + 1}) &= (2n) E(Z^{2n -1})\\\\\n          &= (2n) (2n -2) E(Z^{2n - 3})\\\\\n          &= (2n) (2n -2) \\ldots 1 E(Z)\\\\\n          &= 0.\n\\end{align}\n\\]\nSetting \\(X = \\sigma Z\\), we find that\n\\[\nE(X^{2n}) = \\sigma^{2n} (2n -1)(2n -3) \\ldots 1.\n\\]\nWe can’t have a mean different from zero: without symmetry about zero the special structure of odd and even functions would be lost."
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#mgf-of-exponential-random-variables",
    "href": "FCSC/ch1/ch1_exercises.html#mgf-of-exponential-random-variables",
    "title": "Chapter One: Exercises",
    "section": "1.11 MGF of Exponential Random Variables",
    "text": "1.11 MGF of Exponential Random Variables\nShow that for a random variable \\(X \\sim exp(\\lambda)\\)\n\\[\nE(e^{t X}) = \\frac{ \\lambda}{\\lambda - t }, t < \\lambda.\n\\]\nUsing LOTUS and choosing \\(t < \\lambda\\)\n\\[\\begin{align}\nE(e^{tX}) &= \\int_{0}^{\\infty} e^{tx} \\lambda e^{-\\lambda x} dx \\\\\n          &= \\int_0^{\\infty} e^{(t - \\lambda) x} dx \\label{exp:integral} \\tag{*} \\\\\n          &= \\frac{1}{t - \\lambda} e^{(t - \\lambda) x} \\rvert_{0}^{\\infty} \\\\\n          &= \\frac{ \\lambda}{\\lambda - t }.\n\\end{align}\\]\nNote that if \\(t \\geq \\lambda\\), then the integral \\(\\eqref{exp:integral}\\) is not well-defined.\nWe can calculate \\(E(X)\\):\n\\[\nE(X) = \\frac{d}{dt} E(e^{tX}) \\rvert_{t=0} = \\frac{\\lambda}{(\\lambda -t)^2}\\rvert_{t=0} = \\frac{1}{\\lambda}.\n\\]\nWe can also calculate \\(Var(X)\\):\n\\[\nE(X^2) = \\frac{d^2}{dt^2} E(e^{tX}) \\rvert_{t=0} = \\frac{2\\lambda}{(\\lambda -t)^3} \\rvert_{t=0} = \\frac{2}{\\lambda^2}.\n\\]\nand so\n\\[\nVar(X) = E(X^2) - E(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} =\\frac{1}{\\lambda^2}.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#gaussian-tail",
    "href": "FCSC/ch1/ch1_exercises.html#gaussian-tail",
    "title": "Chapter One: Exercises",
    "section": "1.12 Gaussian Tail",
    "text": "1.12 Gaussian Tail\nConsider a random variable \\(X\\) with finite MGF such that\n\\[\\begin{equation}\nE(e^{\\lambda X}) \\leq e^{\\lambda^2/2}\n\\label{gtail} \\tag{1}\n\\end{equation}\\]\nfor all \\(\\lambda \\in \\mathbb{R}\\).\nProve that for \\(a > 0\\)\n\\[\nP(X > a) \\leq e^{-a^2/2}.\n\\]\nFor \\(\\lambda > 0\\) \\[\nP(X > a) = P(e^{\\lambda X} > e^{\\lambda a})\n\\] by the monotonicity of the exponential. By Markov’s inequality and \\(\\eqref{gtail}\\) \\[\nP(e^{\\lambda X} > e^{\\lambda a}) \\leq \\frac{E(e^{\\lambda X})}{e^{\\lambda a}} \\leq e^{\\lambda^2/2 - \\lambda a}.\n\\]\nLet \\(f(\\lambda) = e^{\\lambda^2/2 - \\lambda a}\\). The minimum for \\(f\\) is found by differentiation:\n\\[\nf'(\\lambda) = (\\lambda - a) e^{\\lambda^2/2 - \\lambda a}\n\\]\nand \\(f'(\\lambda) = 0\\) is solved for \\(\\lambda = a\\). Therefore,\n\\[\nP(X > a) \\leq e^{a^2/2 - a^2} = e^{-a^2/2}.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#expectation-from-cdf",
    "href": "FCSC/ch1/ch1_exercises.html#expectation-from-cdf",
    "title": "Chapter One: Exercises",
    "section": "1.13 Expectation from CDF",
    "text": "1.13 Expectation from CDF\nLet \\(X\\) be a random variable such that \\(X >= 0\\). Then\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx.\n\\]\nProof:\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} (1 - F(x)) dx\n\\]\nwhere \\(F(x) = P(X <= x)\\). We can write\n\\[\n1 - F(x) = \\lim_{x \\to \\infty} F(x) - F(x) = \\int_x^{\\infty} dF(t)\n\\]\nwhere the integral is understood in the Lebesgue sense.\nNow,\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} \\int_x^{\\infty} dF(t) dx\n\\]\nand by changing the order of integration (and appealing to Fubini’s Theorem)\n\\[\n\\int_0^{\\infty} P(X > x) dx = \\int_0^{\\infty} \\int_0^t dx dF(t) = \\int_0^{\\infty} t dF(t) = E(X).\n\\]\nSubtle point: we’re using the fact that X >= 0 to arrive at zero for the lower limit of the inner integral.\n\n\n\n\n\n\nConsequence: LOTUS (Law Of The Unconscious Statistician)\n\n\n\n\n\nLet \\(g: \\mathbb{R} \\mapsto \\mathbb{R}\\) be measurable. Then \\(E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) dF(x)\\).\nFirst suppose that \\(g >= 0\\). Then\n\\[\nE(g(X)) = \\int_0^{\\infty} P(g(X) > x) dx\n\\] by the result above.\nNow,\n\\[\n\\int_0^{\\infty} P(g(X) > x) dx = \\int_0^{\\infty} \\int_{\\{z: g(z) > x\\}} dF(z) dx.\n\\]\nChanging the order of integration, we see get\n\\[\n\\int_{-\\infty}^{\\infty} \\int_0^{g(z)} dx dF(z) = \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\]\nNow suppose we have general \\(g\\). Split \\(g\\) into the sum of non-negative and negative components\n\\[\ng = g_+ - g_{-}\n\\]\n\\[\n\\begin{align}\nE(g(X)) &= E(g_+(X)) - E(g_{-}(X)) \\\\\n&= \\int_{-\\infty}^{\\infty} g_+(x) dF(x) - \\int_{-\\infty}^{\\infty} g_{-}(x) dF(x) \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\nConsequence: Limit of xP(X>x)\n\n\n\n\n\nIf \\(E(X_+) < \\infty\\), then \\(\\lim_{x \\to \\infty} x P(X > x) = 0\\).\nWe prove this by contradiction. Suppose that there exists \\(c > 0\\) such that \\(lim_{x \\to \\infty} xP(X >x) \\geq c\\). Then for some \\(x(c)\\), \\(xP(X>x) \\geq c\\) for all \\(x > x(c)\\) and so\n\\[\nE(X+) = \\int_0^{\\infty} P(X > x) dx \\geq \\int_{x(c)}^{\\infty} P(X > x) dx \\geq \\int_{x(c)}^{\\infty} \\frac{c}{x} dx = \\infty.\n\\]\nThis contradicts \\(E(X_+) < \\infty\\).\nContrast this with the Markov inequality which states that\n\\[\nP(X > x) <= \\frac{E(X)}{x}\n\\]\nwhich puts a bound on \\(x P(X > x)\\):\n\\[\nx P(X > x) <= E(X).\n\\]\n\n\n\nTake a random variable \\(X\\) such the \\(E(|X|) < \\infty\\). Prove that\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X \\leq x) dx.\n\\]\nDefine \\(X_+ = X 1_{X \\geq 0}\\) and \\(X_{-} = X 1_{X < 0}\\) and note that \\(X = X_+ + X_{-}\\). Then\n\\[\nE(X) = E(X_+) + E (X_{-})\n\\] by linearity of expectation. From above,\n\\[\nE(X_+) = \\int_0^{\\infty} P(X_+ > x) dx = \\int_0 ^{\\infty} P(X > x) dx.\n\\]\nSet \\(Y = -X_{-}\\). Then \\(Y > 0\\) and\n\\[\nE(Y) = \\int_0^{\\infty} P(Y > y) dy.\n\\]\nNow,\n\\[\\begin{align}\nE(X 1_{X < 0}) &= - E(- X_{-}) \\\\\n               &= - E(Y) \\\\\n               &= -\\int_0^{\\infty} P(-X_{-} >y) dy \\\\\n               &= \\int_{0}^{-\\infty} P(X_{-} < x) dx \\\\\n               &= \\int_0^{-\\infty} P(X < x) dx\n\\end{align}\\]\nThen\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X < x) dx.\n\\]\n\n\n\n\n\n\nThe result we have to prove is not generally true\n\n\n\n\\[\n\\int (P(X <= x) -P(X<x)) dx = \\int P(X = x) dx = 0\n\\]\nwhen \\(X\\) has a PDF (the CDF is at least continuous). In this case, we can state\n\\[\nE(X) = \\int_0^{\\infty} P(X > x) dx - \\int_{-\\infty}^0 P(X \\leq x) dx.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#characteristic-function",
    "href": "FCSC/ch1/ch1_exercises.html#characteristic-function",
    "title": "Chapter One: Exercises",
    "section": "1.14 Characteristic Function",
    "text": "1.14 Characteristic Function\n\nNote that the expectation of a complex-valued random variable (or indeed the complex-valued random variable) is defined in Chapter 1.\n\nA good definition is\n\\[\nE(Z) = E(\\Re(Z)) + i E(\\Im(Z)).\n\\]\nNote that if \\(E(|Z|) < \\infty\\), then the real and imaginary parts of \\(Z\\) are also integrable.\nWe can extend LOTUS to complex valued functions. Suppose that \\(g: \\mathbb{R} \\to \\mathbb{C}\\) and \\(E(|g(X)|) < \\infty\\). We can decompose \\(g\\) as\n\\[\ng = g_1 + i g_2.\n\\]\nNote that \\(g_1(X)\\) and \\(g_2(X)\\) are integrable.\nwhere \\(g_1, g_2 : \\mathbb{R} \\to \\mathbb{R}\\). Then by linearity of expectation\n\\[\\begin{align}\nE(g(X)) &= E(g_1(X)) + i E(g_2(X))\\\\\n        &= \\int_{-\\infty}^{\\infty} g_1(x) dF(x) + i \\int_{-\\infty}^{\\infty} g_2(x) dF(x)\\\\\n        &= \\int_{-\\infty}^{\\infty} (g_1(x) + i g_2(x)) dF(x)\\\\\n        &= \\int_{-\\infty}^{\\infty} g(x) dF(x).\n\\end{align}\\]\nUsing this result, we see that\n\\[\nE(e^{itX}) = E(\\cos(tX)) + i E(\\sin(tX))\n\\]\nfrom the familiar Euler identity \\(e^{itx} = \\cos(tx) + i \\sin(tx)\\).\nThe same result can be argued using the Taylor series:\n\\[\\begin{align}\nE(e^{itX}) &= \\sum_{n=0}^{\\infty} E( \\frac{(itX)^n}{n!})\\\\\n           &= \\sum_{n=0}^{\\infty} E( \\frac{((-1)^{n} t^{2n} X^{2n}}{(2n)!}) + i \\sum_{i=0}^{\\infty} E( \\frac{((-1)^{n} t^{2n +1} X^{2n + 1}}{(2n + 1)!})\\\\\n           &= E(\\cos(tX)) + iE(\\sin(tX).\n\\end{align}\\]\n\nLet \\(X \\sim \\mathcal{N}(0, 1)\\). Then\n\n\\[\nE(e^{itX}) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} e^{itx} e^{-\\frac{1}{2}x^2} dx.\n\\]\nDefine \\(g(t) = E(e^{itX})\\) and try to build a differential equation we can solve which is hopefully equal to the desired result.\nUsing the Dominated Convergence Theorem, we can take differentiation inside the expectation integral (the derivative of the integrand is dominated by \\(e^{-\\frac{1}{2}x^2}\\) which is integrable).\nWe get\n\\[\\begin{align}\ng'(t) &= \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} i x e^{itx} e^{-\\frac{1}{2}x^2} dx.\n\\end{align}\\]\nUse integration by parts:\nSet \\(u = ie^{itx}\\) so \\(du = -t e^{itx}\\) and\n\\[\\begin{align}\ndv &= \\frac{1}{\\sqrt{2 \\pi}} x e^{-\\frac{1}{2}x^2}\\\\\n   &= -\\frac{1}{\\sqrt{2 \\pi}} \\frac{d}{dx}(e^{-\\frac{1}{2}x^2}).\n\\end{align}\\]\nPerforming the integration by parts we see that \\[\n\\begin{align}\ng'(t) &= \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} i x e^{itx} e^{-\\frac{1}{2}x^2} dx \\\\\n      &= -\\frac{1}{\\sqrt{2 \\pi}} ie^{itx} e^{-\\frac{1}{2}x^2} \\rvert_{-\\infty}^{\\infty} -  \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} te^{itx}  e^{-\\frac{1}{2}x^2} dx \\\\\n      &= 0 + iE(e^{itX})\\\\\n      &= -t g(t). \\label{characteristic:diffeqn} \\tag{*}\n\\end{align}\n\\] Moreover,\n\\[\ng(0) = E(e^{0}) = E(1) = 1.\n\\]\nThe unique solution to \\(\\eqref{characteristic:diffeqn}\\) is\n\\[\ng(t) = \\frac{1}{2}e^{-t^2/2}\n\\]\nand so\n\\[\nE(e^{itX}) = \\frac{1}{2}e^{-t^2/2}.\n\\]\nLet \\(Z = \\sigma X + \\mu\\) for \\(\\sigma > 0\\) and \\(\\mu \\in \\mathbb{R}\\). Then\n\\[\\begin{align}\nE(e^{itZ}) &= E(e^{it(\\sigma X + \\mu)}) \\\\\n           &= E(e^{\\sigma X} e^{it\\mu}) \\\\\n           &= e^{it\\mu}  E(e^{it\\sigma X})\\\\\n           &= e^{it\\mu}e^{-(t\\sigma)^2/2}\\\\\n           &= e^{it \\mu - \\sigma^2t^2/2}.\n\\end{align}\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#when-ex-infty",
    "href": "FCSC/ch1/ch1_exercises.html#when-ex-infty",
    "title": "Chapter One: Exercises",
    "section": "1.15 When \\(E(X) < \\infty\\)",
    "text": "1.15 When \\(E(X) < \\infty\\)\nLet \\(X \\geq 0\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\). If \\(E(X) < \\infty\\), then \\(P(X = \\infty) = 0\\).\nProof:\nLet \\(A_n = \\{\\omega \\in \\Omega: X(\\omega) > n \\}\\) for \\(n > 0\\).\nWe note that the sequence of events \\(A_n\\) is decreasing and so by continuity of probability\n\\[\n\\lim_{n\\to \\infty} P(A_n) = P(\\cap A_n) = P(X = \\infty).\n\\]\nThe proof is complete with an application of Markov’s inequality: \\[\nP(X = \\infty) = \\lim_{n \\to \\infty} P(A_n) \\leq \\lim_{n\\to\\infty} \\frac{1}{n} E(X) = 0.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_exercises.html#when-ex-0",
    "href": "FCSC/ch1/ch1_exercises.html#when-ex-0",
    "title": "Chapter One: Exercises",
    "section": "1.16 When \\(E(X) = 0\\)",
    "text": "1.16 When \\(E(X) = 0\\)\nLet \\(X \\geq 0\\) be a random variable on \\((\\Omega, \\mathcal{F}, P)\\). If \\(E(X) = 0\\), then \\(P(X = 0) = 1\\).\nProof:\nBy Markov’s inequality\n\\[\nP(X > 1/n) \\leq n E(X) = 0\n\\]\nand so \\(P(X > 1/n) = 0\\) for \\(n = 1, 2, \\ldots\\). It follows that\n\\[\nP(X \\leq 1/n) = 1 - P(X > 1/n) = 1.\n\\]\nThe sequence of events \\(\\{ X \\leq 1/n \\}\\) is decreasing and\n\\[\n\\{ X = 0 \\} = \\cap \\{ X \\leq 1/n \\}.\n\\]\nBy the continuity of probability\n\\[\nP(X = 0) = \\lim_{n \\to \\infty} P(X \\leq 1/n) = 1.\n\\]"
  },
  {
    "objectID": "FCSC/ch1/ch1_experiments.html",
    "href": "FCSC/ch1/ch1_experiments.html",
    "title": "Chapter One: Computer Experiments",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# (a)\nN = 10000\nsamples = np.random.default_rng().uniform(0, 1, N)\n\n# (b)\n\nbins = 50\nplt.hist(samples, bins, label='Unform')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nUniform Distribution\n\n\n\n\n\n# (c)\nplt.hist(samples, bins, cumulative=True, label='CDF X ~ U(0,1)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n# Redo (b) and (c) for X^2\n\nsamples_squared = [ x**2 for x in samples]\n\n\nplt.hist(samples_squared, bins, label='PDF X^2')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\nplt.hist(samples_squared, bins, cumulative=True, label='CDF X^2')\nplt.legend(loc='upper left')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch1/ch1_experiments.html#the-law-of-large-numbers",
    "href": "FCSC/ch1/ch1_experiments.html#the-law-of-large-numbers",
    "title": "Chapter One: Computer Experiments",
    "section": "1.2 The Law of Large Numbers",
    "text": "1.2 The Law of Large Numbers\n\n# (a)\nN = 10000\nsamples = np.random.default_rng().exponential(1, N)\naverages = [sample/(idx +1) for idx, sample in enumerate(np.cumsum(samples))]\n\nbins = np.linspace(0, 2, 100)\nplt.hist(averages, bins)\nplt.show()\n\n\n\n\n\nplt.plot(averages)\nplt.ylabel('average of N samples')\nplt.xlabel('N')\nplt.show()\n\n\n\n\n\n# (b)\n\ndef average(n):\n    return np.sum(np.random.default_rng().exponential(1, n))/n\n\nbins = np.linspace(-0, 2, 100)\nplt.hist([average(100) for _ in range(0, 10000)], bins, label='average(100)', alpha=0.5)\nplt.hist([average(10000) for _ in range(0, 10000)], bins, label='average(10000)', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nNote how the averages of 10000 samples have less variance than the averages of 100 samples."
  },
  {
    "objectID": "FCSC/ch1/ch1_experiments.html#central-limit-theorem",
    "href": "FCSC/ch1/ch1_experiments.html#central-limit-theorem",
    "title": "Chapter One: Computer Experiments",
    "section": "1.3 Central Limit Theorem",
    "text": "1.3 Central Limit Theorem\n\ndef Y(N):\n    sum = np.sum(np.random.default_rng().exponential(1, N))\n    return (sum - N) /np.sqrt(N)\n\nsamples = 10000\n\nbins = np.linspace(-3, 3, 50)\n\n\nplt.hist([ Y(100) for _ in range(0, samples)], bins, label='Y(100)', alpha=0.5)\nplt.hist(np.random.default_rng().normal(0, 1, samples), bins, label='N(0,1)', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch1/ch1_experiments.html#sampling-cauchy-random-variables",
    "href": "FCSC/ch1/ch1_experiments.html#sampling-cauchy-random-variables",
    "title": "Chapter One: Computer Experiments",
    "section": "1.4 Sampling Cauchy Random Variables",
    "text": "1.4 Sampling Cauchy Random Variables\n\ndef invF(y):\n    return np.tan((y - 0.5)*np.pi)\n\nN = 10000\nsamples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]\n\n\nbins = np.linspace(-10, 10, 100)\nplt.hist(samples, bins, alpha=0.5, label='Cauchy')\nplt.hist(np.random.default_rng().normal(0, 1, N), bins, alpha=0.5, label='normal')\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\n\nfor i in range(0,4):\n    samples = [ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ]\n    plt.plot([ s/(idx + 1) for idx, s in enumerate(np.cumsum(samples)) ], label=f'average {i}')\n\nplt.legend(loc='upper right')\nplt.xlabel('N')\nplt.show()\n\n\n\n\n\ndef cauchy_empirical_mean(N):\n    return np.sum([ invF(u) for u in np.random.default_rng().uniform(0, 1, N) ])/N\n\n\nbins = np.linspace(-40, 40, 1000)\nplt.hist([cauchy_empirical_mean(10) for _ in range(0, 10000)], bins, label='N=10', alpha=0.5)\nplt.hist([cauchy_empirical_mean(100) for _ in range(0, 10000)], bins, label='N=100', alpha=0.5)\nplt.legend(loc='upper right')\nplt.show()\n\n\n\n\nNote how the empirical means with 10 samples and 100 samples appear to be identically distributed. The Cauchy distribution has no defined mean (even though it is symmetrical about 0) so the Central Limit Theorem does not apply.\nIf we have two iid variables \\(X\\) and \\(Y\\) with Cauchy distribution i.e. with pdf\n\\[\nf(x) = \\frac{1}{\\pi} \\frac{1}{1 + x^2} dx.\n\\]\nWe can get the distribution of \\(\\frac{1}{2}(X + Y)\\) by considering the characteristic function of the distribution, \\(e^{-|t|}\\).\nThe characteristic function of \\(\\frac{1}{2}(X + Y)\\) is\n\\[\nE(e^{it(X + Y)/2}) = E(e^{itX/2}) E(e^{itY/2}) = e^{-2|t/2|} = e^{-|t|}.\n\\]\nThis tells us that \\(\\frac{1}{2}(X + Y)\\) has the same distribution as \\(X\\) and \\(Y\\). So, when we calculate empirical means of Cauchy distribution independent variables, the result does not converge to a constant plus a narrow Gaussian error: instead, we get a random variable with the same distribution the samples, regardless of how many samples we take!"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html",
    "href": "FCSC/ch2/ch2_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\G}{\\mathcal{G}}\n\\newcommand{\\qed}{\\tag*{$\\square$}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#an-example-of-uncorrelated-random-variables-that-are-not-independent",
    "href": "FCSC/ch2/ch2_exercises.html#an-example-of-uncorrelated-random-variables-that-are-not-independent",
    "title": "Exercises",
    "section": "2.1 An Example of Uncorrelated Random Variables that are not Independent",
    "text": "2.1 An Example of Uncorrelated Random Variables that are not Independent\nLet \\(X\\) be a standard Gaussian. Show that \\(\\Cov(X^2, X) = 0\\).\n\\[\n\\Cov(X^2, X) = \\E(X^3) - \\E(X^2)\\E(X)\n\\]\nThe standard Gaussian has odd moments equal to zero so\n\\[\n\\Cov(X^2, X) = 0 - \\E(X^2).0 = 0.\n\\]\nIf you don’t have the knowledge at your fingertips, there’s always direct calculation:\nWe already know that \\(\\E(X) = 0\\) for the standard Gaussian (it has mean \\(0\\)).\nUsing integration by parts: \\[\n\\begin{align}\n\\E(X^3) & = \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} x^3 e^{-x^2/2} dx \\\\\n&=  \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} x^2 \\frac{d}{dx}(-e^{-x^2/2}) dx \\\\\n&= \\frac{1}{\\sqrt{2 \\pi}} (-x^2 e^{-x^2/2} \\rvert_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} 2 x e^{-x^2/2} dx) \\\\\n& = 2 \\E(X) = 0.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#sum-of-exponentials-is-gamma",
    "href": "FCSC/ch2/ch2_exercises.html#sum-of-exponentials-is-gamma",
    "title": "Exercises",
    "section": "2.2 Sum of Exponentials is Gamma",
    "text": "2.2 Sum of Exponentials is Gamma\nThe sum of \\(n\\) IID random variables with exponential distribution with parameter \\(\\lambda\\) is gamma with pdf\n\\[\n\\begin{align}\nf(x) = \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{-\\lambda x} &, x \\geq 0.\n\\end{align}\n\\tag{1}\\]\nProof:\nThe pdf of the sum of two IID random variables is the convolution of the pdfs of the summands.\nTherefore, \\[\n\\begin{align}\nf(x) &= \\int_{0}^{x} \\lambda^2 e^{-\\lambda(x - y)}e^{-\\lambda y} dy \\\\\n&= \\lambda^2 e^{-\\lambda x} \\int_{0}^{x} dy \\\\\n&= \\lambda^2 x e^{-\\lambda x}.\n\\end{align}\n\\]\nSo, it’s at least plausible.\nTo prove the result, we use the MGF of the exponential random variables \\(X\\) with parameter \\(\\lambda\\):\n\\[\n\\begin{align}\n\\E(e^{tX}) &= \\frac{\\lambda}{\\lambda -t} &, t < \\lambda.\n\\end{align}\n\\]\nLet \\(X_i\\) be a collection of IID exponential random variables with parameter \\(\\lambda\\). Then \\(Z = \\sum X_i\\) satisfies\n\\[\n\\E(e^{tZ}) = \\prod_{i=1}^n \\E(e^{tX_i}) = \\frac{\\lambda^n}{(\\lambda -t)^n}.\n\\]\nSuppose that \\(Y\\) has pdf (Equation 1), then for \\(t < \\lambda\\) we see that be repeated integration by parts\n\\[\n\\begin{align}\n\\E(e^{tY}) &= \\int_0^{\\infty} e^{tx} \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{-\\lambda x} dx \\\\\n&=    \\int_0^{\\infty} \\frac{\\lambda^n}{(n-1)!}x^{n-1} e^{(t -\\lambda) x} dx \\\\\n&= \\int_0^{\\infty} \\frac{\\lambda^n}{(n-1)!} x^{n-1} \\frac{(-1)^n}{(\\lambda -t)^n} \\frac{d^n}{dx^n}e^{(t-\\lambda)x} dx \\\\\n&= \\frac{\\lambda^n}{(n-1)!} \\frac{(-1)^n}{(\\lambda -t)^n} \\int_0^{\\infty} x^{n-1} \\frac{d^n}{dx^n}e^{(t-\\lambda)x} dx \\\\\n&= \\frac{\\lambda^n}{(n-1)!} \\frac{(-1)^n}{(\\lambda -t)^n} (-(n-1)\\int_0^{\\infty} x^{n-2} \\frac{d^{n-1}}{dx^{n-1}}e^{(t-\\lambda)x} dx) \\\\\n&= \\frac{\\lambda^n}{(\\lambda - t)^n} \\frac{(-1)^n}{(n-1)!}(-1)^{n-1} (n-1)!(e^{(t-\\lambda)x}\\rvert_0^{\\infty}) \\\\\n&= \\frac{\\lambda^n}{(\\lambda - t)^n} (-1)^{2n-1} (-1) \\\\\n&= \\frac{\\lambda^n}{(\\lambda -t)^n}.\n\\end{align}\n\\]\nThe MGF characterises the distribution of the random variable so the proof is complete."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#why-sqrt2-pi",
    "href": "FCSC/ch2/ch2_exercises.html#why-sqrt2-pi",
    "title": "Exercises",
    "section": "2.3 Why \\(\\sqrt{2 \\pi}\\)?",
    "text": "2.3 Why \\(\\sqrt{2 \\pi}\\)?\nUsing polar coordinates\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-x^2 - y^2} dx dy &= \\int_0^{2 \\pi} \\int_0^{\\infty} r e^{-r^2} dr d\\theta \\\\\n&=  2 \\pi \\int_0^{\\infty} r e^{-r^2} dr \\\\\n&=  2 \\pi \\int_0^{\\infty} \\frac{-1}{2} \\frac{d}{dr}(e^{-r^2}) dr \\\\\n&=  - \\pi e^{-r^2} \\rvert_0^{\\infty} \\\\\n&= \\pi.\n\\end{align}\n\\]\nNow, \\[\n(\\int_{-\\infty}^{\\infty} e^{-x^2} dx)^2 =\n\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-x^2 - y^2} dx dy = \\pi.\n\\]\nTherefore \\[\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}.\n\\]\nI think the author may have meant for us to show that\n\\[\n\\int_{-\\infty}^{\\infty} e^{-x^2/2} dx = \\sqrt{2 \\pi}\n\\]\nwhich follows by a change of variables \\(x = y/\\sqrt{2}\\)\n\\[\n\\begin{align}\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx &= \\int_{-\\infty}^{\\infty} e^{-(y/\\sqrt{2})^2} \\frac{1}{\\sqrt{2}} dy \\\\\n&= \\frac{1}{\\sqrt{2}} \\int_{-\\infty}^{\\infty} e^{-y^2/2} dy = \\sqrt{\\pi}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#box-muller",
    "href": "FCSC/ch2/ch2_exercises.html#box-muller",
    "title": "Exercises",
    "section": "2.4 Box-Muller",
    "text": "2.4 Box-Muller\nLet \\(U_1 \\sim U(0,1)\\) and \\(U_2 \\sim U(0,1)\\). Define random variables\n\\[\nZ_1 = \\sqrt{-2 \\log(U_1)} \\cos(2 \\pi U_2)\n\\] and \\[\nZ_2 = \\sqrt{-2 \\log(U_1)} \\sin(2 \\pi U_2).\n\\]\nShow that \\(Z_1\\) and \\(Z_2\\) are independent standard Gaussians.\nChange to polar coordinates.\nNote that\n\\[\nR = \\sqrt{Z_1^2 + Z_2^2} = \\sqrt{-2 \\log(U_1)}\n\\]\nand\n\\[\n\\tan(\\Theta) = \\frac{Z_2}{Z_1} = \\tan(2 \\pi U_2).\n\\] so \\[\n\\Theta = 2 \\pi U_2\n\\].\nThe random variable \\(R\\) has CDF\n\\[\n\\begin{align}\nF_R(r) &= P(R \\leq r) \\\\\n&= P(\\sqrt{-2 \\log(U_2)} \\leq r) \\\\\n&= P(U_2 \\geq e^{-r^2/2}) \\\\\n&= 1 - P(U_2 < e^{-r^2/2})\n&= 1 - \\begin{cases}\n0 & \\text{if } e^{-r^2/2} <0, \\\\\ne^{-r^2/2} & \\text{for } 0 \\leq e^{-r^2/2} < 1, \\\\\n1 & \\text{if } e^{-r^2/2} \\geq 1\n\\end{cases} \\\\\n&= 1 - e^{-r^2/2}.\n\\end{align}\n\\]\nObviously, \\(\\Theta \\sim U(0, 2 \\pi)\\). Therefore, \\((Z_1, Z_2)\\) has the same distribution as \\((X, Y)\\) where \\(X, Y\\) are IID standard Gaussians."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#marginally-gaussian-but-not-jointly-gaussian.",
    "href": "FCSC/ch2/ch2_exercises.html#marginally-gaussian-but-not-jointly-gaussian.",
    "title": "Exercises",
    "section": "2.5 Marginally Gaussian but not Jointly Gaussian.",
    "text": "2.5 Marginally Gaussian but not Jointly Gaussian.\nLet \\(X\\) be a standard Gaussian and define\n\\[\nY = \\begin{cases}\nX & \\text{if } |X| \\leq 1, \\\\\n-X & \\text{otherwise.}\n\\end{cases}\n\\]\n\\(Y\\) is also a standard Gaussian.\nProof:\nLet\n\\[\ng(x) = \\begin{cases}\nx & \\text{if } |x| \\leq 1, \\\\\n-x & \\text{otherwise.}\n\\end{cases}\n\\]\nThen the MGF of \\(Y\\) can be expressed (using LOTUS) as\n\\[\n\\begin{align}\n\\sqrt{2 \\pi} \\E(e^{tY}) &= \\sqrt{2 \\pi} \\E(e^{tg(X)}) \\\\\n&= \\int_{-\\infty}^{\\infty} e^{tg(x)} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{-\\infty}^{-1} + \\int_1^{\\infty}) e^{-t x} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{\\infty}^{1} + \\int_{-1}^{-\\infty}) - e^{t x} e^{-x^2/2} dx \\\\\n&= \\int_{-1}^{1} e^{t x} e^{-x^2/2} dx + (\\int_{1}^{\\infty} + \\int_{-\\infty}^{-1} e^{t x} e^{-x^2/2} dx \\\\\n&= \\int_{-\\infty}^{\\infty} e^{tx} e^{-x^2/2} dx \\\\\n&= \\sqrt{2 \\pi} \\E(e^{tX}).\n\\end{align}\n\\]\nTherefore, \\(Y\\) and \\(X\\) are identically distributed. They are definitely not independent as \\(Y\\) is a function of \\(X\\) and so we have no right to expect that \\(X + Y\\) is also Gaussian.\nTo see that \\(X +Y\\) is not Gaussian, note that its range is in \\([-2,2]\\); there are lower bounds on Gaussian tails which are non-zero.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrg = np.random.default_rng()\nN = 10000\nbins = 100\n\nX = rg.normal(0,1, N)\nY = [x if np.abs(x) <=1 else -x for x in X]\nplt.hist(X+Y, bins=bins, label='X + Y', alpha=0.5)\nplt.hist(X, bins, label='X', alpha=0.5)\nplt.hist(Y, bins, label='Y', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp3_f0y_ed/texput.png'\n\n\n\\(\\displaystyle \\left[ \\right]\\)\n\n\n\n\n\n\ndata=[X+Y, X, Y]\nax = plt.subplot()\nax.violinplot(data, range(len(data)), vert=False)\nax.set_yticks(range(len(data)))\nax.set_yticklabels(['X+Y', 'X', 'Y'])\nplt.plot()\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpcpp22eac/texput.png'\n\n\n\\(\\displaystyle \\left[ \\right]\\)"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#pdf-of-brownian-bridge",
    "href": "FCSC/ch2/ch2_exercises.html#pdf-of-brownian-bridge",
    "title": "Exercises",
    "section": "2.6 PDF of Brownian Bridge",
    "text": "2.6 PDF of Brownian Bridge\nLet \\((M_t, t \\in [0,1])\\) be a Brownian bridge.\n\nwrite down the PDF of \\((M_{1/4}, M_{3/4})\\)\n\nThe covariance matrix is\n\n\nCode\nimport sympy as sp\nfrom sympy.abc import s,t\n\ndef cov(s, t):\n    return sp.Min(s,t) -s * t\n\na, b = sp.Rational(1, 4), sp.Rational(3/4)\n\nC = sp.Matrix([[cov(a, a), cov(a, b)],[cov(b, a), cov(b, b)]]); C\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpsnab0re7/texput.png'\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{3}{16} & \\frac{1}{16}\\\\\\frac{1}{16} & \\frac{3}{16}\\end{matrix}\\right]\\)\n\n\nUsing sympy, we can get the PDF easily:\n\n\nCode\nfrom fractions import Fraction\nfrom sympy.abc import x,y\n\nsp.init_printing()\n\n\nCinv = C.inv()\nxy = sp.Matrix([x, y])\n\n\nf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/(2 *sp.pi * sp.sqrt(C.det(), evaluate=False))\npdf = f[0]\npdf\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp267yhslr/texput.png'\n\n\n\\(\\displaystyle \\frac{2 \\sqrt{2} e^{- 3 x^{2} + 2 x y - 3 y^{2}}}{\\pi}\\)\n\n\n\nwrite down the probability of the event \\({M_{1/4}^2 + M_{3/4}^2 \\leq 1}\\) as a double integral\n\n\n\nCode\nr = sp.symbols('r', nonegative=True)\ntheta = sp.symbols('theta')\n\npdf = pdf.subs({x: r*sp.cos(theta), y: r*sp.sin(theta)}).simplify()\n\nprob = sp.Integral(pdf * r, (r, 0, 1), (theta, 0, 2 *sp.pi)); prob\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpfjc_bo3n/texput.png'\n\n\n\\(\\displaystyle \\int\\limits_{0}^{2 \\pi}\\int\\limits_{0}^{1} \\frac{2 \\sqrt{2} r e^{r^{2} \\left(\\sin{\\left(2 \\theta \\right)} - 3\\right)}}{\\pi}\\, dr\\, d\\theta\\)\n\n\nThe integral can be calculated numerically using Simpson’s rule or a Monte Carlo simulation:\n\nfrom scipy.integrate import simps\n\ndef evaluate_integral(integrand):\n    r = np.linspace(0, 1, 1000)\n    theta = np.linspace(0, 2* np.pi, 1000)\n\n    zz = integrand(r.reshape(-1, 1), theta.reshape(1, -1))\n    return simps([simps(zz_r, r) for zz_r in zz], theta)\n\ndef monte_carlo(integrand):\n    samples = 1000\n    sum = 0\n    for _ in range(samples):\n        u = np.random.default_rng().uniform(0, 1, 2)\n        r, theta = u[0], 2*np.pi*u[1]\n        sum +=integrand(r, theta)\n    return 2*np.pi * sum/samples\n\nnumerical_soln = evaluate_integral(sp.lambdify([r, theta], pdf *r)); print(numerical_soln)\n\nmonte_carlo(sp.lambdify([r, theta], pdf*r))\n\n0.926901699267537\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpqj34v8hx/texput.png'\n\n\n\\(\\displaystyle 0.917571490638876\\)\n\n\nAlternatively, we can approximate the desired probability by using the Cholesky decomposition:\n\ndef simulate(samples):\n    rg = np.random.default_rng()\n    C = [[3/16, 1/16], [1/16, 3/16]]\n    A = np.linalg.cholesky(C)\n    samples = 1000\n    in_disk = 0\n    for _ in range(samples):\n        sample = A.dot(rg.normal(0,1, 2))\n        if sample[0]**2 + sample[1]**2 <= 1:\n            in_disk += 1\n    return in_disk/samples\n\nsimulated_result = simulate(1000); simulated_result\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpsosp4e0l/texput.png'\n\n\n\\(\\displaystyle 0.938\\)\n\n\nThe two approaches differ by\n\nabs(simulated_result - numerical_soln)\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpx2x0wi4b/texput.png'\n\n\n\\(\\displaystyle 0.011098300732463\\)"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#the-covariance-of-a-random-vector-is-always-positive-semidefinite",
    "href": "FCSC/ch2/ch2_exercises.html#the-covariance-of-a-random-vector-is-always-positive-semidefinite",
    "title": "Exercises",
    "section": "2.7 The Covariance of a Random Vector is Always Positive Semidefinite",
    "text": "2.7 The Covariance of a Random Vector is Always Positive Semidefinite\nLet \\(\\mathcal{C}\\) be the covariance matrix of a random vector \\(X = (X_i)\\).\nWe must demonstrate that\n\\[\n\\sum_{i,j} a_i a_j \\mathcal{C}_{ij} \\geq 0\n\\tag{2}\\] for any \\(a \\in \\mathbb{R}^n\\).\nDefine \\[\nY = \\sum_{i=1}^n a_i X_i\n\\]\nand calculate the variance of \\(Y\\) showing that it is equal to the left-hand side of (Equation 2):\n\\[\n\\begin{align}\n\\Var(Y) &= \\E(Y^2) - \\E(Y)^2 \\\\\n      &= \\E(\\sum_{i,j=1} a_i a_j X_i X_j ) - \\sum_{i,j=1}^n a_i a_j \\E(X_i)\\E(X_j) \\\\\n      &= \\sum_{i,j=1}^n a_i a_j (\\E(X_i X_j) - \\E(X_i) \\E(X_j)) \\\\\n      &=  \\sum_{i,j=1}^n a_i a_j \\mathcal{C}_{ij}.\n\\end{align}\n\\]\nThe proof is complete by noting that \\(\\Var(Y) \\geq 0\\).\n\n\n\n\n\n\nNote\n\n\n\nThe variance of a random variable is invariant under translation i.e.\nif \\(X\\) is a random variable and \\(c \\in \\mathbb{R}\\), then\n\\[\n\\Var(X + c) = \\Var(X).\n\\] This follows by a simple book-keeping exercise: \\[\n\\begin{align}\n\\Var(X+c) &= \\E(X^2 + 2cX + c^2) - (\\E(X) + c)^2\\\\\n&= \\E(X^2) +2c\\E(x) +c^2 - \\E(X)^2  - 2x \\E(X) - c^2 \\\\\n&= \\E(X^2) - \\E(X)^2 \\\\\n&= \\Var(X).\n\\end{align}\n\\]\nIt is clear that \\(\\Var(X) \\geq 0\\) when \\(X\\) has mean zero:\n\\[\n\\Var(X) = \\E(X^2) = \\int_{-\\infty}^{\\infty} x^2 dF(x) \\geq 0.\n\\]\nIf \\(m\\) is the mean of \\(X\\), then \\(X -m\\) has mean zero and so using invariance of the variance:\n\\[\n\\Var(X) = \\Var(X - m) \\geq 0.\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(X\\) is a constant with probability 1 \\(\\iff \\Var(X) = 0\\).\nProof:\nIf \\(X = c\\), then \\[\n\\Var(X) = \\E(c^2) - \\E(c)^2 = c^2 - c^2 = 0.\n\\]\nIf \\(\\Var(X) = 0\\), then \\(\\Var(X - \\E(X)) = 0\\), by translation invariance of \\(\\Var\\). Let \\(Y = X - \\E(X)\\). Then\n\\[\n\\Var(Y) = \\int_{-\\infty}^{\\infty} y^2 dF_Y(y)\n\\] and so \\[\nF_Y(y) = \\begin{cases}\n0 & \\text{for } y < 0, \\\\\n1 & \\text{otherwise}.\n\\end{cases}\n\\] Therefore, \\(Y = 0\\) with probability 1 and so \\(X = \\E(X)\\) with probability 1 i.e. \\(X\\) is a constant.\n\n\nSince the LHS of (Equation 2) is the variance of \\(\\sum_i a_i X_i\\), we see that \\(\\mathcal{C}\\) is positive definite if and only if the variance of \\(\\sum_i a_i X_i\\) is non-zero for all non-zero \\(a\\).\nThe covariance matrix of a random vector \\(X = (X_i)\\) is positive-definite if and only if each linear combination of the coordinates is non-constant.\nWhen \\(X_i\\) are jointly Gaussian, then each linear combination is a Gaussian random variable: the only constant a linear combination of Gaussian variables can sum to is zero.\nTherefore, for jointly Gaussian random variables, the covariance matrix is positive-definite if and only if the the variables are linearly independent. See Section 12."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#a-linear-transformation-of-gaussian-vector-is-also-gaussian",
    "href": "FCSC/ch2/ch2_exercises.html#a-linear-transformation-of-gaussian-vector-is-also-gaussian",
    "title": "Exercises",
    "section": "2.8 A Linear Transformation of Gaussian Vector is Also Gaussian",
    "text": "2.8 A Linear Transformation of Gaussian Vector is Also Gaussian\nLet \\(X = (X_i)_{i=1}^n\\) be a Gaussian vector an \\(M\\) be an \\(m \\times n\\) matrix.\n\\(Y = MX\\) is also Gaussian.\nProof:\nWe can write\n\\[\n\\begin{align}\nY &= (\\sum_{i=1}^n M_{ij} X_i)_{j=1}^m\n\\end{align}\n\\] and so express \\(Y\\) as a vector of linear combinations of \\(X_i\\) making its components Gaussian. Moreover, we can express any linear combination of components of \\(MX\\) as a linear combination of \\(X_i\\):\n\\[\n\\begin{align}\n\\sum_{j=1}^m a_j Y_j &= \\sum_{j=1}^m a_j (\\sum_{i=1}^n M_{ji} X_i) \\\\\n&= \\sum_{i,j} a_j M_{ji} X_i \\\\\n&= \\sum_{i} b_i X_i\n\\end{align}\n\\] where \\[\nb_i = \\sum_j a_j M_{ji}.\n\\] Written in a more suggestive manner: \\[\na^T MX = (M^T a)^T X.\n\\] \\(\\square\\)\nLet \\(\\mathcal{C}\\) be the covariance matrix of \\(X\\).\nWe can express the covariance matrix of \\(Y\\) in terms of \\(M\\) and \\(\\mathcal{C}\\):\n\\[\n\\begin{align}\n\\Cov(Y_i, Y_j) &= \\E(Y_i Y_j) - \\E(Y_i) \\E(Y_j) \\\\\n&= \\E( \\sum_\\alpha M_{i \\alpha} X_\\alpha \\sum_\\beta M_{j \\beta} X_\\beta ) - \\sum_\\alpha M_{i \\alpha} \\E(Y_i) \\sum_\\beta M_{j \\beta } \\E(Y_j) \\\\\n&= \\sum_\\alpha \\sum_\\beta M_{i\\alpha }M_{j \\beta} (\\E(X_\\alpha X_\\beta) - \\E(X_\\alpha) \\E(X_\\beta)) \\\\\n&= \\sum_\\alpha \\sum_\\beta M_{i \\alpha}\\mathcal{C}_{\\alpha \\beta} M^T_{\\beta j}. \\\\\n\\end{align}\n\\]\nIn shorter notation:\n\\[\n\\Cov(Y) = M \\mathcal{C}M^T.\n\\]\nSuppose that \\(m =n\\). Then,\n\\[\n\\begin{align}\n\\det(\\Cov(Y)) &= \\det(M)\\det(\\mathcal{C})\\det(M^T) \\\\\n&= \\det(M)^2\\det(\\mathcal{C}).\n\\end{align}\n\\] If follows that \\(Y\\) is non-degenerate (\\(\\det(\\Cov(Y)) \\neq 0\\)) \\(\\iff\\) \\(\\det(M) \\neq 0\\).\nFor general \\(M\\), it is necessary that \\(m \\leq n\\) and \\(M\\) is full-rank \\(m\\) for \\(\\Cov(Y)\\) to be positive definite.\n\n\n\n\n\n\nNote\n\n\n\nThe domain of \\(M^T\\) is \\(m\\)-dimensional. The range of \\(M^T\\), \\(\\mathcal{R}(M^T)\\), has dimension \\(\\rank(M^T\\)). Since \\(\\mathcal{C}\\) is invertible, \\(\\mathcal{C}(\\mathcal{R}(M^T))\\) has dimension \\(\\rank(M^T)\\). Finally, \\(\\dim(M(\\mathcal{C}(\\mathcal{R}(M^T)))) = \\rank(M)\\) by virtue of \\(\\rank(M^T) = \\rank(M)\\). That is, \\(\\rank(\\Cov(Y)) = \\rank(M)\\).\nFor \\(\\Cov(Y)\\) to be invertible (making \\(\\Cov(Y)\\) positive definite), we require \\(\\rank(M) = m\\) i.e \\(M\\) must be full-rank. Given that \\(\\rank(M) \\leq \\min(m, n)\\), when \\(M\\) is full-rank \\(m \\leq n\\)."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#iid-decomposition",
    "href": "FCSC/ch2/ch2_exercises.html#iid-decomposition",
    "title": "Exercises",
    "section": "2.9 IID Decomposition",
    "text": "2.9 IID Decomposition\nLet \\((X, Y)\\) be a Gaussian vector with mean \\(0\\) and covariance matrix \\[\n\\mathcal{C} = \\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix},\n\\] for \\(\\rho \\in (-1, 1)\\).\nTake \\(Z_1 = X\\). \\(Z_1\\) is a standard Gaussian: it has mean \\(0\\) by assumption and the covariance matrix tells us that \\(\\Var(X_1) = 1\\).\nWe perform Gram-Schmidt to express \\((X, Y)\\) as a linear combination of IID standard Gaussians. Start with \\[\n\\begin{align}\nZ^{'}_2 &= Y - \\E(YZ_1)Z_1 \\\\\n&= Y - \\rho Z_1.\n\\end{align}\n\\]\nBy linearity of expectation, \\[\n\\begin{align}\n\\Var(Z^{'}_2) &= \\E((Y - \\rho Z_1)^2) \\\\\n&= \\E(Y^2) - 2\\rho \\E(Y Z_1) + \\rho^2 \\E(Z_1^2) \\\\\n&= 1 -  \\rho^2.\n\\end{align}\n\\] We set \\[\nZ_2 = \\frac{1}{\\sqrt{1 - \\rho^2}} Z_2^{'}\n\\] and so \\(Z_2\\) has variance 1: \\(Z_1\\) is a standard Gaussian.\n\n\n\n\n\n\n\\(Z_1\\) and \\(Z_2\\) are independent\n\n\n\n\n\nFor Gaussians, it is sufficient to demonstrate that \\[\n\\E(Z_1 Z_2) = 0.\n\\] We expect this from the Gram-Schmidt process but it doesn’t hurt to check:\n\\[\n\\begin{align}\n\\E(Z_1Z_2) & = \\frac{1}{\\sqrt{1 - \\rho^2}}\\E(X(Y - \\rho X)) \\\\\n&= \\frac{\\E(XY) - \\rho\\E(X^2)}{\\sqrt{1 -\\rho^2}} \\\\\n&= 0.\n\\end{align}\n\\]\n\n\n\nWe can write \\[\n(X, Y) = (Z_1, \\rho Z_1 + \\sqrt{1 - \\rho^2} Z_2)\n\\] to express \\((X, Y)\\) as a linear combination of IID standard Gaussians.\nThe PDF of \\((X, Y)\\) is given by\n\n\nCode\nimport sympy as sp\nfrom sympy.abc import x, y, rho\nfrom fractions import Fraction\n\nsp.init_printing()\n\n\nC = sp.Matrix([[1, rho], [rho, 1]])\nCinv = C.inv()\nfactor = sp.gcd(tuple(Cinv))\nxy = sp.Matrix([x, y])\n\n\nf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/(2 *sp.pi * sp.sqrt(C.det())); f[0]\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpomsjgd7x/texput.png'\n\n\n\\(\\displaystyle \\frac{e^{\\frac{- 2 \\rho x y + x^{2} + y^{2}}{2 \\left(\\rho - 1\\right) \\left(\\rho + 1\\right)}}}{2 \\pi \\sqrt{1 - \\rho^{2}}}\\)"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#iid-decomposition-1",
    "href": "FCSC/ch2/ch2_exercises.html#iid-decomposition-1",
    "title": "Exercises",
    "section": "2.10 IID Decomposition",
    "text": "2.10 IID Decomposition\n\nC = sp.Matrix([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\nA = C.cholesky()\nAinv = A.inv()\n\nz1, z2, z3, x1, x2, x3 = sp.symbols('z_1 z_2 z_3 x_1 x_2 x_3')\n\nx = sp.Matrix([x1, x2, x3])\n\n\nz = sp.MatMul(Ainv, x, evaluate=True); z\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp885eg_3_/texput.png'\n\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{\\sqrt{2} x_{1}}{2}\\\\- \\frac{\\sqrt{6} x_{1}}{6} + \\frac{\\sqrt{6} x_{2}}{3}\\\\- \\frac{\\sqrt{3} x_{1}}{6} - \\frac{\\sqrt{3} x_{2}}{6} + \\frac{\\sqrt{3} x_{3}}{2}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#iid-decomposition-2",
    "href": "FCSC/ch2/ch2_exercises.html#iid-decomposition-2",
    "title": "Exercises",
    "section": "2.11 IID Decomposition",
    "text": "2.11 IID Decomposition\n\\[\n\\mathcal{C}= \\begin{bmatrix}\n3 & 1 & 1 \\\\\n1 & 3 & -1 \\\\\n1 & -1 & 3\n\\end{bmatrix}\n\\]\n\\(\\det{\\mathcal{C}}\\) is\n\n\nCode\nC = [[3, 1, 1], [1,3,-1], [1, -1, 3]]\nnp.linalg.det(C)\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp18o46f_h/texput.png'\n\n\n\\(\\displaystyle 16.0\\)\n\n\nand so \\(X\\) is non-degenerate.\n\nclass Sim211:\n    C = [[3, 1, 1], [1,3,-1], [1, -1, 3]]\n    def __init__(self):\n        self.__A = np.linalg.cholesky(C)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return self.__A.dot(rg.normal(0, 1, len(C[0])))\n\nsim = Sim211()\n\nfor _ in range(100):\n    plt.plot(range(1, 4), sim.path())\nplt.show()"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#sec-degenerate-is-li",
    "href": "FCSC/ch2/ch2_exercises.html#sec-degenerate-is-li",
    "title": "Exercises",
    "section": "2.12 Degenerate Means Linearly Independent",
    "text": "2.12 Degenerate Means Linearly Independent\nLet \\(X = (X_i)_{i=1}^n\\) be a Gaussian vector with covariance \\(\\mathcal{C}\\).\n\\(\\det{\\mathcal{C}} = 0 \\iff\\) there exists \\(c \\in \\mathbb{R}^n\\setminus{0}\\) such that\n\\[\n\\sum_i c_i X_i = 0.\n\\]\nProof:\nBy definition, \\(X\\) is a degenerate Gaussian vector iff \\(\\det{\\mathcal{C}} = 0\\). Since \\(\\mathcal{C}\\) is symmetric semi-positive definite, \\(\\det{\\mathcal{C}} = 0\\) iff there exists \\(c \\in \\mathbb{R}^n\\setminus{0}\\) such that\n\\[\nc^T \\mathcal{C} c = 0.\n\\]\nThis can be written as a statement about the variance of a Gaussian random variable:\n\\[\nc^T \\mathcal{C} c = \\Var(\\sum_i c_i X_i) = 0.\n\\]\nA random variable with variance 0 is a constant; the only constant a Gaussian variable can equal is 0.\nTherefore,\n\\[\nc^T C c = 0 \\iff \\sum_i c_i X_i = 0.\n\\] \\(\\square\\)\nThe textbook hints that we should use \\(\\mathcal{C} = AA^T\\) but I think this needs some care to avoid circularity if we’re using this to fill in proofs in the text. Obviously, we can’t use results where the existence of \\(A\\) is predicated on \\(X\\) being non-degenerate!\nThe approach I take here is to prove the equivalent theorem\n\\(\\det{\\mathcal{C}} \\neq 0 \\iff X_i\\) are linearly independent.\nProof:\nSuppose that \\(X\\) has mean zero without loss of generality.\nSuppose \\((X_i)\\) are linearly independent and so form an \\(n\\)-dimensional vector space \\(V\\) with addition being the usual addition of random variables. The vector space is equipped with the inner product \\(\\E: V \\times V \\mapsto \\mathbb{R}\\). This allows us to can use the Gram-Schmidt process to create \\(n\\) IID (orthogonal wrt \\(\\E\\)) Gaussian standard vectors \\((Z_i)\\) with mean zero. The space \\(V\\) is spanned by both \\((X_i)\\) and \\((Z_i)\\) and so they are related by an invertible change of coordinates \\(A\\) such that \\(X = AZ\\). In addition,\n\\[\n\\mathcal{C} = AA^T\n\\]\nsince\n\\[\n\\mathcal{C}_{ij} = \\E(X_i Xj) = \\E(\\sum_m A_{im} Z_m \\sum_n A_{jn} Z_n) = (AA^T)_{ij}.\n\\]\nSince \\(A\\) is invertible, \\(\\det(A) = \\det(A^T) \\neq 0\\) and so\n\\[\n\\det(\\mathcal{C} ) = \\det(A)^2  > 0.\n\\]\nNow, suppose that \\(\\det{C} \\neq 0\\). Then \\(X\\) is non-degenerate and so there exists an invertible matrix \\(A\\) and \\(n\\) IID standard Gaussian random variables such that \\(X=AZ\\). The variables \\(Z_i\\) are linearly independent and we can leverage this and the invertibility of \\(A^T\\):\n\\[\n\\sum_i c_i X_i = \\sum_i c_i A^T Z_i = A^T (\\sum_i c_i Z_i)\n\\] and so \\(\\sum_i c_i X_i = 0\\) iff \\(\\sum_i c_i Z_i = 0\\) which would be a contradiction if the \\(c_i\\) are not all zero.\nWe have shown that\n\\(\\det{\\mathcal{C}} \\neq 0 \\iff X_i\\) are linearly independent.\nEquivalently, \\[\n\\det{\\mathcal{C}} = 0 \\iff \\exists c \\in \\mathbb{R}^n\\setminus{0} \\text{ such that } \\sum_i c_i X_i = 0.\n\\]\n\\(\\square\\)"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#iid-decomposition-for-degenerate-vectors",
    "href": "FCSC/ch2/ch2_exercises.html#iid-decomposition-for-degenerate-vectors",
    "title": "Exercises",
    "section": "2.13 IID Decomposition For Degenerate Vectors",
    "text": "2.13 IID Decomposition For Degenerate Vectors\nLet \\(X = (X_i)_{i=1}^{n}\\) be a degenerate Gaussian vector with mean zero.\n\n\n\n\n\n\nWarning\n\n\n\nThe question does not state that the mean of \\(X\\) is zero: this appear to be an error.\n\n\nThe Gaussian variables \\(X_i\\) form a finite-dimensional vector space \\(V\\) under addition of random variables and multiplication by reals. The dimension of \\(V\\) is less than \\(n\\), because the \\(X_i\\) are linearly dependent.\nThe expectation operator is an inner-product on \\(V\\) and we can use this inner-product to to form a basis \\(Z = (Z_i)_{i=1}^m\\) \\(m < n\\) for \\(V\\) of IID standard Gaussian variables. Each \\(X_i\\) can be written in exactly one way as a linear combination of \\(Z\\):\n\\[\nX_i = \\sum_{j=1}^m a_{ij} Z_j.\n\\] That is, there is an \\(m \\times n\\) matrix \\(A\\) such that \\[\nX = A Z.\n\\]\nThe elements of the covariance matrix of \\(X\\) can be expressed in terms of \\(A\\):\n\\[\n\\begin{align}\n\\mathcal{C}_{ij} &= \\E(X_i X_j) \\\\\n&= \\E(\\sum_\\alpha a_{i\\alpha} Z_\\alpha \\sum_\\beta a_{j \\beta} Z_\\beta) \\\\\n&= \\sum_{\\alpha, \\beta} a_{i\\alpha}a_{j\\beta} \\E(Z_\\alpha Z_\\beta) \\\\\n&= \\sum_\\alpha a_{i \\alpha} a_{j \\alpha} \\\\\n&= (A A^T)_{ij}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#brownian-bridge-from-brownian-motion",
    "href": "FCSC/ch2/ch2_exercises.html#brownian-bridge-from-brownian-motion",
    "title": "Exercises",
    "section": "2.14 Brownian Bridge From Brownian Motion",
    "text": "2.14 Brownian Bridge From Brownian Motion\nLet \\((B_t, t \\in [0, 1])\\) be a Brownian motion.\n\nShow that the process given by \\((M_t = B_t - t B_1, t \\in [0, 1])\\) is a Brownian bridge.\n\nStep 1: show that \\(M_t\\) is a Gaussian process.\nChoose \\(0 = t_1 < t_2 < ... t_n =1\\). The process \\((M_{t_i})\\) is a linear transformation of \\((B_{t_i})\\):\n\\[\nA_{ij} = \\delta_{ij} - t \\delta_{nj}\n\\]\ne.g for a process with three time points\n\\[\nA = \\begin{bmatrix}\n1 & 0 & -t \\\\\n0 & 1 & -t \\\\\n0 & 0 & 1-t\n\\end{bmatrix}.\n\\]\nStep 2: Show that the mean of \\((M_t)\\) is equal to zero\n\\[\n\\E(M_t) = \\E(B_t - t B_1) = \\E(B_t) - t\\E(B_1) = 0.\n\\]\nStep 3: Show that the \\(\\Cov(M_t, M_s) = \\min(s,t) - st\\).\n\\[\n\\begin{align}\n\\Cov(M_t, M_s) &= \\Cov(B_t -t B_1, B_s - t B_1) \\\\\n&= \\E(B_t B_s - t B_t B_1 -t B_s B_1 + t^2 B_1^2) \\\\\n&= \\E(B_t B_s) -t \\E(B_t B_1) -t \\E(B_s B_1) + t^2 \\E(B_1^2) \\\\\n&= \\min(s, t) - st.\n\\end{align}\n\\]\n\nShow that the random variable \\(B_t - t B_1\\) is independent of of \\(B_1\\) for any \\(t \\in [0, 1]\\).\n\nConsider the covariance:\n\\[\n\\begin{align}\n\\Cov(B_t - t B_1, B_1) &= \\E(B_tB_1 -t B_1 B_1) \\\\\n&= \\E(B_tB_1) -t \\E(B_1^2) \\\\\n&= t - t \\\\\n&= 0.\n\\end{align}\n\\]\nSince \\(B_t -t B_1\\) and \\(B_1\\) are Gaussian variables, this is enough to establish independence."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#triangle-inequality",
    "href": "FCSC/ch2/ch2_exercises.html#triangle-inequality",
    "title": "Exercises",
    "section": "2.15 Triangle Inequality",
    "text": "2.15 Triangle Inequality\nLet \\(X, Y\\) be two random variables in \\(L^2(\\Sigma, \\mathcal{F}, \\P)\\). Prove the triangle inequality\n\\[\n\\sqrt{\\E((X+Y)^2)} \\leq \\sqrt{\\E(X^2) + \\E(Y^2)}.\n\\]\nExpand the square and use Cauchy-Schwarz:\n\\[\n\\begin{align}\n\\E((X+Y)^2) &= \\E(X^2 + 2 X Y + Y^2) \\\\\n&= \\E(X^2) + 2 \\E(XY) + \\E(Y^2) \\\\\n&\\leq \\E(X^2) + 2 \\sqrt{\\E(X^2) \\E(Y^2)} + \\E(Y^2) \\\\\n&= (\\sqrt{\\E(X^2)} + \\sqrt{\\E(Y^2)})^2.\n\\end{align}\n\\]\nLet \\(S_k = \\sum_{i=1}^k X_i\\). Suppose that the triangle inequality holds for \\(S_k\\):\n\\[\n\\sqrt{\\E(S_k^2)} \\leq \\sum_{i=1}^k\\sqrt{\\E(X_i^2)}.\n\\]\nConsider the next case \\(S_{k+1}\\):\n\\[\n\\sqrt{\\E(S_{k+1}^2)}  = \\sqrt{\\E((X_{k+1} + S_k)^2)} \\leq \\sqrt{\\E(X_{k+1}^2)} + \\sqrt{\\E(S_k^2)} = \\sum_{i=1}^{k+1}\\sqrt{\\E(X_k^2)}\n\\] using the two-term triangle inequality. The result follows by induction since the triangle inequality holds for \\(k=2\\)."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#the-space-of-integrable-random-variables-is-l1",
    "href": "FCSC/ch2/ch2_exercises.html#the-space-of-integrable-random-variables-is-l1",
    "title": "Exercises",
    "section": "2.16 The Space of Integrable Random Variables is \\(L^1\\)",
    "text": "2.16 The Space of Integrable Random Variables is \\(L^1\\)\n\\(L^1(\\Omega, \\mathcal{F}, \\P)\\) is a linear space.\nProof:\nIf \\(X, Y \\in L^1\\) and \\(a, b \\in \\mathbb{R}\\), then\n\\[\naX + bY\n\\]\nis a random variable on the probability space and by the triangle inequality\n\\[\n|aX + bY| \\leq |a| |X| + |b| |Y|.\n\\] The right-hand side is integrable so |aX + BY| is integrable. Therefore, \\(aX + bY \\in L^1\\). The zero vector of \\(L^1\\) is a random variable on the probability space which is zero with probability one.\n\\(\\square\\)\n\\(\\| X \\|_1 \\triangleq \\E(|X|)\\) is a norm for \\(L^1\\).\nProof:\nTo verify that \\(\\| \\cdot \\|_1\\) is a norm for \\(L^1\\) we must verify that\n\n\\(\\| X + Y \\|_1 \\leq \\| X\\|_1 + \\|Y \\|_1\\) for \\(X,Y \\in L^1\\)\nFor all scalars \\(a\\) and \\(X \\in L^1\\), \\(\\| a X\\|_1 = |a| \\| X \\|_1\\)\nFor all \\(X \\in L^1\\), if \\(\\| X\\|_1 = 0\\), then \\(X=0\\).\n\n\nBy the triangle inequality for the reals,\n\n\\[\n| X + Y| \\leq |X| + |Y|\n\\] with probability one for \\(X, Y \\in L^1\\) and so\n\\[\n\\| X + Y \\|_1 = \\E(|X + Y|) \\leq \\E(|X|) + \\E(|Y|) = \\|X\\|_1 + \\|Y\\|_1.\n\\]\n\n\n\nFor scalar \\(a\\) and \\(X \\in L^1\\),\n\\[\n\\| a X \\|_1 = \\E(|a X|) = |a| \\E(|X|) = |a| \\| X \\|_1.\n\\]\n\n\n\nIt is clear that if \\(X = 0\\), then \\(\\|X\\|_1 = 0\\).\nIf \\(X \\in L^1\\) and \\(\\| X \\|_1 = 0\\), then\n\\[\n\\E(|X|) =0.\n\\]\nBy Markov’s inequality\n\\[\n\\P(|X| > x) \\leq \\frac{1}{x} \\E(|X|) = 0\n\\] for \\(x > 0\\) i.e.\n\\[\nP(|X| > x) = 0\n\\] for \\(x> 0\\). It follows that \\(P(|X| \\leq 1/n) = 1\\) for all \\(n=1, 2, \\ldots\\).\nTherefore, we can create a decreasing, nested sequence of events \\(A_n = \\{\\omega \\in \\Omega : \\left |X(\\omega)\\right| \\leq 1/n\\}\\) and by continuity of probability \\[\n\\P(X = 0) = \\lim_{n\\to\\infty} P(X \\leq 1/n) =  1.\n\\]\nTherefore, \\(X = 0\\)."
  },
  {
    "objectID": "FCSC/ch2/ch2_exercises.html#wicks-formula",
    "href": "FCSC/ch2/ch2_exercises.html#wicks-formula",
    "title": "Exercises",
    "section": "2.17 Wick’s Formula",
    "text": "2.17 Wick’s Formula\n\nLet \\(Z=(Z_i)_{i=1}^n\\) be IID standard Gaussians and let \\(G: \\mathbb{R}^n \\mapsto \\mathbb{R}\\) be a smooth function for which \\(\\E(G(Z))\\) and \\(\\E(\\partial_i G(Z))\\) are well-defined for every \\(i \\leq n\\). Prove that\n\n\\[\n\\E(Z_i G(Z)) = \\E(\\partial_iG(Z)).\n\\]\nProof:\nBy integration by parts for the one-dimensional Gaussian and the law of total expectation\n\\[\n\\begin{align}\n\\E(\\E(\\partial_iG(Z)|\\cap_{j\\neq i} Z_j)) &= \\E(\\E(Z_i G(Z)|\\cap_{j \\neq i} Z_j)) \\\\\n&= E(Z_i G(Z)).\n\\end{align}\n\\]\n\nLet \\(X\\) be a non-degenerate Gaussian vector of mean zero, and let \\(F: \\mathbb{R}^n \\mapsto \\mathbb{R}\\) be a smooth function for which \\(\\E(F(X))\\) and \\(\\E(\\partial_iF(X))\\) are well-defined for every \\(i \\leq n\\). Prove that\n\n\\[\n\\E(X_i F(X)) = \\sum_{j \\leq n} \\E(X_i X_j) \\E(\\partial_iF(X))\n\\] for \\(i \\leq n\\).\n\n\n\n\n\n\nWarning\n\n\n\nTypo in the question.\nLHS should read \\(\\E(X_i F(X))\\) instead of \\(\\E(X_i F(Z))\\).\n\n\nProof:\nLet \\(A\\) be the Cholesky decomposition of the covariance matrix of \\(X\\). Then\n\\[\nX = AZ\n\\]\nwhere \\(Z=(Z_i)\\) are IID standard Gaussian variables with mean zero.\nDefine \\(G(Z) = F(AZ)\\). Then\n\\[\n\\begin{align}\n\\E(Z_i G(Z)) &= \\E(\\partial_i G(Z)) \\\\\n&= \\E(\\partial_i F(AZ)) \\\\\n&= \\E(\\sum_j \\partial_j F(X) A_{ji}) \\\\\n&= \\sum_j A_{ji} E(\\partial_j F(X)) \\\\.\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nDefine \\(a(x) = Ax\\). Then by the chain rule \\[\n\\begin{align}\n\\partial_i(F(a_1(x), \\ldots, a_n(x))) &= \\sum_j \\frac{\\partial F}{\\partial a_j}\\frac{\\partial a_j}{\\partial x_i} \\\\\n&= \\sum_j \\partial_j F(a_1(x), \\ldots, a_n(x)) A_{ji}.\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\n\\E(X_i F(X)) &= \\E(\\sum_k A_{ik} Z_k G(Z)) \\\\\n&= \\sum_k A_{ik} \\E(\\partial_k G(Z)) \\\\\n&= \\sum_k A_{ik} \\sum_j \\E(\\partial_j F(X) A_{kj}) \\\\\n&= \\sum_j \\sum_k A_{ik} A_{kj} \\E(\\partial_j F(X) ) \\\\\n&= \\sum_j  (AA^T)_{ij} \\E(\\partial_j F(X)) \\\\\n&= \\sum_j  \\E(X_i X_j) \\E(\\partial_j F(X)).\n\\end{align}\n\\]\n\nFor any \\(m\\)-tuple \\((i_1, \\ldots , i_m)\\) where \\(i_k \\leq n\\) we have the \\(E(X_{i_1}\\ldots X_{i_m}) = 0\\) if \\(m\\) is odd, and if \\(m\\) is even,\n\n\\[\n\\E(X_{i_1} \\ldots X_{i_m}) = \\sum_{p \\in P_m^2} \\prod_{p = (p_1, p_2)} \\E(X_{p_1} X_{p_2})\n\\]\nwhere \\(P_m^2\\) is the set of all pairing of elements of the \\(m\\)-tuple.\nProof:\nFor \\(m=2\\), the result is trivial.\nSuppose true for \\(m\\) even. Define \\(F(X) = X_{i_1} \\ldots X_{i_m} X_{i_{m+1}}\\)\nThen\n\\[\n\\begin{align}\n\\E(X_{i_1} \\ldots X_{i_{m+2}}) &= \\E(F(X) X_{i_{m+2}}) \\\\\n&= \\sum_{j \\leq m+1} \\E(X_{i_{m+2}} X_j) \\sum_{p\\in P_m} \\prod_{p=(p_1, p2)} E(X_{p_1}X_{p_2}) \\\\\n&= \\sum_{p \\in P_{m+2}^2} \\prod_{p = (p_1, p_2)} \\E(X_{p_1} X_{p_2}).\n\\end{align}\n\\] By induction, the theorem is true for even \\(m\\).\nFor \\(m\\) odd, the base case is \\(m=3\\). Define \\(F(X) = X_{i_2} X_{i_3}\\).\nThen \\[\n\\begin{align}\n\\E(X_{i_1} X_{i_2} X_{i_3}) &= \\E(X_{i_1} X_{i_2})\\E(X_{i_3}) + \\E(X_{i_1} X_{i_3}) \\E(X_{i_4}) \\\\\n&= 0.\n\\end{align}\n\\]\nSuppose that the theorem is true for odd \\(m\\). Then for \\(m+2\\), define \\(F(X) = X_{i_1}\\ldots X_{i_{m+1}}\\).\n\\[\n\\begin{align}\n\\E(X_{i_1} \\ldots X_{i_{m+2}}) &= \\E(F(X) X_{i_{m+2}}) \\\\\n&= \\sum_{j \\leq m+2} \\E(X_{i_{m+2}}X_j) \\E(\\partial_j F(X)) \\\\\n&= 0\n\\end{align}\n\\]\nsince \\(\\partial_j F(X)\\) is the product of an odd number of elements of \\(X\\).\nAnother approach to proving the odd case is to appeal to symmetry:\nfor any \\(i\\), the distribution of \\(-X_i\\) is the same as that of \\(X_i\\). Moreover, any product \\(X_{i_1} \\ldots X_{i_n}\\) will have the same distribution as \\((-1)^n X_{i_1} \\ldots X_{i_n}\\). Therefore,\n\\[\n\\E (X_{i_1} \\ldots X_{i_n}) = (-1)^n\\E(X_{i_1} \\ldots X_{i_n}).\n\\] When \\(n\\) is odd, \\[\n\\E (X_{i_1} \\ldots X_{i_n}) = -\\E(X_{i_1} \\ldots X_{i_n}) = 0.\n\\]\n\\(\\square\\)\n```"
  },
  {
    "objectID": "FCSC/ch2/ch2_experiments.html",
    "href": "FCSC/ch2/ch2_experiments.html",
    "title": "Chapter Two: Computer Experiments",
    "section": "",
    "text": "Let \\(U_1 \\sim U(0,1)\\) and \\(U_2 \\sim U(0,1)\\). Define random variables\n\\[\nZ_1 = \\sqrt{-2\\log(U_1)} \\cos(2 \\pi U_2)\n\\] and \\[\nZ_2 = \\sqrt{-2\\log(U_1)} \\sin(2 \\pi U_2).\n\\]\nGenerate \\(10000\\) samples of \\((Z_1, Z_2)\\) and plot the histograms of each random variable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrg = np.random.default_rng()\n\nN = 10000\nbins=100\n\nU1 = rg.uniform(0, 1, N)\nU2 = rg.uniform(0, 1, N)\n\nnormal = rg.normal(0, 1, N)\n\nZ1 = [np.sqrt(-2 * np.log(u[0])) * np.cos(2 * np.pi * u[1]) for u in zip(U1, U2)]\nZ2 = [np.sqrt(-2 * np.log(u[0])) * np.sin(2 * np.pi * u[1]) for u in zip(U1, U2)]\n\n\nplt.hist(normal, bins=bins, label='normal', alpha=0.5)\nplt.hist(Z1, bins=bins, label='Z1', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\n[]\n\n\n\n\n\n\nplt.hist(normal, bins=bins, label='normal', alpha=0.5)\nplt.hist(Z2, bins=bins, label='Z2', alpha=0.5)\nplt.legend(loc='upper right')\nplt.plot()\n\n[]\n\n\n\n\n\n\nplt.hist2d(Z1, Z2, bins=bins, density=True)\nplt.plot()\n\n[]\n\n\n\n\n\n\n\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch2/ch2_experiments.html#simulating-brownian-motion",
    "href": "FCSC/ch2/ch2_experiments.html#simulating-brownian-motion",
    "title": "Chapter Two: Computer Experiments",
    "section": "2.2 Simulating Brownian Motion",
    "text": "2.2 Simulating Brownian Motion\n\nclass Brownian:\n    def __init__(self, samples):\n        covariance = [[min(i,j) for i in range(1, samples)] for j in range(1, samples)]\n        self.__A = (1/np.sqrt(samples))*np.linalg.cholesky(covariance)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return np.r_[0, self.__A.dot(rg.normal(0, 1, samples -1))]\n\n\n\nsamples = 100\nx_axis = np.linspace(0, 1, samples)\n\nbrownian = Brownian(samples)\n\nfor _ in range(100):\n    plt.plot(x_axis, brownian.path())\nplt.title('Brownian motion')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch2/ch2_experiments.html#simulating-the-ornstein-uhlenbeck-process",
    "href": "FCSC/ch2/ch2_experiments.html#simulating-the-ornstein-uhlenbeck-process",
    "title": "Chapter Two: Computer Experiments",
    "section": "2.3 Simulating the Ornstein-Uhlenbeck Process",
    "text": "2.3 Simulating the Ornstein-Uhlenbeck Process\n\nclass OU:\n    def __init__(self, samples):\n        covariance = [ [ (1/2)*np.exp(-(1/samples) * np.abs(i -j))*(1 - np.exp(-(2/samples)*min(i, j))) for i in range(1, samples)] for j in range(1, samples)]\n        self.__A = np.linalg.cholesky(covariance)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return np.r_[0, self.__A.dot(rg.normal(0, 1, samples-1))]\n\nou = OU(samples)\n\nfor _ in range(100):\n    plt.plot(x_axis, ou.path())\nplt.title('Ornstein-Uhlenbeck Process')\nplt.show()\n\n\n\n\n\nclass OUStationary:\n    def __init__(self, samples):\n        covariance = [ [ (1/2)*np.exp(-(1/samples) * np.abs((i -j))) for i in range(samples)] for j in range(samples)]\n        self.__A = np.linalg.cholesky(covariance)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return self.__A.dot(rg.normal(0, 1, samples))\n\n\nouStationary = OUStationary(samples)\n\nfor _ in range(100):\n    plt.plot(x_axis, ouStationary.path())\nplt.title('Stationary Ornstein-Uhlenbeck Process')\n\nText(0.5, 1.0, 'Stationary Ornstein-Uhlenbeck Process')"
  },
  {
    "objectID": "FCSC/ch2/ch2_experiments.html#simulating-fractional-brownian-motion",
    "href": "FCSC/ch2/ch2_experiments.html#simulating-fractional-brownian-motion",
    "title": "Chapter Two: Computer Experiments",
    "section": "2.4 Simulating Fractional Brownian Motion",
    "text": "2.4 Simulating Fractional Brownian Motion\n\nclass FractionalBrownian:\n    def __init__(self, samples, H):\n        def cov(i,j):\n            scale = 1/samples\n            i, j = scale*i, scale*j\n            return (1/2) *(i**(2*H) + j**(2*H) - np.abs(i -j)**(2*H))\n        covariance = [[cov(i, j) for i in range(1, samples)] for j in range(1, samples)]\n        self.__A = np.linalg.cholesky(covariance)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return np.r_[0, self.__A.dot(rg.normal(0, 1, samples-1))]\n\n\n\n\nfor H in [0.1, 0.5, 0.9]:\n    fractionalBrownian = FractionalBrownian(samples, H)\n    for _ in range(100):\n        plt.plot(x_axis, fractionalBrownian.path())\n    plt.title(f'Fractional Brownian H = {H}')\n    plt.show()"
  },
  {
    "objectID": "FCSC/ch2/ch2_experiments.html#simulating-the-brownian-bridge",
    "href": "FCSC/ch2/ch2_experiments.html#simulating-the-brownian-bridge",
    "title": "Chapter Two: Computer Experiments",
    "section": "2.5 Simulating the Brownian Bridge",
    "text": "2.5 Simulating the Brownian Bridge\n\n\n\n\nclass BrownianBridge:\n    def __init__(self, samples):\n        def cov(i, j):\n            scale = 1/samples\n            s = min(i,j)*scale\n            t = max(i,j)*scale\n            return s*(1 - t)\n\n        covariance = [[cov(i,j) for i in range(1, samples -1)] for j in range(1, samples -1)]\n        self.__A = np.linalg.cholesky(covariance)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return np.r_[np.r_[0, self.__A.dot(rg.normal(0, 1, samples-2))], 0]\n\n\nbrownianBridge = BrownianBridge(samples)\nfor _ in range(100):\n    plt.plot(x_axis, brownianBridge.path())\nplt.title('Brownian Bridge')\nplt.show()\n\n\n\n\n\n\n\n\nclass BrownianBridgeAlt:\n    def __init__(self, samples):\n        self.__brownian = Brownian(samples)\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        path = self.__brownian.path(rg)\n        return [ path[i] - (i/samples) * path[-1] for i in range(len(path)) ]\n\nbrownianBridgeAlt = BrownianBridgeAlt(samples)\n\nfor _ in range(100):\n    plt.plot(x_axis, brownianBridgeAlt.path())\nplt.title('Brownian Bridge Generated from Brownian Motion')\nplt.show()\n\n\n\n\n\n\n\n\nseed = 2\npath_a = brownianBridge.path(rg = np.random.default_rng(seed))\npath_b = brownianBridgeAlt.path(rg = np.random.default_rng(seed))\n\nplt.plot(x_axis, path_a, label='(a)')\nplt.plot(x_axis, path_b, label='(b)')\nplt.legend(loc='upper right')\nplt.show()\n\nplt.plot(x_axis, path_b - path_a)\nplt.plot(x_axis, x_axis)\nplt.title('difference between brownian bridge paths')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html",
    "href": "FCSC/ch4/ch4_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\G}{\\mathcal{G}}\n\\newcommand{\\qed}{\\tag*{$\\square$}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#conditional-expectation-of-continuous-random-variables",
    "href": "FCSC/ch4/ch4_exercises.html#conditional-expectation-of-continuous-random-variables",
    "title": "Exercises",
    "section": "4.1 Conditional Expectation of Continuous Random Variables",
    "text": "4.1 Conditional Expectation of Continuous Random Variables\nLet \\((X, Y)\\) be two random variables with joint density \\(f(x, y)\\) on \\(\\mathbb{R}^2\\). Suppose that\n\\[\n\\int_{\\mathbb{R}} f(x, y)\\, dx > 0\n\\] for every \\(y \\in \\mathbb{R}\\).\nThen \\(\\E(Y | X) = h(X)\\) where\n\\[\nh(x) = \\frac{\\int_{\\mathbb{R}} y f(x, y) dy}{\\int_{\\mathbb{R}} f(x, y) dy}.\n\\]\nProof:\nThe conditional expectation \\(E(Y | X)\\) is the function \\(\\eta : \\mathbb{R} \\to \\mathbb{R}\\) satisifying\n\\[\n\\E(g(X) Y) = \\E(g(X)\\eta(X))\n\\tag{1}\\]\nfor any bounded random variable of the form \\(g(X)\\) for some function \\(g\\). That is,\n\\[\nY - \\eta\n\\] is othorgonal to \\(g(X)\\).\nWe can show that \\(h(X) = \\E(Y | X)\\) by showing that it satisfies (Equation 1) and invoking the uniqueness of such a function.\nLet \\(g\\) be any function such that \\(g(X)\\) is bounded and measurable. Using LOTUS\n\\[\n\\begin{align}\n\\E(g(X) Y) &= \\int \\int g(x) y f(x, y)\\, dx\\, dy \\\\\n&= \\int g(x) \\left (\\int y f(x, y) \\, dy\\,\\right) dx \\\\\n&= \\int g(x) \\left (\\frac{\\int y f(x, y) \\, dy}{\\int f(x, y)\\, dy}\\right )\\left(\\int f(x, y)\\, dy\\right) \\, dx \\\\\n&= \\int \\int g(x) h(x) f(x, y) \\,dx\\,dy \\\\\n&= \\int g(x) h(x) f_X(x) \\,dx \\\\\n&= \\E(g(X) h(X)).\n\\end{align}\n\\]\nTherefore \\(h= \\E(Y|X)\\). In particular, setting \\(g = 1\\), we see that\n\\[\n\\E(\\E(Y|X)) = \\E(Y).\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#exercises-on-sigma-fields",
    "href": "FCSC/ch4/ch4_exercises.html#exercises-on-sigma-fields",
    "title": "Exercises",
    "section": "4.2 Exercises on Sigma-Fields",
    "text": "4.2 Exercises on Sigma-Fields\n\nLet \\(A, B\\) be two proper subset of \\(\\Omega\\) such that \\(A \\cap B \\neq \\emptyset\\).\n\nPartition \\(\\Omega\\) into \\(4\\) disjoint elements of \\(\\sigma\\):\n\\[\n\\Omega = (A\\setminus B) \\cup (B \\setminus A) \\cup (A \\cap B) \\cup (A \\cup B)^c.\n\\]\nTo ease notation, define\n\\[\n\\begin{align}\nS_0 &= A \\setminus B, \\\\\nS_1 &= B \\setminus A, \\\\\nS_2 &= A \\cap B \\\\\nS_3 &= (A \\cup B)^c.\n\\end{align}\n\\]\nEach element of \\(\\sigma\\) can be expressed as a union of at most \\(4\\) of these sets: the number of elements of \\(\\sigma\\) is \\(2^4 = 16\\).\nEnumerating these:\n\\[\n\\begin{align}\n0000 &\\to \\emptyset, \\\\\n0001 &\\to A \\setminus B, \\\\\n0010 &\\to B \\setminus A, \\\\\n0011 &\\to (A \\setminus B) \\cup (B \\setminus A) = (A \\cup B) \\setminus (A\\cap B), \\\\\n0100 &\\to A \\cap B, \\\\\n0101 &\\to (A \\cap B) \\cup (A \\setminus B) = A, \\\\\n0110 &\\to (A \\cap B) \\cup (B \\setminus A) = B, \\\\\n0111 &\\to (A \\cap B) \\cup (B \\setminus B) \\cup (A \\setminus B) = A \\cup B, \\\\\n1000 &\\to (A \\cup B)^c, \\\\\n1001 &\\to (A \\cup B)^c \\cup (A \\setminus B) = B^c,\\\\\n1010 &\\to (A \\cup B)^c \\cup (B \\setminus A) = A^c, \\\\\n1011 &\\to (A \\cup B)^c \\cup (B \\setminus A) \\cup (A \\setminus B) = (A \\cap B)^c,\\\\\n1100 &\\to (A \\cup B)^c \\cup (A \\cap B) = (A \\setminus B)^c \\cap (B \\setminus A)^c, \\\\\n1101 &\\to (A \\cup B)^c \\cup (A \\cap B) \\cup (A \\setminus B) = (B \\setminus A)^c, \\\\\n1110 &\\to (A \\cup B)^c \\cup (A \\cap B) \\cup (B \\setminus A) = (A \\setminus B)^c, \\\\\n1111 &\\to \\Omega.\n\\end{align}\n\\]\nIf \\(A \\cap B = \\emptyset\\), then \\(\\Omega\\) can be paritioned into the union of \\(A\\), \\(B\\), \\((A \\cup B)^c\\): there are \\(2^3 = 8\\) events in \\(\\sigma\\). Alternatively, we can just go through the list of \\(16\\) above and cross off those which end up being empty or duplicate.\n\nShow that the singleton \\(\\{b\\} \\in \\mathcal{B}(\\mathbb{R})\\).\n\nFor \\(n \\geq 0\\), \\(I_n = (b - 1/n, b] \\in \\mathcal{B}(\\mathbb{R})\\). The countable itersection of \\(\\cap_{n \\geq 1} I_n = \\{b\\}\\) is also in the sigma-algebra.\nAll open intervals are in the Borel sigma-field: \\((a, b) = (a, b] \\cap \\{b\\}^c\\). All closed intervals are in the Borel sigma-field: \\([a, b] = \\{a\\} \\cup (a, b) \\cup \\{b\\}\\).\nIs the subset \\(\\mathbb{Q}\\) a Borel set? That is, is \\(\\mathcal{Q} \\in \\mathcal{B}(\\mathbb{R})\\)? Yes: the rationals are countable, so they can be expressed as the union of singleton rational sets which are in the sigma-field \\(\\mathcal{B}(\\mathbb{R})\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#proof-of-theorem-4.16",
    "href": "FCSC/ch4/ch4_exercises.html#proof-of-theorem-4.16",
    "title": "Exercises",
    "section": "4.3 Proof of Theorem 4.16",
    "text": "4.3 Proof of Theorem 4.16\nDefine \\(L^2(\\G) = L^2(\\Omega, \\G, \\P)\\). Let \\(Y\\) be a random variable in \\(L^2(\\Omega, \\F, \\P)\\) and let $Y^{} satisfy\n\\[\n\\min_{Z \\in L^2(\\G)} \\E((Y - Z)^2) = \\E((Y - Y^{\\star})^2).\n\\tag{2}\\]\nWe show that \\(E(Y | X) = Y^{\\star}\\).\nLet \\(W\\) be a random variable in \\(L^2(\\G)\\): we show that \\(W\\) is orthogonal to \\(Y - Y^{\\star}\\) which shows that \\(Y^{\\star}\\) is the orthogonal projection of \\(Y\\) into \\(L^2(\\G)\\).\nDeveloping the sqaure:\n\\[\n\\E((W - (Y - Y^{\\star}))^2) = \\E(W^2) - 2 \\E(W(Y - Y^{\\star})) + \\E((Y - Y^{\\star})^2).\n\\]\nFrom the definition of \\(Y^{\\star}\\) (and the fact that \\(W + Y^{\\star} \\in L^2(\\G)\\))\n\\[\n\\E((W - (Y - Y^{\\star}))^2) \\geq \\E(Y - Y^{\\star}).\n\\]\nIt follows that \\[\n\\E(W^2) \\geq 2\\E(W(Y - Y^{\\star})).\n\\]\nThis holds for any \\(W\\) and so we can assert that \\[\na^2\\E(W^2) \\geq 2 a\\E(W(Y - Y^{\\star})).\n\\] Taking \\(a > 0\\), we find that\n\\[\n\\E(W(Y - Y^{\\star})) \\leq \\frac{a\\E(W^w)}{2}\n\\]\nand for \\(a < 0\\)\n\\[\n\\E(W(Y - Y^{\\star})) \\geq \\frac{a\\E(W^w)}{2}.\n\\]\nTaking the limit as \\(a \\to 0\\), we see that\n\\[\n\\E(W(Y - Y^{\\star})) = 0\n\\]\nwhich is a defining propery of the conditional expectation \\(E(Y | \\G)\\).\nUniqueness:\nSuppose that there are two conditional expectations of \\(Y\\) given \\(\\G\\), \\(C_1\\) and \\(C_2\\). Then for any \\(W \\in L^2(\\G)\\),\n\\[\n\\E(W(C_1 - C_2)) = 0.\n\\]\nSetting \\(W = C_1 - C_2\\), we see that\n\\[\n\\E((C_1 - C_2)^2) = \\| C_1 - C_2 \\|_2^2 = 0\n\\]\nand so \\(C_1 = C_2\\).\n\n\n\n\n\n\nNote\n\n\n\nThe proof of Theorem 4.7 in the book shows uniqueness of \\(Y^{\\star}\\) satifying (Equation 2) but does not actually show that a conditional expection is such a variable."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#another-look-at-conditional-expectation-for-gaussians",
    "href": "FCSC/ch4/ch4_exercises.html#another-look-at-conditional-expectation-for-gaussians",
    "title": "Exercises",
    "section": "4.4 Another Look at Conditional Expectation For Gaussians",
    "text": "4.4 Another Look at Conditional Expectation For Gaussians\nLet \\((X, Y)\\) be a Gaussian vector with mean \\(0\\) and covariance matrix\n\\[\n\\begin{align}\n\\mathcal{C} &= \\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1.\n\\end{bmatrix}\n\\end{align}\n\\]\n\nThe conditional expectation can be calculated using the \\(L^2\\) best approximation form:\n\n\\[\n\\begin{align}\n\\E(Y|X) &= \\frac{\\E(XY) X}{\\E(X^2)} \\\\\n&= \\rho X.\n\\end{align}\n\\]\n\nThe joint pdf of \\((X, Y)\\) is\n\n\nimport sympy as sp\nfrom fractions import Fraction\nx, y, rho = sp.symbols('x, y, rho', real=True)\n\nC = sp.Matrix([[1, rho], [rho, 1]])\nCinv = C.inv()\nxy = sp.Matrix([x, y])\n\n\npdf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/((2 * sp.pi)**(Fraction(len(xy),2)) * sp.sqrt(C.det()))\npdf[0]\n\n\\(\\displaystyle \\frac{e^{\\frac{- 2 \\rho x y + x^{2} + y^{2}}{2 \\left(\\rho - 1\\right) \\left(\\rho + 1\\right)}}}{2 \\pi \\sqrt{1 - \\rho^{2}}}\\)\n\n\n\\[\n\\begin{align}\n\\frac{\\int_{\\mathbb{R}} y f(x, y)\\, dy}{\\int_{\\mathbb{R}} f (x, y) \\, dy} &= \\frac{\\int_{\\mathbb{R}} y e^{\\frac{-2 \\rho xy + x^2 + y^2}{2(\\rho -1)(\\rho + 1)}}\\, dy}{\\int_{\\mathbb{R}}e^{\\frac{-2 \\rho xy + x^2 + y^2}{2(\\rho -1)(\\rho + 1)}}\\, dy} \\\\\n&= \\frac{\\int_{\\mathbb{R}} y e^{\\frac{-2 \\rho xy + y^2}{2(\\rho -1)(\\rho + 1)}}\\, dy}{\\int_{\\mathbb{R}}e^{\\frac{-2 \\rho xy + y^2}{2(\\rho -1)(\\rho + 1)}}\\, dy} \\\\\n&= \\frac{\\int_{\\mathbb{R}} (t + \\rho x) e^{\\frac{-t^2}{2(1-\\rho)(1+\\rho)}} \\, dt}{\\int_{\\mathbb{R}} e^{\\frac{-t^2}{2(1 -\\rho)(1 + \\rho)}} \\, dt} \\\\\n&= \\sqrt{1 - \\rho^2} \\frac{\\int_{\\mathbb{R}} t e^{-t^2/2}\\,dt}{\\int_{\\mathbb{R}} e^{-t^2/2}\\, dt} + \\rho x \\\\\n&= \\rho x = h(x).\n\\end{align}\n\\]\nThis is another way of arriving at \\[\n\\E(Y|X) = h(X).\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning",
    "href": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning",
    "title": "Exercises",
    "section": "4.5 Gaussian Conditioning",
    "text": "4.5 Gaussian Conditioning\n\\[\n\\mathcal{C} = \\begin{bmatrix}\n2 & 2 & 0 \\\\\n2 & 4 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\]\nIt is easy to see that \\(\\det{C} = 4\\); non-zero determinant means that the vector \\((X_1, X_2, X_3 X_3)\\) is non-degenerate.\nFrom the covariance matrix, we see that \\(\\E(X_1X_3) = \\E(X_2 X_3) = 0\\), so \\(X_3\\) is independent of \\(X_1\\) and \\(X_2\\).\n\\[\n\\begin{align}\n\\E(X_2 | X_1) &= \\frac{\\E(X_2X_1)}{\\E(X_1^2)}X_1 \\\\\n&= \\frac{2}{2} X_1 \\\\\n&= X_1.\n\\end{align}\n\\]\nTherefore\n\\[\nX_2 = X_1 + (X_2 - X_1)\n\\]\nis a decomposition of \\(X_2\\) into a linear combination of \\(X_1\\) and a random variable independent of \\(X_1\\), namely \\(X_2 - X_1\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-1",
    "href": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-1",
    "title": "Exercises",
    "section": "4.6 Gaussian Conditioning",
    "text": "4.6 Gaussian Conditioning\n\\((X, Y)\\) is a Gaussian random vector with mean \\(0\\) and covariance given by\n\\[\n\\mathcal{C} = \\begin{bmatrix}\n3/16 & 1/8 \\\\\n1/8 & 1/4 \\\\\n\\end{bmatrix}.\n\\]\n\\((X, Y)\\) is non-degenerate because \\(\\det{C} > 0\\):\n\nC = sp.Matrix([[Fraction(3, 16), Fraction(1, 8)], [Fraction(1,8), Fraction(1,4)]])\nC.det()\n\n\\(\\displaystyle \\frac{1}{32}\\)\n\n\n\\[\n\\E(Y|X) = \\frac{\\E(YX)}{\\E(X^2)}X = (1/8) * (16/3)X =2X/3.\n\\]\n\\(W = (Y - 2X/3)\\) is independent of \\(X\\) (it is orthogonal to all functions of \\(X\\)). We need\n\\[\n\\E(W^2) = \\E(Y^2 - 4XY/3 + 4X^2/9) = \\frac{1}{4} - \\frac{4}{8.3} + \\frac{4.3}{9.16} = \\frac{1}{6}.\n\\]\nWe can calculate the MGF of \\(Y\\) conditioned on \\(X\\):\n\\[\n\\begin{align}\n\\E(e^{aY} | X) &= \\E(e^{a(W + 2X/3)} | X) \\\\\n&= e^{2X/3} \\E(e^{aW} X) \\\\\n&= e^{2X/3} \\E(e^{aW})  \\\\\n&= e^{2X/3+ a^2/12}  \\\\\n\\end{align}\n\\]\nand so the conditional distribution of \\(Y\\) given \\(X\\) is a Gaussian with mean \\(2X/3\\) and variance \\(1/6\\).\nWe define \\(Z_1 = 16X/3\\) and \\(Z_2 = 6(Y - 2X/3)\\): \\(Z_1\\) and \\(Z_2\\) are standard Gaussians and\n\\[\nX = 16X/3\n\\] and \\[\nY = Z_1/9 + Z_2/6.\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-2",
    "href": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-2",
    "title": "Exercises",
    "section": "4.7 Gaussian Conditioning",
    "text": "4.7 Gaussian Conditioning\n\\[\n\\mathcal{C} = \\begin{bmatrix}\n1 & -1 \\\\\n-1 & 2 \\\\\n\\end{bmatrix}.\n\\]\n\\(Z_1 = X_1\\).\n\\[\n\\begin{align}\nX_2 &= (X_2 - \\E(X_2| Z_1)Z_1) + \\E(X_2|Z_1) Z_1 \\\\\n&= (X_2 + Z_1) - Z_1.\n\\end{align}\n\\]\nSo, set \\(Z_2 = X_2 + X_1\\).\nChecking:\n\\[\n\\E(Z_1) = \\E(X_1) = 0\n\\]\nand\n\\[\n\\E(Z_1^2) = \\E(X_1^2) = 1.\n\\]\n\\[\n\\E(Z_2) = \\E(X_2) + \\E(X_1) = 0\n\\] and\n\\[\n\\begin{align}\n\\E(Z_2^2) &= \\E(X_1^2 + 2X_1X_2  + X_2^2)  \\\\\n&= 1 -2.1 + 2 \\\\\n&= 1.\n\\end{align}\n\\]\nMoreover,\n\\[\n\\begin{align}\n\\E(Z_2Z_1)  &= \\E((X_1 + X_2) X_1) \\\\\n&= \\E(X_1^2 + X_1 X_2) \\\\\n&= 1 - 1 \\\\\n&= 0.\n\\end{align}\n\\]\n\\[\n\\E(X_2 | X_1) = \\frac{\\E(X_2 X_1)}{\\E(X_1^2)}X_1 = -X_1.\n\\]\n\\[\n\\begin{align}\n\\E(e^{a X_2} | X_1) &= \\E(e^{a (Z_2 - Z_1)}| Z_1) \\\\\n&= e^{-a Z_1} \\E(e^{a Z_2}| Z_1) \\\\\n&= e^{-a Z_1} \\E(e^{a Z_2}) \\\\\n&= e^{-aZ_1 + a^2/2} \\\\\n&= e^{-aX_1 + a^2/2}.\n\\end{align}\n\\] using the independence of \\(Z_2\\) and \\(Z_1\\) and the MGF of a Gaussian of mean \\(0\\) and variance \\(1\\). The conditional distribution of \\(X_2\\) given \\(X_1\\) is a Gaussian with mean \\(-X_1\\) and variance \\(1\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-3",
    "href": "FCSC/ch4/ch4_exercises.html#gaussian-conditioning-3",
    "title": "Exercises",
    "section": "4.8 Gaussian Conditioning",
    "text": "4.8 Gaussian Conditioning\nWe can use the Cholesky factorisation of the covariance matrix to get a mapping from \\((Z_1, Z_2, Z_3)\\) of IID standard Gaussians to \\((X_1, X_2, X_3)\\):\n\nsp.Matrix([[2,1,1],[1,2,1],[1,1,2]]).cholesky()\n\n\\(\\displaystyle \\left[\\begin{matrix}\\sqrt{2} & 0 & 0\\\\\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{6}}{2} & 0\\\\\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{6}}{6} & \\frac{2 \\sqrt{3}}{3}\\end{matrix}\\right]\\)\n\n\n\\[\n\\begin{align}\nX_1 &= \\sqrt{2} Z_1, \\\\\nX_2 &= \\frac{Z_1}{\\sqrt{2}} + \\sqrt{\\frac{3}{2}} Z_2, \\\\\nX_3  &= \\frac{Z_1}{\\sqrt{2}} + \\frac{Z_2}{\\sqrt{6}} + \\frac{2 Z_3}{\\sqrt{3}}.\n\\end{align}\n\\]\nWe note that\n\\[\nX_3 = X_2 -\\frac{\\sqrt{3}}{2}X_1 + \\frac{2 Z_3}{\\sqrt{3}}.\n\\]\nThis can be used to compute \\(\\E(X_3 |X_2, X_1)\\):\n\\[\n\\begin{align}\n\\E(X_3 | X_2, X_1) &= \\E(X_2 - \\frac{\\sqrt{3}}{2}X_1 + \\frac{2 Z_3}{\\sqrt{3}}| X_2, X_1) \\\\\n&=  X_2 - \\frac{\\sqrt{3}}{2} X_1 + \\E(\\frac{2 Z_3}{\\sqrt{3}}|X_2, X_1) \\\\\n&=  X_2 - \\frac{\\sqrt{3}}{2} X_1 + \\E(\\frac{2 Z_3}{\\sqrt{3}}) \\\\\n&=  X_2 - \\frac{\\sqrt{3}}{2} X_1,\n\\end{align}\n\\]\nwhere we’ve used \\(\\E(X_1|X_1, X_2) = X_1\\) and that \\(Z_3\\) is independent of \\(X_1\\) and \\(X_2\\) (they are linear combinations of \\(Z_1\\) and \\(Z_2\\)).\nWe can also compute \\(\\E(e^{aX_3} | X_2, X_1)\\):\n\\[\n\\begin{align}\n\\E(e^{aX_3}| X_2, X_1) &= e^{a(X_2 -\\frac{\\sqrt{3}}{2}X_1)} \\E(e^{\\frac{2 a Z_3}{\\sqrt{3}}}|X_2, X_1) \\\\\n&= e^{a(X_2 -\\frac{\\sqrt{3}}{2}X_1)} \\E(e^{\\frac{2 a Z_3}{\\sqrt{3}}}) \\\\\n&= e^{a(X_2 -\\frac{\\sqrt{3}}{2}X_1 + \\frac{2a^2}{3})}.\n\\end{align}\n\\] The conditional distribution is a Gaussian with mean \\(X_2 - \\frac{\\sqrt{3}}{2}X_1\\) and variance \\(4/3\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#properties-of-conditional-expectation",
    "href": "FCSC/ch4/ch4_exercises.html#properties-of-conditional-expectation",
    "title": "Exercises",
    "section": "4.9 Properties of Conditional Expectation",
    "text": "4.9 Properties of Conditional Expectation\n\nIf \\(Y\\) is a \\(\\G\\)-measurable random variable and \\(X\\) is another integrable random variable (with \\(XY\\) also integrable), then\n\n\\[\n\\E(XY | \\G) = Y \\E(X | \\G).\n\\]\nProof:\nLet \\(W\\) be a bounded, \\(\\G\\)-measurable random variable.\n\\[\n\\begin{align}\n\\E (W Y \\E( X | \\G)) &= \\E(W XY) \\\\\n&= \\E(W \\E(XY | \\G)).\n\\end{align}\n\\]\n\nIf \\(Y\\) is independent of \\(\\G\\), that is, for any events \\(I = \\{Y \\in (a, b]\\}\\) and \\(A \\in \\G\\),\n\n\\[\n\\P(I \\cap A) = \\P(I) \\P(A),\n\\] then, \\[\n\\E(Y | \\G) = \\E(Y).\n\\]\nProof:\nIf \\(W\\) is \\(\\G\\)-measurable, then \\(W\\) and \\(Y\\) are independent.\n\\[\n\\begin{align}\n\\E(W \\E(Y)) &= \\E(Y) \\E(W) & (\\text{linearity of expectation}) \\\\\n&= \\E(WY) & (\\text{$W$ and $Y$ are independent}) \\\\\n&= \\E(W \\E(Y | \\G)) & (\\text{by definition}).\n\\end{align}\n\\]\nBy uniqueness of the conditional expectation, \\(\\E(Y) = \\E(Y | \\G)\\).\n\nLinearity: Let \\(X\\) be another integrable random variable on \\((\\Omega, \\F, \\P)\\). Then\n\n\\[\n\\E(a X + b Y| \\G) = a \\E(X | \\G) + b \\E( Y | \\G),\n\\] for any \\(a, b \\in \\mathbb{R}\\).\nProof:\n\\[\n\\begin{align}\n\\E(W (a X + b Y)) &= a \\E(WX) + b \\E(WY) & (\\text{by linearity of expectation}) \\\\\n&= a \\E(W\\E(X | \\G)) + b \\E(W \\E(Y | \\G)) & (\\text{by defn of conditional expectation})\n\\end{align}\n\\]\nbut\n\\[\n\\E(W\\E((a X + b Y) | \\G)) = \\E(W(a X + b Y))\n\\] by definition. It follows from the uniqueness of the conditional expectation that\n\\[\n\\E(W\\E((a X + b Y) | \\G)) = \\E(W(a X + b Y)| \\G)\n\\] for \\(a, b \\in \\mathbb{R}\\).\n\nTower Property: If \\(\\mathcal{H} \\subseteq \\mathcal{G}\\) is a sigma-field of \\(\\Omega\\), the\n\n\\[\n\\E(Y | \\mathcal{H}) = \\E(\\E(Y | \\mathcal{G})|\\mathcal{H}).\n\\]\nProof:\n\\[\n\\E(W \\E(Y | \\mathcal{H})) = \\E(W Y)\n\\]\nby definition.\n\\[\n\\E(W \\E(\\E(Y | \\G) | \\mathcal{H})) = \\E(W \\E(Y | \\G)) = \\E(WY)\n\\] by definition. Equating the two, we see that\n\\[\n\\E(W \\E(\\E(Y | \\G) | \\mathcal{H})) = \\E(W \\E(Y | \\mathcal{H}))\n\\] and so\n\\[\n\\E(Y | \\mathcal{H}) = \\E(\\E(Y | \\mathcal{G})| \\mathcal{H}).\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#square-of-brownian-motion",
    "href": "FCSC/ch4/ch4_exercises.html#square-of-brownian-motion",
    "title": "Exercises",
    "section": "4.10 Square of Brownian Motion",
    "text": "4.10 Square of Brownian Motion\nLet \\((B_t, t \\geq 0)\\) be a standard Brownian motion. Then \\(M_t = B_t^2 - t\\) is a martingale for the Brownian filtration.\nProof:\nFirstly, the process \\((M_t)\\) is adapted to the Brownian filtration because both \\(B_t^2\\) and \\(t\\) are measurable w.r.t the Brownian filtration at \\(t\\).\nSecondly,\n\\[\n\\E(|M_t|) \\leq E(B_t^2) + t = 2t < \\infty\n\\] for \\(t \\geq 0\\).\nLastly, we use\n\\[\n\\E(B_t - B_s | \\F_s) = \\E(B_t - B_s) = t - s\n\\] since \\(B_t - B_s\\) is independent of \\(B_s\\), \\[\n\\E(B_s^2 | \\F_s) = B_s^2\n\\] because \\(B_s^2\\) is \\(F_s\\)-measurable:\n\\[\n\\begin{align}\n\\E(M_t | \\F_s) &= \\E(B_t^2 - t |\\F_s) \\\\\n&= \\E(B_t^2 | \\F_s) - t \\\\\n&= \\E((B_t - B_s + B_s)^2 | \\F_s) -t \\\\\n&= \\E((B_t - B_s)^2 + 2B_s(B_t - B_s) + B_s^2|\\F_s) -t \\\\\n&= \\E((B_t - B_s)^2) + 2 \\E(B_s) \\E(B_t - B_s) + B_s^2 - t \\\\\n&= (t -s) + B_s^2 -t \\\\\n&= B_s^2 -t \\\\\n&= M_s\n\\end{align}\n\\] for any \\(s \\leq t\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#geometric-poisson-process",
    "href": "FCSC/ch4/ch4_exercises.html#geometric-poisson-process",
    "title": "Exercises",
    "section": "4.11 Geometric Poisson Process",
    "text": "4.11 Geometric Poisson Process\nLet \\((N_t, t \\geq 0)\\) be a Poisson proces of intensity \\(\\lambda\\). For \\(\\alpha > 0\\), the process \\((e^{\\alpha N_t - \\lambda t(e^{\\alpha} -1)}, t \\geq 0)\\) is a martingale for the filtration of the Poisson process \\((N_t, t \\geq 0)\\).\nProof: Let \\(M_t\\) denote the process. Clearly, \\(M_t\\) is measurable on the filtration of the Poisson process and, using the MGF of the Poisson distribution\n\\[\n\\begin{align}\n\\E(|M_t|) &= \\E(M_t) \\\\\n&= e^{-\\lambda t(e^{\\alpha} -1)} \\E(e^{\\alpha N_t}) \\\\\n&= e^{-\\lambda t(e^{\\alpha} -1)} \\E(e^{\\alpha (N_t - N_0 + N_0}) \\\\\n&= e^{-\\lambda t(e^{\\alpha} -1)} e^{\\lambda t (e^{\\alpha} - 1)} \\E(e^{\\alpha N_0}) \\\\\n&= 1 < \\infty.\n\\end{align}\n\\]\nUsing the MGF of the Poisson distribution (again):\n\\[\n\\begin{align}\n\\E(M_t | \\F_s) &= \\E(e^{\\alpha N_t - \\lambda t(e^{\\alpha} -1)} | \\F_s) \\\\\n&= \\E(e^{\\alpha N_t} e^{- \\lambda t(e^{\\alpha} -1)} | \\F_s) \\\\\n&= e^{- \\lambda t(e^{\\alpha} -1)} \\E(e^{\\alpha N_t} | \\F_s) \\\\\n&= e^{- \\lambda t(e^{\\alpha} -1)} \\E(e^{\\alpha (N_t - N_s + N_s)} | \\F_s) \\\\\n&= e^{- \\lambda t(e^{\\alpha} -1)} \\E(e^{\\alpha (N_t - N_s)} e^{\\alpha N_s} | \\F_s) \\\\\n&= e^{- \\lambda t(e^{\\alpha} -1)} e^{\\alpha N_s}\\E(e^{\\alpha (N_t - N_s)}) \\\\\n&= e^{- \\lambda t(e^{\\alpha} -1)} e^{\\alpha N_s}e^{\\lambda (t - s)(e^{\\alpha} - 1)} \\\\\n&= e^{\\alpha N_s -\\lambda s(e^{\\alpha} - 1)} \\\\\n&= M_s\n\\end{align}\n\\] for \\(t \\geq s\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#another-brownian-martingale",
    "href": "FCSC/ch4/ch4_exercises.html#another-brownian-martingale",
    "title": "Exercises",
    "section": "4.12 Another Brownian Martingale",
    "text": "4.12 Another Brownian Martingale\n\n(a)\n\\(M_t = t B_{t} - B_{t}^3/3\\) is a martingale\nProof:\n\\(M_t\\) is measurable wrt the Brownian filtration.\nWe use a standard trick of creatively adding zero to \\(B_t\\):\n\\[\n\\begin{align}\n\\E(B_t^3 | \\F_s) &= \\E(((B_t - B_s) + B_s)^3 | \\F_s) \\\\\n&= \\E((B_t - B_s)^3 + 3(B_t - B_s)^2B_s + 3(B_t - B_s) B_s^2 + B_s^3 | \\F_s) \\\\\n&= \\underbrace{\\E((B_t -B_s)^3)}_{B_t - B_s, B_s \\text{ are independent}} + \\underbrace{3B_s \\E((B_t - B_s)^2)}_{B_s \\text{ is } \\F_s-\\text{measurable}} + B_s^3 \\\\\n&= 3 B_s(t -s) + B_s^3.\n\\end{align}\n\\]\nRearranging:\n\\[\n\\begin{align}\nM_s &= sB_s - B_s^3/3\\\\\n&= \\underbrace{t B_s}_{\\E(B_t|\\F_s) = B_s} - \\E(B_t^3/3|F_s) \\\\\n&= \\E(t B_t - B_t^3/3 | F_s) \\\\\n&= \\E(M_t | \\F_s)\n\\end{align}\n\\]\nfor any \\(s \\leq t\\).\n\n\n(b)\nLet \\(a > 0\\)\n\\(M_t = t B_{t} - B_{t}^3/3\\) is a martingale; the stopped martingale \\(M_{t \\wedge \\tau}\\) is also a martingale and so\n\\[\n\\E(M_{\\tau \\wedge t}) = \\E(M_0) = 0.\n\\]\nWe can’t use Doob’s optional stopping theorem directly as \\(M_{\\tau\\wedge t}\\) is not bounded. However, we can argue that\n\\[\n\\lim_{t \\to \\infty} \\E(\\tau \\wedge t B_{\\tau \\wedge t}) = \\E(\\tau B_\\tau)\n\\] by the dominated convergence theorem:\n\\[\n\\left | \\tau \\wedge t B_{\\tau \\wedge t} \\right| \\leq \\left| \\tau (a \\vee b) \\right|\n\\] and \\(\\E(| \\tau |) = \\E(\\tau) < \\infty\\). Similar reasoning for \\(B_{\\tau \\wedge t}^3\\) gives\n\\[\n\\begin{align}\n0 &= \\E(M_0) = E(M_{\\tau\\wedge t})\\\\\n&= \\lim_{t\\to \\infty} E(M_{\\tau \\wedge t})\\\\\n&= \\E(\\tau B_{\\tau} - B_{\\tau}^3/3).\n\\end{align}\n\\]\nWe apply this to get our result:\n\\[\n\\begin{align}\n\\E(\\tau B_{\\tau}) &= \\E(\\tau B_{\\tau} - B_{\\tau}^3/3 + B_{\\tau}^3/3) \\\\\n&= \\E(B_{\\tau}^3/3) \\\\\n&=  a^3 \\P(B_{\\tau} = a) + (-b)^3\\P(B_{\\tau} = -b)  \\\\\n&=  \\frac{a^3b + (-b)^3a}{3(a + b)} \\\\\n&=  \\frac{ab(a^2 -b^2)}{3(a + b)} \\\\\n&= \\frac{ab}{3}(a -b).\n\\end{align}\n\\]\n\n\n(c)\nThe errata mentions that the notation chosen is unfortunate. Let’s consider\n\\[\nM_t = e^{\\lambda B_t - \\lambda^2 t/2}.\n\\]\n\\(M_t\\) is a martingale for \\(\\lambda > 0\\).\nProof:\n\\(M_t\\) is adapted to the Brownian filtration.\n\\(M_t\\) is integrable:\n\\[\n\\begin{align}\n\\E(|M_t|) &= \\E(e^{\\lambda B_t - \\lambda^2 t/2}) \\\\\n&= e^{-\\lambda^2 t/2} \\underbrace{e^{\\lambda^2 t/2}}_{\\text{MGF of } \\lambda B_t} \\\\\n&= 1.\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\E(e^{\\lambda B_t - \\lambda^2 t/2}| \\F_s) &= e^{\\lambda^2 t/2} \\E(e^{\\lambda (B_t - B_s) + \\lambda B_s } | \\F_s) \\\\\n&= e^{\\lambda^2 t/2} e^{\\lambda B_s} \\E(e^{\\lambda (B_t - B_s)}) \\\\\n&= e^{\\lambda^2 t/2} e^{\\lambda B_s} \\underbrace{e^{\\lambda^2(t-s)/2}}_{\\text{MGF of } \\lambda(B_t - B_s)} \\\\\n&= M_s.\n\\end{align}\n\\] \\[\\qed\\]\nWe can use Doob’s optional stopping theorem to assert that\n\\[\n\\E(e^{\\lambda B_{\\tau} - \\lambda^2 \\tau/2}) = 1.\n\\]\n\n\n(d)\nWith the notation \\(X_\\tau = \\lambda B_{\\tau} - \\lambda^2 \\tau/2\\): \\[\n\\begin{align}\n\\frac{d}{d\\lambda}\\E(e^{X_\\tau}) &=  \\E(e^{X_{\\tau}}\\frac{dX_\\tau}{d\\lambda}) = 0, \\\\\n\\frac{d^2}{d\\lambda^2}\\E(e^{X_\\tau}) &=  \\E(e^{X_{\\tau}}(\\frac{d^2X_\\tau}{d\\lambda^2} + (\\frac{dX_{\\tau}}{d\\lambda})^2)) = 0,\\\\\n\\frac{d^3}{d\\lambda^3}\\E(e^{X_\\tau}) &=  \\E(\\frac{dX_{\\tau}}{d\\lambda} e^{X_{\\tau}}(\\frac{d^2X_\\tau}{d\\lambda^2} + (\\frac{dX_{\\tau}}{d\\lambda})^2) + e^{X_{\\tau}}(\\frac{d^3X_{\\tau}}{d\\lambda^3} + 2\\frac{dX_{\\tau}}{d\\lambda}\\frac{d^2X_{\\tau})}{d\\lambda^2}) = 0\\\\\n\\end{align}\n\\] for \\(n \\geq 1\\).\nWe have the following:\n\\[\n\\begin{align}\n\\frac{dX_\\tau}{d\\lambda} &= B_{\\tau} - \\lambda \\tau, \\\\\n\\frac{d^2X_\\tau}{d\\lambda^2} &= -\\tau, \\\\\n\\frac{d^3X_\\tau}{d\\lambda^3} &= 0.\n\\end{align}\n\\]\nSubstituting and taking the limit as \\(\\lambda \\to 0\\):\n\\[\n\\begin{align}\n\\frac{d^3}{d \\lambda^3} \\E(e^{X_{\\tau}}) &= \\E(e^{X_\\tau}\\left ((B_\\tau - \\lambda \\tau)(-\\tau +B_{\\tau}^2 -2 \\lambda \\tau B_{\\tau} + \\lambda \\tau^2) -2 \\tau(B_\\tau - \\lambda \\tau) \\right)) \\\\\n& \\to \\E(B_{\\tau}^3 - 3 \\tau B_{\\tau}) =0.\n\\end{align}\n\\]\nAs we have already seen \\[\n\\begin{align}\n\\E(B_{\\tau}^3) &= (a^3b - b^3a)/(a + b) = ab(a^2 - b^2)/(a+b) = ab(a - b)\n\\end{align}\n\\] so\n\\[\n\\E(\\tau B_{\\tau}) = \\frac{1}{3}\\E(B_{\\tau}^3) = \\frac{ab}{3}(a -b).\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#limit-of-geometric-brownian-motion",
    "href": "FCSC/ch4/ch4_exercises.html#limit-of-geometric-brownian-motion",
    "title": "Exercises",
    "section": "4.13 Limit of Geometric Brownian Motion",
    "text": "4.13 Limit of Geometric Brownian Motion\nDefine \\(M_t = S_0 e^{\\sigma B_t + \\mu t}\\) for fixed \\(\\sigma > 0\\) andd \\(\\mu < 0\\).\nWe have \\[\n\\lim_{t \\to \\infty} \\frac{B_t}{t} = 0\n\\] almost surely: in particular, there is an event \\(A \\subseteq \\Omega\\) with \\(\\P(A) =1\\) such that for any \\(\\varepsilon > 0\\) there exists \\(t(\\varepsilon) > 0\\) such that for \\(t > t(\\varepsilon)\\)\n\\[\n|B_t| < \\varepsilon t.\n\\]\nFor \\(\\omega \\in A\\), choose \\(\\varepsilon < - \\mu/ 2\\sigma\\), then\n\\[\ne^{\\sigma B_t + \\mu t} \\leq e^{\\sigma |B_t| + \\mu t} < e^{\\frac{\\mu t}{2 \\sigma}}\n\\] for \\(t > t(\\varepsilon)\\) i.e. for \\(\\omega \\in A\\),\n\\[\ne^{\\sigma B_t + \\mu t} \\to 0\n\\] as \\(t \\to 0\\).\nOn the other hand,\n\\[\n\\begin{align}\n\\E(e^{\\sigma B_t + \\mu t}| \\F_s) &= e^{\\mu t} \\E(e^{\\sigma(B_t - B_s) + \\sigma B_t}| \\F_s) \\\\\n&=  e^{\\sigma B_s + \\mu t} \\E(e^{\\sigma(B_t - B_s)}) \\\\\n&=  e^{\\sigma B_s + \\mu t} e^{\\sigma^2(t-s)^2/2}. \\\\\n\\end{align}\n\\]\nTaking the expectation of both sides we see that\n\\[\n\\E(M_t) = \\E(M_s)e^{\\sigma^2(t-s)^2/2}.\n\\]\nIn particular,\n\\[\n\\E(M_t) = S_0 e^{\\sigma^2 t^2/2}.\n\\]\nIn \\(L^1\\):\n\\[\n\\| M_t \\|_1 = |S_0| e^{\\sigma^2 t^2/2} \\to \\infty\n\\] as \\(t \\to \\infty\\).\nIn \\(L^2\\):\n\\[\n\\| M_t \\|_2^2  = \\E(M_t^2) = S_0^2 e^{2 \\sigma^2 t^2} \\to \\infty\n\\] as \\(t \\to \\infty\\).\nTherefore \\(M_t \\to 0\\) almost surely but does not converge to zero in \\(L^1\\) or \\(L^2\\)."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#gamblers-ruin-at-french-roulette",
    "href": "FCSC/ch4/ch4_exercises.html#gamblers-ruin-at-french-roulette",
    "title": "Exercises",
    "section": "4.14 Gambler’s Ruin at French Roulette",
    "text": "4.14 Gambler’s Ruin at French Roulette\nLet \\((S_n, n \\geq 0)\\) be a simple random walk with bias starting at \\(S_0 = 100\\) with\n\\[\nS_n = S_0 + \\sum_{k=1}^n X_k,\n\\] where \\(\\P(X_1 = +1) = p\\) and \\(\\P(X_1 = -1) = 1 - p =q\\) with \\(p < 1/2\\).\n\n(a)\n\\[\nM_n = (q/p)^{S_n}\n\\]\nis a martingale for the filtration \\((\\F_n, n \\in \\mathbb{N})\\) where \\[\n\\F_n = \\sigma(X_m, m \\leq n).\n\\]\nProof:\n\\(M_n\\) is measurable wrt to \\(\\F_n\\) as it is a function of measurable random variables on \\(\\F_n\\).\nFor \\(n \\in \\mathbb{N}\\)\n\\[\n\\begin{align}\n\\E(|M_n|) &= \\E(|(q/p)^{S_n}|) \\\\\n&= \\E((q/p)^{S_n}) \\\\\n&= 2^{S_n} < \\infty.\n\\end{align}\n\\]\n\\(M_n\\) has the martingale property:\n\\[\n\\begin{align}\n\\E(M_n |\\F_m) &= \\E((q/p)^{S_n} | \\F_m) \\\\\n&= \\E((q/p)^{S_n - S_m}(q/p)^{S_m}| \\F_m) \\\\\n&= (q/p)^{S_m} \\underbrace{\\E((q/p)^{S_n - S_m})}_{\\text{increments are independent}} \\\\\n&= M_s \\E((q/p)^{X_{m+1} + \\cdots + X_{n}}) \\\\\n&= M_s \\sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \\P(\\sum_{i=1}^{n-m} X_i = k) \\\\\n&= M_s \\sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \\binom{n-m}{(k + (n-m))/2}p^{(k + (n-m))/2}q^{(n-m) -(k +(n+m))/2} \\\\\n&= M_s (pq)^{(n-m)/2} \\sum_{k=-(n-m),k+=2}^{n-m} (q/p)^k \\binom{n-m}{(k + (n-m))/2}(p/q)^{k/2} \\\\\n&= M_s (pq)^{(n-m)/2} \\sum_{k=-(n-m),k+=2}^{n-m} \\binom{n-m}{(k + (n-m))/2}(q/p)^{k/2} \\\\\n&= M_s (pq)^{(n-m)/2} \\sum_{k=0}^{n-m} \\binom{n-m}{k}(q/p)^{k -(n-m)/2} \\\\\n&= M_s p^{(n-m)} \\sum_{k=0}^{n-m} \\binom{n-m}{k}(q/p)^k \\\\\n&= M_s p^{(n-m)} (1 + q/p)^{n-m} \\\\\n&= M_s p^{(n-m)} (1/p)^{n-m} \\\\\n&= M_s.\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(b)\nDefine the stopping time \\(\\tau = \\min\\{n \\geq 0: S_n = 200 \\text{ or } S_n = 0\\}\\). Then\n\\[\n\\P(\\tau < \\infty) = 1.\n\\]\nProof:\nLet \\[\nE_n = \\{\\sum_{i=200n}^{200(n+1)} X_i = 200\\}.\n\\]\nIt is clear that if any \\(E_n\\) occurs, then \\(\\tau < \\infty\\): that is,\n\\[\n\\{ \\tau < \\infty \\} \\subseteq \\cap_n E_n^c.\n\\] By independence,\n\\[\n\\P(E_n) = \\P(E_0).\n\\]\nSo\n\\[\n\\P(E_0^c \\cap \\ldots \\cap E_n^c) = (1- P(E_0))^n \\to 0\n\\] as \\(n \\to \\infty\\).\nThe events \\(F_n = \\cap_{i=0}^n E_n\\) are decreasing so we can use continuity of probability to state that\n\\[\nP(\\cap_n F_n) = 0.\n\\]\nThe result follows by noting that \\(\\{ \\tau = \\infty\\} \\subseteq \\cap_n F_n\\). \\[\\qed\\].\nTherefore\n\\[\n\\P(\\tau = \\infty) \\leq \\P(\\cap E_n^c) = 0.\n\\]\n\n\n(c)\n\\[\n\\P( S_{\\tau} = 200) = \\frac{1-(q/p)^{100}}{1 - (q/p)^{200}}.\n\\]\nProof:\n\\(\\tau\\) is a stopping time. \\(M_{\\tau \\wedge t}\\) is bounded so Doob’s optional stopping theorem applies:\n\\[\n\\E(M_\\tau) = \\E(M_0) = (q/p)^{S_0}.\n\\]\n\\(S_{\\tau}\\) is a discrete random variable taking values \\(0\\) and \\(200\\) so it’s easy to calculate the LHS of the above:\n\\[\n\\begin{align}\n\\E((q/p)^{S_\\tau}) &= (q/p)^0 \\P(S_\\tau = 0) + (q/p)^{200} \\P(S_\\tau = 200) \\\\\n&= (1 - P(S_\\tau = 200)) + (q/p)^{200} \\P(S_\\tau = 200) \\\\\n&= 1 - P(S_\\tau = 200)(1 + (q/p)^{200}) = (q/p)^{100}.\n\\end{align}\n\\]\nRearranging yields the result: \\[\nP(S_{\\tau} = 200) = \\frac{1 - (q/p)^{100}}{1 - (q/p)^{200}}.\n\\]\n\\[\n\\qed\n\\]\n\n\n(d)\n\np = 18/38\nq = 1 - p\n\n((1 - (q/p)**100)/(1 - (q/p)**200))\n\n2.656069339841474e-05\n\n\n\n\n(e)\nWe seek a starting point \\(S_0\\) for which the probability of hitting \\(100\\) is the same as \\(200\\). I estimate this will be close to \\(200\\), given the numerical experiments on roulette.\n\\[\n\\begin{align}\n\\E((q/p)^{S_\\tau}) &= (q/p)^{100} \\P(S_\\tau = 100) + (q/p)^{200} \\P(S_\\tau = 200) \\\\\n&= (q/p)^{S_0}\n\\end{align}\n\\] because \\((q/p)^{S_n}\\) is a martingale.\nIf \\[\\P(S_\\tau = 100) = \\P(S_\\tau = 200) = 1/2,\\] then \\[\n\\begin{align}\n(q/p)^{S_0 - 100} = 1/2(1 + (q/p)^{100})\n\\end{align}\n\\]\nand so\n\\[\nS_0 = \\frac{\\log(\\frac{1 + (q/p)^{100}}{2})}{\\log(q/p)} + 100\n\\]\n\nimport math\nS_0 = math.log((1 + (q/p)**100)/2)/math.log(q/p) + 100; S_0\n\n193.42143861781375"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#la-martingale-classique",
    "href": "FCSC/ch4/ch4_exercises.html#la-martingale-classique",
    "title": "Exercises",
    "section": "4.15 La Martingale Classique",
    "text": "4.15 La Martingale Classique\nDefine a process \\(M_n\\) by\n\\[\nM_n = \\sum_{k=0}^{n-1} 2^k(S_{k+1} - S_k)\n\\] where \\(S_0 = 0\\) and\n\\[\nS_n = \\sum_k X_k\n\\] where \\((X_k, k \\geq 1)\\) are IID random variables that take value \\(\\pm 1\\) with probability \\(1/2\\). We are given that \\(M_n\\) is a martingale.\n\n(a)\nConsider the stopping time \\(\\tau\\), the first time \\(m\\) with \\(X_m = +1\\). \\[\n\\E(M_{\\tau}) = 1\n\\] yet\n\\[\n\\E(M_0) = 0.\n\\]\nProof:\n\n\n\n\n\n\nNote\n\n\n\nThe way the winnings/losses \\(M_n\\) is expressed makes this question easy as the telescoping sum is then obvious. It could have been written as\n\\[\nM_n = \\sum_{k=0}^{n-1} 2^k X_{k+1}\n\\] and the trick may not be as immediate unless you are familiar with properties of binary expansions of integers: \\[\n2^{n} = 1 + \\sum_{k=0}^{n-1} 2^{k}.\n\\]\n\n\n\\[\n\\begin{align}\n\\E(M_\\tau) &= \\E(\\sum_{k=0}^{m-1} 2^k(S_{k+1} - S_k)) \\\\\n&= \\E(\\sum_{k=0}^{m-2} 2^k((-1) - (-1)) + 1) \\\\\n&= \\E(1) \\\\\n&= 1.\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\E(M_0) &= 0\n\\end{align}\n\\] by definition. \\[\n\\qed\n\\]\n\n\n(b)\nOptional stopping doesn’t apply here because \\(M_{\\tau \\wedge t}\\) is not bounded: \\(M_{\\tau \\wedge t}\\) can take arbitrarily large negative values before reaching the first \\(m\\) such that \\(X_m = +1\\).\n\n\n(c)\nThe weakness of this strategy is that to execute it, the player needs to have unbounded money to gamble and unbounded time to play."
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#a-martingale-from-conditional-expectation",
    "href": "FCSC/ch4/ch4_exercises.html#a-martingale-from-conditional-expectation",
    "title": "Exercises",
    "section": "4.16 A Martingale From Conditional Expectation",
    "text": "4.16 A Martingale From Conditional Expectation\nLet \\(X \\in L^1(\\Omega, \\F, \\P)\\), and let \\((\\F_t, t \\geq 0)\\) be a filtration. Then the process defined by\n\\[\nM_t = \\E(X | \\F_t)\n\\]\nis a martingale.\nProof:\nClearly, \\(M_t\\) is measurable on \\(\\F_t\\). Also, \\(M_t\\) is integrable:\n\\[\n\\E(|M_t|) = \\E(|E(X | \\F_t)|) \\leq \\E(\\E(|X||\\F_t)) = \\E(|X|) < \\infty.\n\\]\nFor \\(s \\leq t\\)\n\\[\n\\begin{align}\n\\E(M_t | \\F_s) &= \\E(\\E(X|\\F_t) |F_s) \\\\\n&= \\underbrace{\\E(X| F_s)}_{\\text{by the tower property}} \\\\\n&= M_s.\n\\end{align}\n\\]\n\\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#joint-distribution-of-max_t-leq-t-b_t-b_t",
    "href": "FCSC/ch4/ch4_exercises.html#joint-distribution-of-max_t-leq-t-b_t-b_t",
    "title": "Exercises",
    "section": "4.17 Joint Distribution of \\((max_{t \\leq T} B_t, B_T)\\)",
    "text": "4.17 Joint Distribution of \\((max_{t \\leq T} B_t, B_T)\\)\n\\[\n\\P(max_{t \\leq T} B_t > m, B_T \\leq a) = \\P(B_T > 2m -a)\n\\] and the joint pdf between the two is\n\\[\nf(m, a) = \\frac{2(2m -a)}{T^{3/2}\\sqrt{2 \\pi}} e^{-\\frac{(2m -a)^2}{2T}}.\n\\]\nProof:"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#zeros-of-brownian-motion",
    "href": "FCSC/ch4/ch4_exercises.html#zeros-of-brownian-motion",
    "title": "Exercises",
    "section": "4.18 Zeros of Brownian Motion",
    "text": "4.18 Zeros of Brownian Motion\nFor any \\(t > 0\\), \\(\\P(\\max_{s\\leq t} B_s > 0) = \\P(\\min_{s \\leq t} B_s < 0) = 1\\)\nProof:\nBachelier’s formula yields the following:\n\\[\n\\P(\\max_{s \\leq t} B_s \\leq 0) = \\P(|B_t| \\leq 0) = 0.\n\\]\n\n\n\n\n\n\nWhy is \\(P(|B_t| = 0)\\)?\n\n\n\n\n\nWe know that \\(B_t\\) is continous: in fact, it is absolutely continous because it has a probability density function. By definition, a random variable \\(X\\) is continous if\n\\[\n\\P(X = x) = 0\n\\] for all \\(x\\).\n\n\n\nTherefore,\n\\[\n\\P(\\max_{s \\leq t} B_s > 0) = 1.\n\\]\nThe result follows by applying the result to \\(-B_t\\), which is also a Brownian motion. \\[\\qed\\]\nThe function \\(s \\mapsto B_s(\\omega)\\) has infinitely many zeros in the interval \\([0, t]\\), with probability one.\nProof:\nIn the interval \\([0, t]\\), our function takes a maximum value \\(M> 0\\) and a minimum \\(m<0\\), at \\(t_M>0\\) and \\(t_m>0\\), respectively. Between these times, the function has a zero by continuity. There exists a \\(\\varepsilon>0\\) such that \\(T = \\min(t_M, t_m) - \\varepsilon > 0\\). Repeating this argument for \\([0, T]\\) we see that \\(s \\mapsto B_s(\\omega)\\) has two zeros in [0, t] and we can repeat as many times as we wish to get \\(n\\) zeros for any \\(n \\in \\mathbb{N}\\) i.e. there are infinitely many zeros of \\(s \\mapsto B_s(\\omega)\\) in \\([0, t]\\). \\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#doobs-maximal-inequalities",
    "href": "FCSC/ch4/ch4_exercises.html#doobs-maximal-inequalities",
    "title": "Exercises",
    "section": "4.19 Doob’s Maximal Inequalities",
    "text": "4.19 Doob’s Maximal Inequalities\nLet \\((M_k, k \\geq 1)\\) be a positive submartingale for the filtration \\(( \\F_k, k \\in \\mathbb{N})\\). Then for any \\(1 \\leq p < \\infty\\) and \\(a > 0\\)\n\\[\n\\P(\\max_{k\\leq n}M_k > a) \\leq \\frac{1}{a^p} \\E(M_n^p).\n\\]\nProof:\n\n(a)\nUse Jensen’s inequality to show that if \\((M_k, k \\geq 1)\\) is a submartingale, then so is \\((M^p_k, k\\geq 1)\\) for \\(1 \\leq p < \\infty\\). For \\(x \\geq 0\\),\n\\[\nx \\mapsto x^p\n\\] is a convex function for \\(1 \\leq p < \\infty\\). The process \\(M_n\\) is positive and so by Jensen’s inequality\n\\[\n\\E(M^p_n|\\F_m) \\geq \\E(M_n|\\F_m)^p = M_m^p.\n\\] That is, the process \\((M_k^p, k\\geq 1)\\) is submartingale.\nIf we prove the statement for \\(p=1\\), then\n\\[\n\\frac{1}{a^p}\\E(M_n^p) \\geq \\frac{1}{a}\\E(M_n)^p \\geq \\P(\\max_{k \\leq n}M_k > a)\n\\] for \\(1 \\leq p < \\infty\\): moreover, we need only prove that \\(M_n\\) is a submartingale for the theorem to apply.\n\n\n(b)\nConsider the events\n\\[\nB_k = \\cap_{j < k} \\{ \\omega : M_j(\\omega) \\leq a\\} \\cap\\{\\omega: M_k(\\omega) > a\\}.\n\\]\nThe \\(B_k\\)’s are disjoint: if \\(m < n\\) and \\(\\omega \\in B_m\\), then \\(M_m(\\omega) > a\\) and so \\(\\omega \\notin B_n\\).\n\\[\n\\cup_{k \\leq n} B_k = \\{\\max_{k \\leq n} M_k > a\\} \\triangleq B.\n\\]\nChoose \\(\\omega\\) in the LHS set. Then \\(\\omega\\) is in exactly one \\(B_k\\) and so \\(M_k(\\omega) > a\\) which means that\n\\[\n\\max_{k \\leq n} M_k(\\omega) > a,\n\\] that is\n\\[\n\\cup_{k \\leq n} B_k \\subseteq \\{\\max_{k \\leq n} M_k > a\\}.\n\\]\nTo prove the reverse inclusion, take \\(\\omega \\in \\{\\max_{k \\leq n} M_k > a\\}\\). Then, for some \\(k \\leq n\\), \\(M_k(\\omega) > a\\) and \\(M_j(\\omega) \\leq a\\) for \\(j < k\\) and so \\(\\omega \\in \\cup_{k \\leq n} B_k\\).\n\n\n(c)\n\\[\n\\E(M_n) \\geq \\E(M_n \\mathbb{1}_B) \\geq a \\sum_{k \\leq n} \\P(B_k) = a\\P(B).\n\\]\nProof:\n\\[\n\\begin{align}\n\\E(M_n) &\\underbrace{\\geq \\E(M_n \\mathbb{1}_B)}_{\\text{only true because } M_n >= 0} \\\\\n&= \\sum_k\\E(M_n \\mathbb{1}_{B_k}) \\\\\n&= \\sum_k\\E(\\E(M_n \\mathbb{1}_{B_k}| \\F_k)) \\\\\n&= \\sum_k\\E(\\underbrace{\\mathbb{1}_{B_k}}_{\\text{this is $\\F_k$-measurable}} \\E(M_n| \\F_k)) \\\\\n&\\geq \\sum_k\\E(\\mathbb{1}_{B_k} \\underbrace{M_k}_{\\text{submartingale}})) \\\\\n&\\geq a\\sum_k\\E( \\mathbb{1}_{B_k} ) \\\\\n&= a \\sum_k \\P(B_k) \\\\\n&= a \\P(B).\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(d)\nFor any finite set of times \\(0 = t_0 < t_1 < \\ldots < t_n = t\\), the inequality\n\\[\n\\P(\\max_{k \\leq n} M_{t_k} > a) \\leq \\frac{1}{a^p} \\E(M_n^p)\n\\]\nbecause\n\\[\nS_k = M_{t_k}\n\\]\ndefines a discrete submartingale.\nFor each \\(n\\), define a discretisation of \\([0, t]\\) by\n\\[\nt_k = \\frac{kt}{2^n}\n\\] for \\(k =1, \\ldots, 2^n\\). Define a collection of events\n\\[\nA_n = \\{\\omega : \\max_{k \\leq 2^n} M_{t_k} > a\\}.\n\\]\nThe sets are nested:\n\\[\nA_m \\subseteq A_n\n\\] if \\(m \\leq n\\). For each \\(n\\),\n\\[\nP(A_n) \\leq \\frac{1}{a^p}\\E(M_t^p).\n\\]\nUsing the continuity of probability\n\\[\n\\P(A) = \\lim_{n\\to \\infty} \\P(A_n)\n\\] where \\[\nA = \\cup_{n} A_n.\n\\]\nTherefore,\n\\[\n\\P(\\max_{s \\in \\mathcal{D}} M_s > a) \\leq \\frac{1}{a^p} \\E(M_t^p)\n\\] where \\(\\mathcal{D}\\) is the dense subset of \\([0, t]\\) of real numbers of the form \\[\nkt/2^n\n\\] for some \\(k\\) and \\(n\\).\nIt is clear that\n\\[\n\\{\\omega: \\max_{s \\in \\mathcal{D}} M_s(\\omega) > a\\} \\subseteq \\{\\omega: \\max_{s \\in [0, t]} M_s(\\omega) > a\\}.\n\\]\nThe reverse inclusion follows from the fact that\n\\[\ns \\mapsto M_s(\\omega)\n\\] is continous, almost surely: \\(M_s(\\omega)\\) will attain a maximum on \\([0, t]\\) and can be approximated to arbitrary precision with \\(M_{s'}(\\omega)\\) for \\(s' \\in \\mathcal{D}\\).\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#an-application-of-doobs-maximal-inequalities",
    "href": "FCSC/ch4/ch4_exercises.html#an-application-of-doobs-maximal-inequalities",
    "title": "Exercises",
    "section": "4.20 An Application of Doob’s Maximal Inequalities",
    "text": "4.20 An Application of Doob’s Maximal Inequalities\n\n(a)\nIf \\((B_t, t \\geq 0)\\) is a Brownian motion, then\n\\[\n\\lim_{n \\to \\infty} \\frac{B_n}{n} = 0\n\\] a.s. when \\(n\\) is an integer.\nProof:\nWe do the usual telescoping sum trick:\n\\[\n\\begin{align}\n\\frac{B_n}{n} &= \\frac{\\sum_{k=0}^{n-1} (B_{k+1} - B_k)}{n} \\\\\n&= \\frac{\\sum_{k=0}^{n-1} X_k}{n}\n\\end{align}\n\\]\nwhere \\(X_k = B_{k+1} - B_k\\) are Brownian increments. Using the strong law of large numbers\n\\[\n\\lim_{n \\to \\infty} \\frac{B_n}{n} = \\lim_{n\\to\\infty} \\bar{X_k} = \\E(X_0) = 0.\n\\]\n\\[\\qed\\]\n\n\n(b)\n\\[\n\\sum_{n \\geq 0} \\P(\\max_{0 \\leq s \\leq 1} |B_{n+s} - B_n| > \\delta n) < \\infty\n\\]\nfor any \\(\\delta > 0\\).\nProof:\nThe process \\(M_t = |B_{n+t} - B_n|\\) is a positive submartingale of the filteration \\((\\G_s = \\F_{n+s}, s \\geq 0)\\):\n\\[\n\\begin{align}\n\\E(M_t | \\G_s) &= \\E(|B_{n+t} - B_n| | \\F_{n+s}) \\\\\n& \\geq \\left| \\E(B_{n + t} - B_n| \\F_{n +s })\\right| \\\\\n&=\\left| \\E(B_{n + t}|\\F_{n+s}) - \\E(B_n| \\F_{n +s })\\right | \\\\\n&=\\left | \\underbrace{B_{n+s}}_{\\text{martingale property}} - \\E(B_n| \\F_{n +s })\\right| \\\\\n&=\\left | B_{n+s} - \\underbrace{B_n}_{B_n \\text{is } \\F_{n+s} \\text{ measurable}} \\right| \\\\\n&= M_s\n\\end{align}\n\\] for \\(s \\leq t\\).\nDoob’s maximal inequality applies to \\(M_t\\):\n\\[\n\\begin{align}\n\\P(\\max_{0 \\leq s \\leq 1}|B_{n + s} - B_n| > \\delta n) &\\leq \\frac{1}{\\delta^2 n^2}\\E((B_{n+1} - B_n)^2) \\\\\n&= \\frac{1}{\\delta^2 n^2}\\\\\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(c)\nTaking the sum \\[\n\\sum_n \\P(\\max_{0 \\leq s \\leq 1}|B_{n + s} - B_n| > \\delta n) = \\frac{\\pi^2}{6 \\delta^2} < \\infty.\n\\]\nAs a consequence\n\\[\n\\lim_{n\\to\\infty} \\max_{0 \\leq s \\leq 1} \\frac{|B_{n+s} - B_n|}{n} = 0\n\\] almost surely.\n\\[\n\\qed\n\\]\n\n\n(d)\nIf \\(t_n \\uparrow \\infty\\), then\n\\[\n\\lim_{n\\to \\infty} \\frac{B_{t_n}}{t_n} = 0.\n\\]\nProof:\nLet \\(k = \\lfloor t_n \\rfloor\\) and \\(s = t_n - k\\). Then\n\\[\n\\begin{align}\n\\lim_{n\\to \\infty} \\frac{B_{t_n}}{t_n} &= \\lim_{n\\to\\infty} \\frac{B_{k+s} - B_k + B_k}{t_n} \\\\\n&\\leq \\lim_{n\\to\\infty} \\frac{B_{k+s} - B_k + B_k}{k} \\\\\n&\\leq \\lim_{n\\to\\infty} \\max_{0 \\leq s \\leq 1}\\frac{|B_{k+s} - B_k| + |B_k|}{k} \\\\\n& = 0.\n\\end{align}\n\\]\n\\[\n\\qed\n\\]\n\n\n(e)\nIf \\[\nX_t = t B_{1/t},\n\\] then \\[\n\\lim_{t \\to 0+} X_t = 0\n\\] almost surely.\nProof:\n\\[\nX_{\\frac{1}{t}} = \\frac{B_t}{t} \\to 0\n\\] as \\(t \\to \\infty\\), almost surely.\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_exercises.html#an-example-of-fubinis-theorem",
    "href": "FCSC/ch4/ch4_exercises.html#an-example-of-fubinis-theorem",
    "title": "Exercises",
    "section": "4.21 An Example of Fubini’s Theorem",
    "text": "4.21 An Example of Fubini’s Theorem\nLet \\((X_n, n \\geq 1)\\) be a sequence of random variables on \\((\\Omega, \\F, \\P)\\). If\n\\[\n\\sum_n \\E(|X_n|) < \\infty\n\\] then\n\\[\n\\E \\left(\\sum_{n} X_n \\right ) = \\sum_n \\E(X_n).\n\\]\nProof:\nLet\n\\[\nS_n = \\sum_{k=1}^n X_k.\n\\]\nAlmost surely\n\\[\nS_n \\to \\sum_{k=1}^{\\infty} X_k\n\\] as \\(n \\to \\infty\\) and\n\\[\n| S_n | =  |\\sum_{k=1}^{n} X_k| \\leq \\sum_{k=1}^{n} |X_k| \\leq \\sum_{k=1}^{\\infty} |X_k|.\n\\]\nSo \\(S_n\\) is dominated by\n\\[\n\\sum_k |X_k|\n\\] and this is integrable by our assumptions:\n\\[\n\\E(\\sum_{k=1}^{\\infty} |X_k|)  \\leq \\sum_{k=1}^{\\infty} \\E(|X_k|) < \\infty\n\\]\nSo we can invoke the dominated convergence theorem to state that\n\\[\n\\lim_{n\\to \\infty} \\E(S_n) = \\E(\\lim_{n \\to \\infty} S_n).\n\\] That is\n\\[\n\\sum_n \\E(X_n) = \\E\\left(\\sum_n X_n\\right).\n\\]\n\\[\n\\qed\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html",
    "href": "FCSC/ch4/ch4_experiments.html",
    "title": "Computer Experiments",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html#standard-brownian-motion-is-a-martingale",
    "href": "FCSC/ch4/ch4_experiments.html#standard-brownian-motion-is-a-martingale",
    "title": "Computer Experiments",
    "section": "Standard Brownian Motion is a Martingale",
    "text": "Standard Brownian Motion is a Martingale\n\nExample 4.28 (i)\nIn the book, it is stated that\n\\[\n\\E(B_t | \\F_s) = \\E(B_s + B_t - B_s | \\F_s) = B_s + E(B_t - B_s) = B_s.\n\\]\nLet’s break this down:\n\\[\n\\E(B_s | \\F_s) = B_s\n\\]\nbecause \\(B_s\\) is \\(\\F_s\\) measurable.\n\\[\n\\E(B_t - B_s | \\F_s) = \\E(B_t - B_s) = 0.\n\\] because \\(B_t - B_s\\) is independent of \\(B_s\\) (by independence of increments and \\(B_s = B_s - B_0\\)).\n\n\nExample 4.29 (i)\nSymmetric random walk:\n\\((X_i, i \\in \\mathbb{N})\\) are random IID variables with \\(E(X_1) = 0\\) and \\(E(|X_1|) < \\infty\\). Take \\(\\F_n = \\sigma(X_i, i \\leq n)\\) and \\[\nS_n = X_1 + \\cdots + X_n, S_0 = 0.\n\\]\n\\[\n\\begin{align}\nE(S_{n+k} | \\F_n) &= E(S_n + \\sum_{i=n+1}^k S_i | \\F_n) \\\\\n&=S_n + \\sum_{i = n+1}^k \\E(S_i| \\F_n) \\\\\n&=S_n\n\\end{align}\n\\] where \\(\\E(S_i | \\F_n) = 0\\) for \\(i > n\\) by the IID assumption.\n\n\nExample 4.36 Last Passage Time is NOT a Stopping Time\nThe last passage time is for \\(a\\)\n\\[\n\\rho(\\omega) = \\max \\{ s \\geq 0 : X_s(\\omega) \\geq a \\}.\n\\]\nSuppose that \\(X_t\\) is a martingale with filtration \\((\\F_t, t \\geq 0)\\). Suppose that \\(\\{ \\omega : \\rho(\\omega) \\leq t\\} \\in \\F_t\\) for all \\(t\\geq 0\\). Consider \\(\\{ \\omega : \\rho(\\omega) \\leq 0 \\} \\in \\F_0\\). We know that \\(\\F_0 = \\{ \\Omega, \\emptyset \\}\\).\n\\[\n\\begin{align}\n\\{ \\omega : \\rho(\\omega) \\leq 0 \\} &= \\{ \\omega: \\max\\{ s \\geq : X_s(\\omega) \\geq a \\} \\leq 0\\} \\\\\n&= \\{ \\omega: X_s(\\omega) < a,\\, \\forall s > 0 \\}.\n\\end{align}\n\\]\nWe have \\[\n\\begin{align}\n\\{ \\omega: X_s(\\omega) < a, \\, \\forall s > 0 \\} &= \\Omega\n\\end{align}\n\\]\nor \\[\n\\begin{align}\n\\{ \\omega: X_s(\\omega) < a, \\, \\forall s > 0 \\} &= \\emptyset.\n\\end{align}\n\\] If the former is true, then \\(X_s(\\omega) < a\\) for all \\(s > 0\\) with probability one. If the latter is true, then \\(X_s(\\omega) \\geq a\\) for all \\(s > 0\\) with probability one. Therefore, if \\(\\rho\\) is a stopping time for \\(X_t\\), then \\(X_t\\) is bounded above or below by \\(a\\). By continuity of paths, \\(X_0 = a\\) and so \\(\\E(X_0) = a\\) which in turn implies that \\(\\E(X_t) = a\\). Suppose that \\(X_t \\geq a\\):\n\\[\n\\begin{align}\n0 &= \\E(X_t -a) \\\\\n&= \\int_{0}^{\\infty} \\P(X_t -a > x) dx \\\\\n\\end{align}\n\\] and so we must have \\(X_t = a\\), almost surely.\nOn the other hand, if \\(X_t < a\\), then \\[\n\\begin{align}\n0 &= \\E(a - X_t) \\\\\n&= \\int_{0}^{\\infty} \\P(a - X_t > x) dx \\\\\n\\end{align}\n\\] and so, again, \\(X_t = a\\), almost surely.\nSo, \\(\\rho\\) is only a stopping time for \\(X_t\\) if \\(X_t = a\\).\n\n\nDominated Convergence Theorem\n\n\n\n\ngraph TD\n  A[Convergence Almost Surely] -->|Dominated Convergence| B[Convergence in Probability]\n  C[L^2 Convergence] -->|Chebyshev Inequality| B\n  B --> D[Convergent Sum of Probabilities]\n  D -->A"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html#simulating-martingales",
    "href": "FCSC/ch4/ch4_experiments.html#simulating-martingales",
    "title": "Computer Experiments",
    "section": "4.1 Simulating Martingales",
    "text": "4.1 Simulating Martingales\n\n\\(B_t^2 -t\\), \\(t \\in [0, 1]\\)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN=10\nsteps = 100\n\ndef brownian(steps, start=0, stop=1):\n    variance = (stop - start)/steps\n    return np.r_[0, np.cumsum(np.random.default_rng().normal(0, np.sqrt(variance), steps-1))]\n\nfor _ in range(10):\n  plt.plot(brownian(1000, 0, 10))\nplt.show()\n\n\n\n\n\nt = np.linspace(0, 1, steps)\nfor _ in range(N):\n    plt.plot(t, brownian(steps)**2 - t)\n\nplt.title(f'{N} $B_t^2 -t$ samples')\nplt.show()\n\n\n\n\n\n\\(S_t = e^{B_t - t/2}\\), \\(t \\in [0, 1]\\)\n\n\nfor _ in range(N):\n  plt.plot(t, np.exp(brownian(steps) - t/2))\n\nplt.title(f'{N} $e^{{B_t - t/2}}$ samples')\nplt.show()\n\n\n\n\n\n\\(N_t - t\\), \\(t \\in [0, 1]\\) where \\(N_t\\) is a Poisson process of rate 1\n\n\ndef poisson(steps, start=0, stop=1):\n    rate = (stop -start)/steps\n    increments = np.random.default_rng().poisson(rate, steps)\n    return np.cumsum(increments)\n\nfor _ in range(N):\n    plt.plot(t, poisson(steps) - t)\n\nplt.title(f'{N} $N_t - 1$ samples')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html#maximum-of-brownian-motion",
    "href": "FCSC/ch4/ch4_experiments.html#maximum-of-brownian-motion",
    "title": "Computer Experiments",
    "section": "4.2 Maximum of Brownian Motion",
    "text": "4.2 Maximum of Brownian Motion\n\ndef max_brownian(steps, start=0, stop=1):\n    return np.max(brownian(steps, start, stop))\n\nsamples = 10000\nmaxb_samples = [max_brownian(steps) for _ in range(samples)]\n\nprint(maxb_samples.count(0)/samples)\n\nplt.hist(maxb_samples, density=True, bins=np.logspace(0, 1, 1000)-5)\ns = np.linspace(0, 4, 1000)\nplt.plot(s, (2/np.sqrt(2*np.pi))*np.exp(-(s**2)/2))\nplt.show()\n\n0.0568"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html#first-passage-time",
    "href": "FCSC/ch4/ch4_experiments.html#first-passage-time",
    "title": "Computer Experiments",
    "section": "4.3 First Passage Time",
    "text": "4.3 First Passage Time\n\\[\n\\tau = \\min \\{t \\geq 0 : B_t \\geq 1 \\}.\n\\]\n\ndef tau(steps):\n    start, stop = 0, 10\n    interval = (stop - start)/steps\n    for i, v in enumerate(brownian(steps, start, stop)):\n        if v >= 1:\n            return i * interval\n    return stop\n\ntau_samples = [tau(1000) for _ in range(10000)]\n\nplt.hist(tau_samples, density=True, bins=100)\naverage_tau = np.average(tau_samples)\nplt.title(f'$E(\\\\tau\\\\wedge 10) \\\\approx {average_tau}$')\nplt.show()\n\n\n\n\nThe expectation \\(\\E(\\tau \\wedge 10)\\) can be calculated from the distribution of \\(\\tau\\). Since \\(\\tau \\wedge 10 \\geq 0\\),\n\\[\n\\begin{align}\n\\E(\\tau \\wedge 10) &= \\int_{0}^{\\infty} \\P(\\tau \\wedge 10 > x)\\, dx \\\\\n&= \\int_{0}^{10} \\P(\\tau > x)\\, dx \\\\\n&= \\int_0^{10} (1 - \\P(\\tau \\leq x))\\, dx \\\\\n&= 10 - \\int_0^{10} \\P(\\tau \\leq x)\\, dx\\\\\n&= 10 - \\int_0^{10} \\int_0^{x} \\frac{1}{\\sqrt{2 \\pi y^3}} e^{-1/2y} \\,dy \\,dx \\\\\n\\end{align}\n\\]\n\nimport sympy as sp\nfrom sympy.abc import x,y\nimport IPython.display as disp\nsp.init_printing(use_latex='mathjax')\n\nintegrand = (1/ sp.sqrt(2 * sp.pi * y**3))*sp.exp(-1/ (2*y))\n\nexpectation = 10 - sp.integrate(integrand, (y, 0, x),(x, 0, 10)); expectation\ndisp.display(expectation)\ndisp.display(expectation.evalf())\n\n\\(\\displaystyle -1 + \\frac{2 \\sqrt{5}}{\\sqrt{\\pi} e^{\\frac{1}{20}}} + 11 \\operatorname{erf}{\\left(\\frac{\\sqrt{5}}{10} \\right)}\\)\n\n\n\\(\\displaystyle 4.12995192235593\\)\n\n\nWhat proportion of paths never reach 1? This can be expressed as\n\\[\n\\begin{align}\n\\P(\\max_{t \\in [0, 10]}B_t < 1) &= 1 - \\P(\\max_{t \\in [0,10]} B_t \\geq 1)\\\\\n&= 1 - \\P(\\tau \\leq 10) \\\\\n&= 1 - \\int_0^{10} \\frac{1}{2 \\pi y^3} e^{-1/2y}\\, dy\n\\end{align}\n\\] which can be caluclated with SymPy:\n\nproportion = 1 - sp.integrate(integrand, (y, 0, 10))\ndisp.display(proportion)\ndisp.display(proportion.evalf())\n\n\\(\\displaystyle \\operatorname{erf}{\\left(\\frac{\\sqrt{5}}{10} \\right)}\\)\n\n\n\\(\\displaystyle 0.248170365954151\\)\n\n\nWe can also retrieve an approximation from our simulation:\n\nsum(t == 10 for t in tau_samples)/len(tau_samples)\n\n\\(\\displaystyle 0.2682\\)"
  },
  {
    "objectID": "FCSC/ch4/ch4_experiments.html#gamblers-ruin-at-the-french-roulette",
    "href": "FCSC/ch4/ch4_experiments.html#gamblers-ruin-at-the-french-roulette",
    "title": "Computer Experiments",
    "section": "Gambler’s Ruin at the French Roulette",
    "text": "Gambler’s Ruin at the French Roulette\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef roulette_win(start=100, stop=200, p = 18/38):\n  rolls = 2*np.random.default_rng().binomial(1, p, size=5000) - 1\n  purse = np.cumsum(rolls) + start\n  for p in purse:\n    if p == 0:\n      return False\n    if p == stop:\n      return True\n  return purse[-1] >= start\n\nprint(f'Probability of winning $200 starting at $100 is ~ {sum([roulette_win() for _ in range(100)])/100}')\n\nprobability_estimates = [ sum([roulette_win(start=starting_purse) for _ in range(100)])/100 for starting_purse in range(1, 200)]\n\nplt.plot(range(1, 200), probability_estimates)\nplt.title('Probability of winning $200 starting with different purses')\nplt.show()\n\nProbability of winning $200 starting at $100 is ~ 0.0"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html",
    "href": "FCSC/ch6/ch6_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\sgn}{\\operatorname{sgn}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\G}{\\mathcal{G}}\n\\newcommand{\\qed}{\\tag*{$\\square$}}\n\\newcommand{\\erf}{\\operatorname{erf}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#rotational-symmetry-of-2d-brownian-motion",
    "href": "FCSC/ch6/ch6_exercises.html#rotational-symmetry-of-2d-brownian-motion",
    "title": "Exercises",
    "section": "6.1 Rotational Symmetry of 2D Brownian Motion",
    "text": "6.1 Rotational Symmetry of 2D Brownian Motion\nLet \\(B_t = (B_t^{(1)}, B_t^{(2)})\\) be a two-dimensional Brownian motion. Let\n\\[\nM = \\begin{bmatrix}\n\\cos{\\theta} & \\sin{\\theta} \\\\\n-\\sin{\\theta} & \\cos{\\theta}.\n\\end{bmatrix}\n\\]\nThen \\(W_t = M B_t\\) is also a two-dimensional Brownian motion.\nProof: By matrix multiplication \\[\nM B_t = (\\cos{\\theta} B_t^{(1)} + \\sin{\\theta} B_t^{(2)}, -\\sin{\\theta} B_t^{(1)} + \\cos{\\theta} B_t^{(2)}).\n\\]\nClearly, \\(M B_t\\) is a linear combination of Gaussian vectors because \\((B_t, t \\geq 0)\\) is Gaussian. Therefore, if the covariance of the components is zero, the components are independent:\n\\[\n\\begin{align}\n\\E((\\cos{\\theta} B_s^{(1)} + \\sin{\\theta} B_s^{(2)})(-\\sin{\\theta} B_t^{(1)} + \\cos{\\theta} B_t^{(2)})) \\\\\n= \\E(\\cos{\\theta}\\sin{\\theta} B_s^{(2)} B_t^{(2)} - \\cos{\\theta}\\sin{\\theta} B_s^{(1)}B_t^{(1)} ) \\\\\n= \\cos{\\theta}\\sin{\\theta}(t\\wedge s -t \\wedge s) \\\\\n= 0.\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\E(B_s^{(1)}B_t^{(2)}) = \\E(B_s^{(1)}) \\E(B_t^{(2)}) = 0.\n\\]\n\n\nUsing the independence of the two Brownian processes and the variance properties of Brownian motions we see that \\[\n\\begin{align}\n\\E(W_t^{(1)} W_s^{(1)}) &= \\E(\\cos^2\\theta B_t^{(1)} B_s^{(1)} + \\cos{\\theta}\\sin{\\theta} (B_t^{(1)}B_s^{(2)} + B_t^{(2)} B_s^{(1)}) + \\sin^2\\theta B_t^{(2)} B_s^{(2)}) \\\\\n&= \\cos^2\\theta t \\wedge s + \\sin^2\\theta t \\wedge s \\\\\n&= t \\wedge s.\n\\end{align}\n\\]\nSimilarly, \\[\n\\E(W_t^{(2)} W_s^{(2)}) = t \\wedge s.\n\\]\nThis characterises Brownian motions and the proof is complete. \\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#orthogonal-symmetry-of-brownian-motion",
    "href": "FCSC/ch6/ch6_exercises.html#orthogonal-symmetry-of-brownian-motion",
    "title": "Exercises",
    "section": "6.2 Orthogonal Symmetry of Brownian Motion",
    "text": "6.2 Orthogonal Symmetry of Brownian Motion\nLet \\(B_t = (B_t^{(i)}, i \\leq d)\\) be a \\(d\\)-dimensional Brownian motion. Let \\(M\\) be a \\(d \\times d\\) orthogonal matrix (\\(M^{-1} = M^T\\)). The the process \\(W_t = M B_t\\) is a \\(d\\)-dimensional Brownian motion.\nProof:\n\\(W_t\\) is a Gaussian process because it is a linear transformation of \\(B_t\\) so it is a Gaussian process. It remains to be shown that the components of \\(W_t\\) are independent Brownian motions. Calculating the covariances, we see that the components are independent and the each individual component is Brownian:\n\\[\n\\begin{align}\n\\E(W_t^{(i)} W_s^{(j)}) & = \\E(\\sum_{k} M_{ik} B_t^{(k)} \\sum_{l} M_{jl} B_s^{(l)}) \\\\\n&= \\E(\\sum_{k} M_{ik} B_t^{(k)} M_{jk} B_s^{(k)}) \\\\\n&= \\E(\\sum_k M_{ik} M^T_{kj} B_t^{(k)} B_s^{(k)}) \\\\\n&= \\sum_k (M_{ik}M^T_{kj} t \\wedge s ) \\\\\n&= t \\wedge s \\sum_k M_{ik} M^{-1}_{kj} \\\\\n&= (t \\wedge s) \\delta_{ij}.\n\\end{align}\n\\]\n\\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#races-between-brownian-motions",
    "href": "FCSC/ch6/ch6_exercises.html#races-between-brownian-motions",
    "title": "Exercises",
    "section": "6.3 Races Between Brownian Motions",
    "text": "6.3 Races Between Brownian Motions\nLet \\(B_t = (B_{1, t}, B_{2,t})\\) be a two-dimensional Brownian motion.\nThe probability that \\(B_{1, t}\\) reaches \\(1\\) before \\(B_{2, t}\\) is \\(1/2\\). What is the probability that \\(B_{1, t}\\) reaches \\(2\\) before \\(B_{2, t}\\) reaches \\(1\\)?\nLet \\(\\tau_{i, a} = \\min\\{t \\geq 0: B_{i,t} \\geq a \\}\\). Using Bachelier’s formula,\n\\[\n\\P(\\tau_{1, 2} \\leq T) =  \\P(|B_{1, T}| \\geq 2)\n\\] and \\[\n\\P(\\tau_{2, 1} > T) = \\P(|B_{2, T}| < 1).\n\\]\n\\[\n\\begin{align}\n\\int_0^{\\infty} \\P(\\tau_{2,1} > t| \\tau_{1, 2} = t) dt &= \\int_0^{\\infty} \\left( \\int_{0}^1 \\frac{2}{\\sqrt{2 \\pi t}} e^{-y^2/2t} dy \\right) \\frac{2}{\\sqrt{2 \\pi}} \\frac{e^{-2/t}}{t^{3/2}} dt \\\\\n&= \\int_0^{\\infty} \\int_0^1 \\frac{2}{\\pi} \\frac{e^{-y^2/2t - 2/t}}{t^2} dy dt \\\\\n&= \\int_0^{\\infty} \\frac{2}{\\pi} \\frac{e^{-2/t}}{t^2} \\frac{\\sqrt{2 \\pi t}}{2} \\erf(\\frac{1}{\\sqrt{2t}}) dt \\\\\n&= \\sqrt{\\frac{2}{\\pi}} \\int_0^{\\infty} \\frac{e^{-2/t}}{t^{3/2}} \\erf(\\frac{1}{\\sqrt{2t}}) dt \\\\\n&= \\frac{2}{\\pi} \\tan^{-1}(1/2) \\\\\n&\\approx 0.3.\n\\end{align}\n\\]\nThis can be checked by simulation:\n\nimport numpy as np\n\ndef race():\n  delta = 0.1\n  b_1, b_2 = 0, 0\n  while True:\n    if b_1 >= 1:\n      return 1\n    if b_2 >= 2:\n      return 2\n    b_1 += np.random.default_rng().normal(0, np.sqrt(delta))\n    b_2 += np.random.default_rng().normal(0, np.sqrt(delta))\n\ntotal_wins = 0\ntrials = 1000\nfor _ in range(trials):\n  winner = race()\n  if winner == 2:\n    total_wins += 1\n\ntotal_wins/trials\n\n0.312"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#drill",
    "href": "FCSC/ch6/ch6_exercises.html#drill",
    "title": "Exercises",
    "section": "6.4 Drill",
    "text": "6.4 Drill\n\\[\nX_t = (B^{(1)}_t)^2 + (B^{(2)}_t)^2\n\\] \\[\nY_t = \\exp(B^{(1)}_t)\\cos{B^{(2)}_t}.\n\\]\nApply Itô’s formula to\n\\[\nf(x, y) = x^2 + y^2\n\\] to get \\[\n\\begin{align}\nf(B^{(1)}_s, B^{(2)}_s) &= (B^{(1)}_t)^2 + (B^{(2)}_t)^2 \\\\\n&= \\int_0^t \\partial_1 f(s, B^{(1)}_s, B^{(2)}_s) dB_s^{(1)} \\\\\n&+ \\int_0^t \\partial_2 f(s, B^{(1)}_s, B^{(2)}_s) dB_s^{(2)} \\\\\n&+ \\frac{1}{2}\\int_0^t (\\partial_0 f(s, B^{(1)}_s, B^{(2)}_s) + \\triangle f(s, B^{(1)}_s, B^{(2)}_s) ) ds \\\\\n&= \\int_0^t 2 B_s^{(1)} dB_s^{(1)} + \\int_0^t 2 B_s^{(2)} dB_s^{(2)} + 2t.\n\\end{align}\n\\]\nApply Itô’s formula to \\[\ng(x, y) = \\exp(x) \\cos{y}\n\\] to get\n\\[\n\\begin{align}\ng(B_t^{(1)}, B_t^{(2)}) -g(B_0^{(1)}, B_0^{(2)}) &= \\exp(B_t^{(1)})\\cos{B_t^{(2)}} - 1 \\\\\n&= \\sum_{i=1}^2 \\int_0^t \\partial_i g( B_s^{(1)}, B_s^{(2)})d B_s^{(i)} + \\int_0^t (\\partial_0 g(B_s^{(1)}, B_s^{(2)})) + \\frac{1}{2} \\triangle g(B_s^{(1)}, B_s^{(2)})) ds \\\\\n&= \\int_0^t \\exp(B_s^{(1)}) \\cos{B_s^{(2)}} d B_s^{(1)} - \\int_0^t \\exp(B_s^{(1)})\\sin{B_s^{(2)}} d B_s^{(2)}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#cross-variation-of-b_t1-and-b_t2",
    "href": "FCSC/ch6/ch6_exercises.html#cross-variation-of-b_t1-and-b_t2",
    "title": "Exercises",
    "section": "6.5 Cross-Variation of \\(B_t^{(1)}\\) and \\(B_t^{(2)}\\)",
    "text": "6.5 Cross-Variation of \\(B_t^{(1)}\\) and \\(B_t^{(2)}\\)\nLet \\((t_j, j \\leq n)\\) be a sequence of partitions of \\([0, t]\\) such that\n\\[\n\\max_{j} |t_{j+1} -t_j| \\to 0\n\\] as \\(n \\to \\infty\\).\nThen\n\\[\n\\lim_{n\\to \\infty} \\sum_{j=0}^n (B_{t_{j+1}}^{(1)} - B_{t_j}^{(1)})(B_{t_{j+1}}^{(2)} - B_{t_j}^{(2)}) = 0\n\\] in \\(L^2\\).\nProof:\nFor notational simplicity, define\n\\[\nX_{1,j} = B_{t_{j+1}}^{(1)} - B_{t_j}^{(1)}\n\\] and\n\\[\nX_{2,j} = B_{t_{j+1}}^{(2)} - B_{t_j}^{(2)}.\n\\] Then the second moment of the sum can be expressed \\[\n\\begin{align}\n\\E(\\sum_{j=0}^n\\sum_{k=0}^{n} X_{1,j}X_{2,j} X_{1, k} X_{2,k}).\n\\end{align}\n\\] For \\(j = k\\),\n\\[\n\\begin{align}\n\\E(X_{1, j} X_{2, j} X_{1, k} X_{2, k}) &= \\E(X_{1,j}^2 X_{2, j}^2) \\\\\n&= \\underbrace{\\E(X_{1,j}^2) \\E(X_{2,j}^2)}_{\\text{independence}} \\\\\n&= (t_{j+1} - t_j)^2.\n\\end{align}\n\\] For \\(j \\neq k\\),\n\\[\n\\begin{align}\n\\E(X_{1, j} X_{2, j} X_{1, k} X_{2, k}) &= \\E(X_{1, j})\\E(X_{2, j}) \\E(X_{1, k}) \\E(X_{2, k})\n&= 0\n\\end{align}\n\\] due to independence of the increments of each Brownian motion and the independence of the Brownian motions.\nTherefore, the limit of the second moment of the sum is \\[\n\\begin{align}\n\\sum_{j=0}^n (t_{j+1} - t_j)^2 &\\leq \\max_{j} |t_{j+1} -t_j| \\sum_{j=0}^n |t_{j+1} -t_j| \\\\\n&= t \\max_{j} |t_{j+1} -t_j| \\\\\n& \\to 0.\n\\end{align}\n\\]\n\\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#a-function-of-b_t",
    "href": "FCSC/ch6/ch6_exercises.html#a-function-of-b_t",
    "title": "Exercises",
    "section": "6.6 A Function of \\(B_t\\)",
    "text": "6.6 A Function of \\(B_t\\)\nConsider a two-dimensional Brownian motion \\(B_t = (B_t^{(1)}, B_t^{(2)})\\) and a process\n\\[\nZ_t = B_t^{(1)} B_t ^{(2)}.\n\\]\n\\(Z_t\\) can be written as a sum of Itô integrals and a Riemann integral using Itô’s formula.\nDefine \\[\nf(x, y) = xy\n\\] so that \\[\n\\partial_0 f(x, y) = 0,\n\\] \\[\n\\partial_1 f(x, y) = y,\n\\] \\[\n\\partial_2 f(x, y) = x,\n\\] and \\[\n\\triangle f(x, y) = 0.\n\\].\nIt follows that\n\\[\nB_t^{(1)}B_t^{(2)} = \\int_0^t B_t^{(2)} dB_s^{(1)} + \\int_0^t B_t^{(1)} dB_s^{(2)}.\n\\] Note that the Riemann integral part of the expression is zero because \\(f\\) is harmonic.\nSince, trivially, \\[\n\\partial_0 f(x,y) = -frac{1}{2} \\triangle f(x, y),\n\\]\n\\(Z_t\\) is a martingale."
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#another-function-of-b_t",
    "href": "FCSC/ch6/ch6_exercises.html#another-function-of-b_t",
    "title": "Exercises",
    "section": "6.7 Another Function of \\(B_t\\)",
    "text": "6.7 Another Function of \\(B_t\\)\nLet\n\\[\nf(x, y) = x^3 - 3 xy\n\\] and consider the process \\[\nZ_t = f(B_t^{(1)}, B_t^{(2)}).\n\\]\nWe have\n\\[\n\\partial_0 f(x, y) = 0,\n\\]\n\\[\n\\partial_1 f(x, y) = 3x^2 - 3 y,\n\\]\n\\[\n\\partial_2 f(x, y) = 3 x,\n\\] and \\[\n\\triangle f(x, y) = 6 x.\n\\]\nIt follows from Itô’s formula that\n\\[\n\\begin{align}\nZ_t &= (B_t^{(1)})^3 - 2B_t^{(1)}B_t^{(2)} \\\\\n&= \\int_0^t 3(B_s^{(1)})^2 - 3 B_s d B_s^{(1)} + \\int_0^t 3 B_s^{(1)} dB_s^{(2)} + \\int_0^t 3 B_s^{(1)} ds.\n\\end{align}\n\\]\n\\(Z_t\\) is not a martingale;\n\\[\nZ_t - \\int_0^t 3 B_s^{(1)} ds\n\\] is a martingale."
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#harmonic-functions",
    "href": "FCSC/ch6/ch6_exercises.html#harmonic-functions",
    "title": "Exercises",
    "section": "6.8 Harmonic Functions",
    "text": "6.8 Harmonic Functions\n\\[\nh(x) = \\begin{cases}\n\\log{\\|x\\|} & d=2\\\\\n\\|x\\|^{2 -d} & d \\geq 3\n\\end{cases} x \\in \\mathbb{R}^d.\n\\]\n\\(h\\) is harmonic.\nProof:\nFor \\(d=2\\),\n\\[\n\\begin{align}\n\\triangle \\log(\\|x\\|) & = \\triangle \\log(\\sqrt{x_1^2 + x_2^2}) \\\\\n&= \\partial_1 (\\frac{x_1}{(x_1^2 + x_2^2)}) + \\partial_2 (\\frac{x_2}{x_1^2 + x_2^2}) \\\\\n&= \\frac{1}{x_1^2 + x_2^2} - \\frac{2 x_1^2}{(x_1^2 + x_2)^2} +  \\frac{1}{x_1^2 + x_2^2} - \\frac{2 x_2^2}{(x_1^2 + x_2)^2} \\\\\n&= 0.\n\\end{align}\n\\]\nFor \\(d \\geq 3\\), \\[\n\\begin{align}\n\\triangle \\|x\\|^{2 -d} &= \\nabla.(\\nabla \\|x\\|^{2-d}) \\\\\n&= \\nabla.((2-d)\\|x\\|^{-d}x) \\\\\n&= \\sum_{i=1}^d  ((2 -d)(-d/2) \\|x\\|^{-(d+2)} 2x^2_i + (2 -d) \\|x\\|^{-d}) \\\\\n&= (2-d) \\|x\\|^{-d} \\sum_{i=1}^d (1 - \\frac{d x_i^2}{\\|x\\|^2} ) \\\\\n&= (2 -d) \\|x\\|^{-d} (d - \\frac{d}{\\|x\\|^2} \\sum_{i=1}^d x_i^2 ) \\\\\n&= (2 -d) \\|x\\|^{-d} (d - \\frac{d}{\\|x\\|^2} \\|x \\|^2 ) \\\\\n&= 0.\n\\end{align}\n\\]\n\\[\\qed\\]\nFor \\(a, b \\in \\mathbb{R}\\), \\(a h(x) + b\\) is harmonic.\nProof:\n\\[\n\\triangle (a h(x) + b) = a \\triangle h(x) = 0.\n\\]\n\\[\\qed\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#waiting-time-for-d-2",
    "href": "FCSC/ch6/ch6_exercises.html#waiting-time-for-d-2",
    "title": "Exercises",
    "section": "6.9 Waiting Time for \\(d > 2\\)",
    "text": "6.9 Waiting Time for \\(d > 2\\)\nLet \\(B_t\\) be a \\(d\\)-dimensional Brownian motion. Define a process\n\\[\nM_t = \\sum_{i=1}^d (B_t^{(i)})^2 - dt\n\\] for \\(t \\geq 0\\).\nLet \\(f(t, x) = \\|x\\|^2 -dt\\) so that\n\\[\nM_t = f(t, B_t).\n\\]\nWe have\n\\[\n\\partial_0 f(t, x) = -d,\n\\] \\[\n\\partial_i f(t, x) = 2 x_i,\n\\] and \\[\n\\triangle f(t, x) = 2d.\n\\] Applying Itô’s formula, we see that \\[\n\\begin{align}\nM_t &= \\sum_{i=1}^d \\int_0^t 2 B_s^{(i)} d B_s^{(i)}\n\\end{align}\n\\] so \\(M_t\\) is a martingale.\nLet \\[\n\\tau = \\min \\{t \\geq 0: \\|B_t\\| \\geq 1 \\}.\n\\]\nThe stopped process \\(M_{\\tau \\wedge t}\\) is a martingale and\n\\[\n\\E_x(M_{\\tau \\wedge t}) = M_0 = \\|x\\|^2.\n\\] The LHS is \\[\n\\begin{align}\n\\E_x(M_{\\tau \\wedge t}) &= \\E_x(\\sum_{i=1}^d (B_{\\tau \\wedge t}^{(i)})^2) - d E_x( \\tau \\wedge t).\n\\end{align}\n\\]\nTaking the limit as \\(t \\to \\infty\\),\n\\[\n\\begin{align}\n\\|x\\|^2 &= \\underbrace{\\E_x(\\sum_{i=1}^d (B_\\tau^{(i)})^2)}_{\\text{bounded convergence thm}} - \\underbrace{d \\E_x(\\tau)}_{\\text{monotone convergence thm}} \\\\\n&= 1 - d \\E_x(\\tau).\n\\end{align}\n\\] Therefore,\n\\[\n\\E_x(\\tau) = \\frac{1}{d} (1 - \\|x\\|^2).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIt makes intuitive sense that the expected time to exit the unit ball for a Brownian motion starting at, say, the origin is \\(1/d\\). As \\(d\\) increases the expected time decreases as we only need one of the component, independent Brownian motions to leave the interval \\((-1, 1)\\). The more dimensions we have, the more possibilities there are for an exit event."
  },
  {
    "objectID": "FCSC/ch6/ch6_exercises.html#the-heat-equation",
    "href": "FCSC/ch6/ch6_exercises.html#the-heat-equation",
    "title": "Exercises",
    "section": "6.10 The Heat Equation",
    "text": "6.10 The Heat Equation\n\\[\nf(t, x) = \\E_x(g(B_t))\n\\] where \\(B_t\\) is a Brownian motion starting at \\(x\\). This can be rewritten as \\[\nf(t, x) = \\E(g(x + B_t))\n\\] where \\(B_t\\) is a Brownian motion starting at \\(0\\).\nAs an integral \\[\nf(t, x) = \\int g(x +y) \\frac{e^{-\\frac{\\|y\\|^2}{2t}}}{(2\\pi t)^{d/2}} dy.\n\\]\nTaking the derivative inside the integral \\[\n\\begin{align}\n\\partial_0 f(t, x) &= \\int g(x + y) ((\\|y\\|^2/(2t^2))(2\\pi t)^{d/2} - (d/2)(2 \\pi t)^{d/2 -1} 2 \\pi)e^{\\frac{-\\|y\\|^2}{2t}})/(2 \\pi t)^d dy \\\\\n&= \\int g(x + y) (\\|y\\|^2/(2t^2) - d/(2t)) \\frac{e^{\\frac{-\\|y\\|^2}{2t}}}{(2 \\pi t)^d} dy \\\\\n&= \\E(g(x+ B_t)(\\|B_t\\|^2/(2t^2) - d/(2t))).\n\\end{align}\n\\]\nTaking the Laplacian inside the integral \\[\n\\begin{align}\n\\triangle f(t, x) &= \\int \\triangle g(x + y) \\frac{e^{\\frac{-\\|y\\|^2}{2t}}}{(2 \\pi t)^{d/2}} dy \\\\\n&= \\sum_{i=1}^{d} \\E(\\partial^2_i g(x + B_t)) \\\\\n&= \\sum_{i=1}^{d} \\E(((B_t^{(i)})^2/t^2 - 1/t)g(x + B_t)) \\\\\n&= \\E(g(x + y)(\\|B_t\\|^2/t^2 - d/t)) \\\\\n&= 2 \\partial_0 f(t, x).\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nGaussian integration by parts \\[\n\\E(F'(Z)) = \\E(Z F(Z))\n\\] for a standard Gaussian \\(Z\\). For a Brownian motion,\n\\[\nG(B_t) = G(\\sqrt{t} Z) = F(Z).\n\\]\n\\[\nF'(Z) = G'(\\sqrt{t} Z) \\sqrt{t} = G'(B_t) \\sqrt{t}.\n\\]\nTherefore, \\[\n\\E(F'(Z)) = \\E(G'(B_t) \\sqrt{t}) = \\E(Z F(Z)) = \\E(\\frac{1}{\\sqrt{t}} B_t G(B_t)).\n\\] That is \\[\nt \\E(G'(B_t)) = \\E(B_t G(B_t)).\n\\]\nDefine\n\\[\nH(x) = x G(x)\n\\] so that \\[\nH'(x) = xG'(x) + G(x).\n\\] Then \\[\n\\begin{align}\n\\E(H'(B_t)) &= \\E(\\frac{1}{t} B_t H(B_t)) \\\\\n&= \\E(\\frac{1}{t}B_t(\\frac{1}{t} B_t G(B_t))) \\\\\n&= \\E(\\frac{B_t^2}{t^2} G(B_t)).\n\\end{align}\n\\] Therefore, \\[\n\\begin{align}\n\\E(G''(B_t)) &= \\E(\\frac{B_t^2}{t^2} G(B_t) - \\frac{1}{t}G(B_t)) \\\\\n&= \\E((\\frac{B_t^2}{t^2} - \\frac{1}{t}) G(B_t)).\n\\end{align}\n\\]\nSimilar relations hold when \\(G: \\mathbb{R}^d \\to \\mathbb{R}\\) takes as argument a \\(d\\)-dimensional Brownian motion:\n\\[\nt \\E(\\partial_i G(B_t)) = \\E(B^{(i)}_t G(B_t)),\n\\] and \\[\n\\E(\\partial_i^2 G(B_t) = \\E((\\frac{(B^{(i)}_t)^2}{t^2} - \\frac{1}{t}) G(B_t)).\n\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_experiments.html",
    "href": "FCSC/ch6/ch6_experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\sgn}{\\operatorname{sgn}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\G}{\\mathcal{G}}\n\\newcommand{\\qed}{\\tag*{$\\square$}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch6/ch6_experiments.html#d-brownian-motion",
    "href": "FCSC/ch6/ch6_experiments.html#d-brownian-motion",
    "title": "Experiments",
    "section": "6.1 2D Brownian Motion",
    "text": "6.1 2D Brownian Motion\nConsider a two-dimensional Brownian motion \\((B_t^{(1)}), B_t^{(2)})\\) starting at \\((0, 0)\\).\n\n(a)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef brownian(increments, start = 0, end = 5):\n  delta = (end -start)/increments\n  return np.cumsum(np.r_[0, np.random.default_rng().normal(0, np.sqrt(delta), increments - 1)])\n\n\ndef twoD_brownian(increments, start = 0, end = 5):\n  return [brownian(increments, start, end), brownian(increments, start, end)]\n\npath = twoD_brownian(1000)\nplt.plot(path[0], path[1])\nplt.plot(path[0][0], path[1][0], 'go')\nplt.plot(path[0][-1], path[1][-1], 'ro')\nplt.show()\n\n\n\n\n\npath = twoD_brownian(5000)\nplt.plot(path[0], path[1])\nplt.plot(path[0][0], path[1][0], 'go')\nplt.plot(path[0][-1], path[1][-1], 'ro')\nplt.show()\n\n\n\n\n\n\n(b)\n\nb = brownian(5000)\nW = [b, 0.5*b + np.sqrt(1-0.5**2)*brownian(5000)]\nplt.plot(W[0], W[1])\nplt.plot(W[0][0], W[1][0], 'go')\nplt.plot(W[0][-1], W[1][-1], 'ro')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch6/ch6_experiments.html#brownian-martingales",
    "href": "FCSC/ch6/ch6_experiments.html#brownian-martingales",
    "title": "Experiments",
    "section": "6.2 Brownian Martingales",
    "text": "6.2 Brownian Martingales\n\nfig, axs = plt.subplots(10, 2)\nstart, end, increments = 0, 1, 100\ndelta = (end - start)/increments\nbrownian_paths = [ twoD_brownian(increments, start=start, end=end) for _ in range(10) ]\nfor idx, b in enumerate(brownian_paths):\n  X = [ b[0][i]**2 + b[1][i]**2 - 2*delta*i for i in range(increments) ]\n  Y = np.exp(b[0])*np.cos(b[1]) \n  axs[idx, 0].plot(X)\n  axs[idx, 0].plot(Y)\n\n  X = np.cumsum([2*b[0][i]*(b[0][i+1] - b[0][i]) + 2*b[1][i]*(b[1][i+1] - b[1][i]) for i in range(increments -1)])\n  Y = np.cumsum([np.exp(b[0][i])*np.cos(b[0][i])*(b[0][i+1] - b[0][i]) - np.exp(b[1][i])*np.sin(b[1][i])*(b[1][i+1] - b[1][i]) for i in range(increments -1) ])+ 1\n  axs[idx, 1].plot(X)\n  axs[idx, 1].plot(Y)\n\nplt.show()"
  },
  {
    "objectID": "FCSC/ch6/ch6_experiments.html#dirichlet-problem",
    "href": "FCSC/ch6/ch6_experiments.html#dirichlet-problem",
    "title": "Experiments",
    "section": "6.3 Dirichlet Problem",
    "text": "6.3 Dirichlet Problem\n\\[\nh(x, y) = \\begin{cases}\n1 & \\text{ if } x^2 + y ^2 = 1 \\text{ and } y \\geq 0, \\\\\n-1 & \\text{ if } x^2 + y^2 =1 \\text { and } y < 0.\n\\end{cases}\n\\]\n\nstart_point = np.array([0, 1/2])\n\n\ndef h(start, delta=1/100):\n  pos = start.copy()\n  for _ in range(100):\n    r = pos[0]**2 + pos[1]**2\n    if r >= 1:\n      break\n    pos += np.random.default_rng().normal(0, np.sqrt(delta), 2)\n  return 1 if pos[1] >= 0 else -1\n\nnp.average([h(start_point) for _ in range(1000)])\n\n0.564\n\n\n\nn = 200\nmap = np.zeros([n, n])\nfor i, x in enumerate(np.linspace(-1, 1, n)):\n  for j, y in enumerate(np.linspace(-1, 1, n)):\n    map[j, i] = np.average([h([x, y], delta=1/10) for _ in range(100)])\n\nplt.imshow(map)\n\n<matplotlib.image.AxesImage at 0x7f44a80f3130>"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html",
    "href": "FCSC/ch3/ch3_experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#simulating-brownian-motion-using-increments",
    "href": "FCSC/ch3/ch3_experiments.html#simulating-brownian-motion-using-increments",
    "title": "Experiments",
    "section": "3.1 Simulating Brownian Motion Using Increments",
    "text": "3.1 Simulating Brownian Motion Using Increments\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef brownian(steps, time_interval):\n    normal_samples = np.random.default_rng().normal(0, np.sqrt((time_interval[1] - time_interval[0])/steps), steps)\n    return np.r_[0, np.cumsum(normal_samples)]\n\n\ndef plot_brownian(steps, time_interval):\n    for _ in range(10):\n        plt.plot(brownian(steps, time_interval))\n    plt.title('Brownian Motion')\n    plt.show()\n\nplot_brownian(100, [0, 1])\nplot_brownian(1000, [0, 1])"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#simulating-the-poisson-process",
    "href": "FCSC/ch3/ch3_experiments.html#simulating-the-poisson-process",
    "title": "Experiments",
    "section": "3.2 Simulating the Poisson Process",
    "text": "3.2 Simulating the Poisson Process\n\ndef poisson(rate, steps, time_interval):\n    poisson_samples = np.random.default_rng().poisson(rate*(time_interval[1] - time_interval[0])/steps, steps)\n    return np.cumsum(poisson_samples)\n\ndef plot_poisson(samples, rate, steps, time_interval):\n    for _ in range(samples):\n        plt.plot(np.linspace(time_interval[0], time_interval[1], steps), poisson(rate, steps, time_interval))\n    plt.title(f'Poisson Process, rate = {rate}')\n    plt.show()\n\nplot_poisson(10, 1, 100, [0,10])"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#arcsine-law",
    "href": "FCSC/ch3/ch3_experiments.html#arcsine-law",
    "title": "Experiments",
    "section": "3.3 Arcsine Law",
    "text": "3.3 Arcsine Law\n\ndef proportion_positive(samples):\n    positive = 0\n    for s in samples:\n        if s >= 0:\n            positive+=1\n    return positive/len(samples)\n\n\nbrownian_samples = [ proportion_positive(brownian(100, [0,1])) for _ in range(1000) ]\n\ndef pdf(x):\n    return (1/np.pi) * 1 /(np.sqrt(x*(1-x)))\n\nfig, ax = plt.subplots()\nax.hist(brownian_samples, label='histogram', density=True, bins=50)\nx = np.linspace(0, 1, 1000)\nax.plot(x, pdf(x), label='pdf')\nax.legend(loc='upper right')\nplt.plot()\n\n/tmp/ipykernel_46733/2730469909.py:12: RuntimeWarning:\n\ndivide by zero encountered in divide\n\n\n\n[]"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#arcsine-law-for-ornstein-uhlenbeck",
    "href": "FCSC/ch3/ch3_experiments.html#arcsine-law-for-ornstein-uhlenbeck",
    "title": "Experiments",
    "section": "3.4 Arcsine Law for Ornstein-Uhlenbeck",
    "text": "3.4 Arcsine Law for Ornstein-Uhlenbeck\n\nclass OU:\n    def __init__(self, samples):\n        covariance = [ [ (1/2)*np.exp(-(1/samples) * np.abs(i -j))*(1 - np.exp(-(2/samples)*min(i, j))) for i in range(1, samples)] for j in range(1, samples)]\n        self.__A = np.linalg.cholesky(covariance)\n        self.__samples = samples\n        return\n\n    def path(self, rg = np.random.default_rng()):\n        return np.r_[0, self.__A.dot(rg.normal(0, 1, self.__samples-1))]\n\nou = OU(100)\n\nrg = np.random.default_rng()\n\nou_samples = [ proportion_positive(ou.path(rg)) for _ in range(10000)]\n\nfig, ax = plt.subplots()\nax.hist(ou_samples, label='histogram', density=True, bins=100)\nax.legend(loc='upper right')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#brownian-variations",
    "href": "FCSC/ch3/ch3_experiments.html#brownian-variations",
    "title": "Experiments",
    "section": "Brownian Variations",
    "text": "Brownian Variations\n\ndef variation(samples):\n    return np.sum([ np.abs(samples[i+1] - samples[i]) for i in range(len(samples) - 1)])\n\n\ndef quadratic_variation(samples):\n    return np.sum([ (samples[i+1] - samples[i])**2 for i in range(len(samples) - 1)])\n\nbrownian_samples = brownian(2**20, [0,1])\nplt.plot(brownian_samples)\nplt.show()\n\nquadratic_variations = [ quadratic_variation(brownian_samples[::2**i]) for i in range(20, 0, -1) ]\n\nplt.plot(quadratic_variations)\nplt.title('Quadratic Variation')\nplt.show()\n\nvariations = [ variation(brownian_samples[::2**i]) for i in range(20, 0, -1) ]\n\nplt.plot(variations)\nplt.title('Variation')\nplt.show()"
  },
  {
    "objectID": "FCSC/ch3/ch3_experiments.html#simulating-brownian-motion-using-lévys-construction",
    "href": "FCSC/ch3/ch3_experiments.html#simulating-brownian-motion-using-lévys-construction",
    "title": "Experiments",
    "section": "3.6 Simulating Brownian Motion Using Lévy’s Construction",
    "text": "3.6 Simulating Brownian Motion Using Lévy’s Construction\n\ndef levy(N, sample_points):\n    def counter(N):\n        n = 1\n        j = 0\n        while True:\n            for k in range(2**j):\n                n = 2**j + k\n                if n >= N:\n                    return\n                yield j, k, n\n            j+=1\n\n    def Lambda(t):\n        if 0 <= t <= 1/2:\n            return t\n        elif 1/2 <= t <= 1:\n            return 1 - t\n        return 0\n\n    normal_samples = np.random.default_rng().normal(0, 1, N)\n\n    def B(t):\n        b = normal_samples[0] * t\n        for j, k, n in counter(N):\n            b+= 2**(-j/2) * Lambda((2**j) * t - k) * normal_samples[n]\n        return b\n\n    return [B(p) for p in sample_points]\n\n\"\"\"\ndef test_levy(N):\n    sum_t_1, sum_t_2 = 0, 0\n    time_values = np.linspace(0, 1, 3)\n    samples = 10000\n    for _ in range(samples):\n        _, t_1, t_2 = levy(N, time_values)\n        sum_t_1 += t_1**2\n        sum_t_2 += t_2**2\n    return sum_t_1/samples, sum_t_2/samples\n\nt_1_av, t_2_av = test_levy(100)\nprint(t_1_av, t_2_av) # we expect somewhere close to 0.5 and 1\n\"\"\"\n\n\ntime_values = np.linspace(0, 1, 100)\nfor N in [5, 20, 100]:\n    for _ in range(10):\n        plt.plot(levy(N, time_values))\n    plt.title(f'N={N}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI think \\(\\Lambda\\) is incorrectly defined on page 61. Should read\n\\[\n\\Lambda(t) = \\begin{cases}\nt & \\text{if } 0 \\leq t \\leq 1/2, \\\\\n1 - t & \\text{if } 1/2 < t \\leq 1, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]\nThe text is also inconsistent about the \\(2^{-j/2}\\) factor."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html",
    "href": "FCSC/ch3/ch3_exercises.html",
    "title": "Chapter Three: Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\P}{\\operatorname{P}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\trace}{\\operatorname{trace}}\n\\newcommand{\\F}{\\mathcal{F}}\n\\def\\iddots{{\\kern3mu\\raise1mu{.}\\kern3mu\\raise6mu{.}\\kern3mu\n\\raise12mu{.}}}\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#brownian-moments",
    "href": "FCSC/ch3/ch3_exercises.html#brownian-moments",
    "title": "Chapter Three: Exercises",
    "section": "3.1 Brownian Moments",
    "text": "3.1 Brownian Moments\n\n\\(\\E(B_t^6)\\)\n\nWe can use Wick’s/Isserlis’ formula to calculate this: there are \\((6 -1)!! = 15\\) different pairings of the \\(6\\)-tuple \\(\\{B_t, \\ldots, B_t\\}\\) and so\n\\[\n\\begin{align}\n\\E(B_t^6) &= 15 \\E(B_t^2)^3 \\\\\n&= 15 t^3.\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIn general, for a zero mean Gaussian variable \\(X\\) and \\(n >= 1\\)\n\\[\n\\E(X^{2n}) = (2n -1)!!\\E(X^2)^n.\n\\]\nThis can be seen with Wick’s formula:\nthere are \\((2n - 1)!!\\) different pairings of the set of \\(2n\\) elements \\((X,\\ldots, X)\\) and each pairing results in \\(n\\) pairs.\n\n\n\n\\(\\E((B_{t_2} - B_{t_1})(B_{t_3} - B_{t_2}))\\) if \\(t_1 < t_2 < t_3\\)\n\n\\[\n\\begin{align}\n\\E((B_{t_2} - B_{t_1})(B_{t_3} - B_{t_2})) &= \\E(B_{t_2} B_{t_3}) - \\E(B_{t_2}^2) - \\E(B_{t_1} B_{t_3}) + \\E(B_{t_1}B_{t_2}) \\\\\n&= t_2 - t_2 - t_1 + t_1 \\\\\n&= 0.\n\\end{align}\n\\]\n\n\\(\\E(B_s^2 B_t^2)\\) if \\(s < t\\).\n\nThe pairings of \\((B_s, B_s, B_t, B_t)\\) are\n\\(((B_s, B_s),(B_t, B_t)), ((B_s, B_t), (B_s, B_t)), ((B_s, B_t),(B_s, B_t))\\) and so by Wick’s formula\n\\[\n\\begin{align}\n\\E(B_s^2B_t^2) &= \\E(B_s^2)\\E(B_t^2) + 2 \\E(B_sB_t)^2 \\\\\n&= st + 2 s^2.\n\\end{align}\n\\]\n\n\\(\\E(B_s B_t^3)\\) if \\(s < t\\).\n\nThe pairings of \\((B_s, B_t, B_t, B_t)\\) are \\(((B_s, B_t),(B_t, B_t)), ((B_s, B_t), (B_t, B_t)), ((B_s, B_t),(B_t, B_t))\\) and so by Wick’s formula\n\\[\n\\begin{align}\n\\E(B_s B_t^3)  &= 3 \\E(B_sB_t)\\E(B_t^2) \\\\\n&= 3st.\n\\end{align}\n\\]\n\n\\(\\E(B_s^{100} B_t^{101})\\).\n\n\\(\\E(B_s^{100} B_t^{101}) = 0\\) because there are an odd number of multiplicands."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#brownian-probabilities",
    "href": "FCSC/ch3/ch3_exercises.html#brownian-probabilities",
    "title": "Chapter Three: Exercises",
    "section": "3.2 Brownian Probabilities",
    "text": "3.2 Brownian Probabilities\n\n\\(\\P(B_1 > 1, B_2 > 1)\\)\n\nThe integral is given by\n\n\nCode\nimport sympy as sp\nfrom fractions import Fraction\nfrom sympy.abc import x,y\n\nsp.init_printing()\nC=sp.Matrix([[1, 1], [1, 2]])\n\n\nCinv = C.inv()\nxy = sp.Matrix([x, y])\n\n\nf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/(2 *sp.pi * sp.sqrt(C.det(), evaluate=False))\npdf = f[0]\nintegral = sp.Integral(pdf, (x, 1, sp.oo), (y, 1, sp.oo))\nintegral\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp72o1vrwy/texput.png'\n\n\n\\(\\displaystyle \\int\\limits_{1}^{\\infty}\\int\\limits_{1}^{\\infty} \\frac{e^{- x^{2} + x y - \\frac{y^{2}}{2}}}{2 \\pi}\\, dx\\, dy\\)\n\n\nA change of variables gives a finite domain of integration:\n\n\nCode\npdf = pdf.subs({x: 1/x, y: 1/y})*sp.diff(1/x, x)*sp.diff(1/y, y)\n\nsp.Integral(pdf , (x, 0, 1), (y, 0, 1))\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpeszzaooh/texput.png'\n\n\n\\(\\displaystyle \\int\\limits_{0}^{1}\\int\\limits_{0}^{1} \\frac{e^{- \\frac{1}{2 y^{2}} + \\frac{1}{x y} - \\frac{1}{x^{2}}}}{2 \\pi x^{2} y^{2}}\\, dx\\, dy\\)\n\n\nThis can be calculated using a monte-carlo approximation:\n\nimport numpy as np\n\ndef monte_carlo(integrand):\n    samples = 10000\n    sum = 0\n    for _ in range(samples):\n        x, y = np.random.default_rng().uniform(0, 1, 2)\n        sum +=integrand(x, y)\n    return sum/samples\n\nprobability = monte_carlo(sp.lambdify([x, y], pdf))\nprobability\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpsmnekcld/texput.png'\n\n\n\\(\\displaystyle 0.110507682297571\\)\n\n\nAlternatively, we can evaluate the integral with Simpson’s rule:\n\nfrom scipy.integrate import simps\n\ndef simpsons(integrand):\n    x = np.linspace(0.01, 1, 1000)\n    y=  np.linspace(0.01, 1, 1000)\n\n    zz = integrand(x.reshape(-1, 1), y.reshape(1, -1))\n    return simps([simps(zz_r, x) for zz_r in zz], y)\n\nsimpsons(sp.lambdify([x, y], pdf))\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp07v2ichx/texput.png'\n\n\n\\(\\displaystyle 0.108067672895208\\)\n\n\nWe can check this makes sense by simulating a number of Brownian motion paths and checking the ratio of the samples satisfying \\(B_1 >1, B_2>1\\) to the total number of samples.\n\nclass Brownian:\n    def __init__(self, C):\n        self.__A = np.linalg.cholesky(C)\n        return\n    def path(self):\n        n = len(self.__A[0])\n        return self.__A.dot(np.random.default_rng().normal(0, 1, n))\n\nbrownian = Brownian(np.array(C.tolist()).astype(np.float64))\n\n\nsamples=10000\ncount = 0\nfor _ in range(samples):\n    b_1, b_2 = brownian.path()\n    if b_1 > 1 and b_2 > 1:\n        count+=1\n\ncount/samples\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpo8i5plnf/texput.png'\n\n\n\\(\\displaystyle 0.1033\\)\n\n\nWe can also use the fact that \\((B_1, B_2 - B_1)\\) are independent:\n\\[\n\\begin{align}\n\\P(B_1 > 1, B_2> 1) &= \\int \\int_{\\{x > 1, x + y > 1\\}} \\frac{e^{\\frac{-1}{2}(x^2 + y^2)}}{2 \\pi} dx \\, dy.\n\\end{align}\n\\]\nThe domain of integration is:\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nplt.xlim(-1, 3)\nplt.ylim(-1, 3)\n\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\n\naxis = plt.gca()\naxis.add_patch(Polygon([[1, 0], [1,3], [3, 3], [3,-2]], label='domain of integration'))\ndef f(x):\n    return 1 - x\n\nx = np.linspace(-1, 3, 1000)\n\nplt.plot(x, f(x), 'k--', label='y= 1 - x')\nplt.axvline(1, color='b', linestyle='--', label = 'x=1')\nplt.legend(loc='upper left')\n\n\n<matplotlib.legend.Legend at 0x7fc66abe2fd0>\n\n\n\n\n\nWe can calculate in chunks.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\n\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\n\naxis = plt.gca()\naxis.add_patch(Polygon([[1, 0], [1,3], [3, 3], [3,0]], label='A'))\naxis.add_patch(Polygon([[1, 0], [3,0], [3, -3]], label='B', color='red'))\nplt.legend(loc='upper left')\n\n\n<matplotlib.legend.Legend at 0x7fc66a9dc760>\n\n\n\n\n\n\nimport sympy as sp\nfrom fractions import Fraction\nfrom sympy.abc import x,y\n\npdf = sp.exp(Fraction(-1, 2) *(x**2 + y**2))/(2*sp.pi)\n\nA = sp.integrate(pdf, (x, 1, sp.oo), (y, 0, sp.oo));A\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp8c8zble0/texput.png'\n\n\n\\(\\displaystyle \\frac{\\sqrt{2} \\left(- \\sqrt{2} \\sqrt{\\pi} \\operatorname{erf}{\\left(\\frac{\\sqrt{2}}{2} \\right)} + \\sqrt{2} \\sqrt{\\pi}\\right)}{8 \\sqrt{\\pi}}\\)\n\n\n\nB = sp.integrate(pdf, (x, 1-y, sp.oo), (y, -sp.oo, 0)); B\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpe3rbvy4l/texput.png'\n\n\n\\(\\displaystyle \\frac{\\sqrt{2} \\left(\\int\\limits_{-\\infty}^{0} e^{- \\frac{y^{2}}{2}} \\operatorname{erf}{\\left(\\frac{\\sqrt{2} y}{2} - \\frac{\\sqrt{2}}{2} \\right)}\\, dy + \\int\\limits_{-\\infty}^{0} e^{- \\frac{y^{2}}{2}}\\, dy\\right)}{4 \\sqrt{\\pi}}\\)\n\n\n\nprob = (A+B).simplify(); prob\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp1e5_fzjw/texput.png'\n\n\n\\(\\displaystyle \\frac{\\sqrt{2} \\left(\\int\\limits_{-\\infty}^{0} e^{- \\frac{y^{2}}{2}} \\operatorname{erf}{\\left(\\frac{\\sqrt{2} y}{2} - \\frac{\\sqrt{2}}{2} \\right)}\\, dy + \\frac{\\sqrt{2} \\sqrt{\\pi}}{2}\\right) + \\sqrt{\\pi} \\left(1 - \\operatorname{erf}{\\left(\\frac{\\sqrt{2}}{2} \\right)}\\right)}{4 \\sqrt{\\pi}}\\)\n\n\n\nprob.evalf()\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp3hkxhtro/texput.png'\n\n\n\\(\\displaystyle 0.108067672862891\\)\n\n\n\n\\(\\P(B_1 > 1, B_2 > 1, B_3 > 1)\\)\n\n\n\nCode\nimport sympy as sp\nfrom fractions import Fraction\nfrom sympy.abc import x,y,z\n\nsp.init_printing()\nC=sp.Matrix([[1, 1, 1], [1, 2, 2], [1, 2, 3]])\n\n\nCinv = C.inv()\nxy = sp.Matrix([x, y, z])\n\n\nf = sp.exp(Fraction(-1, 2) * sp.MatMul(xy.transpose(), Cinv, xy))/((2 *sp.pi)**Fraction(3,2) * sp.sqrt(C.det(), evaluate=False))\npdf = f[0]\nintegral = sp.Integral(pdf, (x, 1, sp.oo), (y, 1, sp.oo), (z, 1, sp.oo))\nintegral\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp6drwmrzy/texput.png'\n\n\n\\(\\displaystyle \\int\\limits_{1}^{\\infty}\\int\\limits_{1}^{\\infty}\\int\\limits_{1}^{\\infty} \\frac{\\sqrt{2} e^{- x^{2} + x y - y^{2} + y z - \\frac{z^{2}}{2}}}{4 \\pi^{\\frac{3}{2}}}\\, dx\\, dy\\, dz\\)\n\n\nA change of variables gives a finite domain of integration:\n\n\nCode\npdf = pdf.subs({x: -1/x, y: -1/y, z: -1/z})*sp.diff(-1/x, x)*sp.diff(-1/y, y)*sp.diff(-1/z, z)\n\nsp.Integral(pdf , (x, -1, 0), (y, -1, 0), (z, -1, 0))\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp5w0s1da6/texput.png'\n\n\n\\(\\displaystyle \\int\\limits_{-1}^{0}\\int\\limits_{-1}^{0}\\int\\limits_{-1}^{0} \\frac{\\sqrt{2} e^{- \\frac{1}{2 z^{2}} + \\frac{1}{y z} - \\frac{1}{y^{2}} + \\frac{1}{x y} - \\frac{1}{x^{2}}}}{4 \\pi^{\\frac{3}{2}} x^{2} y^{2} z^{2}}\\, dx\\, dy\\, dz\\)\n\n\n\nfrom scipy.integrate import tplquad\n\ndef integrate(integrand):\n    return tplquad(integrand, -1, 0, -1, 0, -1, 0)\n\nintegrate(sp.lambdify([x, y, z], pdf))\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp6yq986h_/texput.png'\n\n\n\\(\\displaystyle \\left( 0.087052233285984, \\  1.48956711000826 \\cdot 10^{-8}\\right)\\)\n\n\n\ndef monte_carlo(integrand):\n    samples = 10000\n    sum = 0\n    for _ in range(samples):\n        x, y, z = np.random.default_rng().uniform(-1, 0, 3)\n        sum +=integrand(x, y, z)\n    return sum/samples\n\nprobability = monte_carlo(sp.lambdify([x, y, z], pdf))\nprobability\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpz27oqen6/texput.png'\n\n\n\\(\\displaystyle 0.0897530470124762\\)\n\n\n\ndef decomposition():\n    brownian = Brownian(np.array(C.tolist()).astype(np.float64))\n\n    samples=10000\n    count = 0\n    for _ in range(samples):\n        b_1 , b_2, b_3 = brownian.path()\n        if b_1 > 1 and b_2 > 1 and b_3 > 1:\n            count+=1\n    return count/samples\n\ndecomposition()\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp_piy40mb/texput.png'\n\n\n\\(\\displaystyle 0.0843\\)"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#equivalence-of-definition-of-brownian-motion",
    "href": "FCSC/ch3/ch3_exercises.html#equivalence-of-definition-of-brownian-motion",
    "title": "Chapter Three: Exercises",
    "section": "3.3 Equivalence of Definition of Brownian Motion",
    "text": "3.3 Equivalence of Definition of Brownian Motion\nIf \\(X = (X_i)_{i=1}^n = (B_{t_1} - 0, B_{t_2} - B_{t_1}, \\ldots, B_{t_n} - B_{t_{n-1}})\\) are independent Gaussians with mean zero and variance \\(t_{j+1} -t_j\\) for \\(j \\leq n-1\\), then the vector \\(Y = (Y_i)_{i=1}^n = (B_{t_1}, \\ldots, B_{t_n} )\\) is Gaussian with mean zero and covariance \\(\\E(B_tB_s) = t \\wedge s\\).\nProof:\nThe linear transformation \\(A\\) maps \\(X\\) to \\(Y\\):\n\\[\nA = \\begin{bmatrix}\n1  &  0    &  \\ldots  &  0 \\\\\n-1 &  1    &  \\ldots  &  0 \\\\\n\\vdots    & \\ddots & &  \\vdots \\\\\n0  &     \\ldots  &  -1      &  1\n\\end{bmatrix}\n\\]\nor\n\\[\nA_{ij} = \\begin{cases}\n1 & \\text{if } i=j, \\\\\n-1 & \\text{if } i = j-1, \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe mean vector of \\(Y\\) is \\(A\\textbf{0} = \\textbf{0}\\). The covariance of \\(Y\\), \\(C_Y\\), is related to the covariance of \\(X\\), \\(C_X\\) by\n\\[\nA C_X A^T = C_Y.\n\\]\nThis linear relationship can be expressed using cancellation of the telescopic sum: \\[\nY_i = \\sum_{k=1}^i X_k.\n\\]\nFor \\(i < j\\),\n\\[\nY_j = Y_i + \\sum_{k=i+1}^j X_k.\n\\]\nSo,\n\\[\nY_j Y_i = Y_i^2 + Y_i \\sum_{k=i+1}^j X_k.\n\\]\nThen, using the independence of \\(X\\): \\[\n\\begin{align}\n\\E(Y_j Y_i) &= \\E(Y_i^2) + \\sum_{k=i+1}^j \\E(Y_i X_k) \\\\\n&= \\E(Y_i^2) + \\sum_{k=i+1}^j \\E(\\sum_{l=1}^i X_l X_k) \\\\\n&= \\E(Y_i^2).\n\\end{align}\n\\]\nWe can calculate the variance of \\(Y_j\\):\n\\[\n\\begin{align}\n\\E(Y_j^2) & = \\E((\\sum_{k=1}^j X_k)^2) \\\\\n&=  \\E(\\sum_{k=1}^j X_k^2 + 2 \\sum_{k,l = 1}^j X_k X_l) \\\\\n&= \\sum_{k=1}^j \\E(X_k^2) \\\\\n&= \\sum_{k=2}^j (t_{i_{k}} -t_{i_{k-1}}) + t_{i_1} \\\\\n&= t_{i_j}\n\\end{align}\n\\]\nThis proves that\n\\(\\E(B_{t_i} B_{t_j}) = t_i \\wedge t_j\\)."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#reflection-at-time-s",
    "href": "FCSC/ch3/ch3_exercises.html#reflection-at-time-s",
    "title": "Chapter Three: Exercises",
    "section": "3.4 Reflection at time \\(s\\)",
    "text": "3.4 Reflection at time \\(s\\)\nFor any \\(s \\geq 0\\), the process \\((\\tilde{B}_t, t\\geq 0)\\) defined by\n\\[\n\\tilde{B}_t = \\begin{cases}\nB_t & \\text{if } t \\leq s, \\\\\nB_s - (B_t - B_s) & \\text{otherwise}\n\\end{cases}\n\\] is a Brownian motion.\nProof:\nClearly, \\(\\tilde{B}_0 = 0\\). For each continuous path, \\(B_t(\\omega)\\) we can have a continuous path \\(\\tilde{B}(\\omega)\\):\n\\(\\tilde{B}_t(\\omega)\\) is piecewise two obviously continuous functions \\(B_t(\\omega)\\) and \\(2B_s(\\omega) - B_t(\\omega)\\) which are equal at \\(t = s\\). Therefore, for a set of \\(\\omega\\) of probability one, \\(\\tilde{B}_t(\\omega)\\) is continuous.\nFor any \\(t_1 < t_2 < \\ldots < t_n\\), the vector \\((\\tilde{B}_{t_i})_{i=1}^n\\) is Gaussian of mean zero because it is a transformation of \\((B_{t_i})_{i=1}^n\\):\n\\[\n\\tilde{B} = A B + C\n\\]\nwhere\n\\[\nA_{ij} = \\begin{cases}\n1 & \\text{if } i=j \\text{ and } t_i \\leq s, \\\\\n-1& \\text{if } i=j \\text{ and } t_i > s, \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nand \\(C\\) is the Gaussian vector defined \\[\nC_i = \\begin{cases}\n2 B_s & \\text{if } t_i > s, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\] The linear transformation \\(AB\\) results in a Gaussian vector; adding \\(C\\) results in another Gaussian vector because the components of \\(C\\) are in the span of the components of \\(B\\). The mean of the result is zero.\nIf \\(t_1, t_2 \\leq s\\), then \\[\n\\begin{align}\n\\E(\\tilde{B}_{t_1} \\tilde{B}_{t_2}) &= \\E(B_{t_1} B_{t_2}) \\\\\n&= t_1 \\wedge t_2.\n\\end{align}\n\\]\nIf \\(t_1 \\leq s\\) and \\(t_2 > s\\), then \\[\n\\begin{align}\n\\E(\\tilde{B}_{t_1} \\tilde{B}_{t_2}) &= \\E(B_{t_1} (2B_s - B_{t_2})) \\\\\n&= 2 \\E(B_{t_1} B_s) - \\E(B_{t_1} B_{t_2}) \\\\\n&= 2 t_1 - t_1 \\\\\n&= t_1 \\\\\n&= t_1 \\wedge t_2.\n\\end{align}\n\\]\nIf \\(t_1, t_2 > s\\), then \\[\n\\begin{align}\n\\E(\\tilde{B}_{t_1} \\tilde{B}_{t_2}) &= \\E((2B_{s} - B_{t_1})(2B_s - B_{t_2})) \\\\\n&= 4 \\E(B_s^2) -2 \\E(B_s B_{t_2}) -2 \\E(B_s B_{t_1}) + \\E(B_{t_1}B_{t_2}) \\\\\n&= 4 s - 2 s - 2s  + t_1 \\wedge t_2 \\\\\n&= t_1 \\wedge t_2.\n\\end{align}\n\\]\nTherefore, for any \\(s, t\\)\n\\[\n\\E(\\tilde{B}_s \\tilde{B}_t) = s \\wedge t.\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#time-reversal",
    "href": "FCSC/ch3/ch3_exercises.html#time-reversal",
    "title": "Chapter Three: Exercises",
    "section": "3.5 Time Reversal",
    "text": "3.5 Time Reversal\nLet \\((B_t, t \\geq 0)\\) be a Brownian motion. The process \\((B_1 - B_{1-t}, t \\in [0,1])\\) has the distribution of a standard Brownian motion on \\([0, 1]\\).\nProof:\n\\(B_1 - B_{1-t} = 0\\) when \\(t = 0\\). If \\(\\omega\\) is such that \\(B_t(\\omega)\\) is continuous, then \\(B_{1-t}(\\omega)\\) is continuous since \\(t \\mapsto 1-t\\) is continuous. It follows that \\(B_1(\\omega) - B_{1-t}(\\omega)\\) is continuous. Therefore, for \\(\\omega\\) in a set of probability one, \\(B_1(\\omega) - B_{1-t}(\\omega)\\) is continuous.\nLet \\(0 \\leq t_1 < t_2 < \\ldots < t_n \\leq 1\\). The vector \\((B_1 -B_{1-t_i})_{i=1}^n\\) is Gaussian: it is simply a Brownian Gaussian vector written in reverse with \\(B_1\\) added, which results in a Gaussian vector. It is easy to see that the mean is zero by linearity of expectation.\nThe covariance reveals the distribution:\n\\[\n\\begin{align}\n\\Cov((B_1 - B_{1-t_i}), (B_1 - B_{1-t_j})) &= \\E((B_1 - B_{1-t_i})(B_1 - B_{1-t_j})) \\\\\n&= \\E(B_1^2) - \\E(B_{1-t_i}B_1) - \\E(B_{1 -t_j}B_1) + \\E(B_{1-t_i} B_{1-t_j}) \\\\\n&= 1 - (1-t_i) - (1-t_j) + (1-t_i) \\wedge (1-t_j) \\\\\n&= t_i + t_j -1 + 1 - t_i \\vee t_j \\\\\n&= t_i + t_j - t_i \\vee t_j \\\\\n& = t_i \\wedge t_j.\n\\end{align}\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#time-inversion",
    "href": "FCSC/ch3/ch3_exercises.html#time-inversion",
    "title": "Chapter Three: Exercises",
    "section": "3.6 Time Inversion",
    "text": "3.6 Time Inversion\n\nLet \\((B_t, t \\geq 0)\\) be a standard Brownian motion. The process\n\n\\[\nX_t = t B_{1/t} \\text{ for } t > 0,\n\\]\nhas the distribution of a Brownian motion on \\(t > 0\\).\nProof:\nLet \\(0 < t_1 < \\ldots < t_n\\) and define \\(s_{n-i} = 1/t_i\\), so that \\(0 < s_1 < \\ldots < s_n\\). The vector \\(T=(B_{t_i})_{i=1}^n\\) is Gaussian by assumption. \\(S=(\\frac{1}{s_i} B_{s_i})_{i=1}^{n}\\) is a linear transformation of \\(T\\) and so is also Gaussian with mean zero:\n\\[\nS = \\begin{bmatrix}\n0 & \\ldots & 0 & \\frac{1}{s_1} \\\\\n0 & \\ldots & \\frac{1}{s_2} & 0 \\\\\n\\vdots & \\iddots & 0 & 0 \\\\\n\\frac{1}{s_n} & 0 & \\ldots & 0\n\\end{bmatrix}\\,T.\n\\]\nThe covariance, and hence the distribution, of \\(S\\) can be found by simple calculation:\n\\[\n\\begin{align}\n\\E(t_i B_{1/t_i} t_jB_{1/t_j}) &= t_it_j \\frac{1}{t_i} \\wedge \\frac{1}{t_j} \\\\\n&= \\frac{t_it_j}{t_i \\vee t_j} \\\\\n&= t_i \\wedge t_j.\n\\end{align}\n\\]\n\n\\(X_t \\to 0\\) in \\(L^2\\) as \\(t \\to 0\\).\n\nProof:\n\\[\n\\begin{align}\n\\| X_t \\|_2 &= \\| t B_{1/t} \\|_2 \\\\\n&= \\E(t^2 B_{1/t}^2) \\\\\n&= t^2 1/t \\\\\n&= t \\to 0\n\\end{align}\n\\] as \\(t \\to 0\\).\n\n\\[\n\\lim_{t \\to \\infty} \\frac{B_t}{t} = 0\n\\] almost surely.\n\nProof:\nWe are allowed to use\n\\[\nX_t \\to 0\n\\] as \\(t \\to 0\\) almost surely. Note: we didn’t show this above.\n\\[\n\\lim_{t\\to \\infty} \\frac{B_t}{t} = X_{1/t} \\to 0\n\\] as \\(t \\to \\infty\\), almost surely."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#convergence-in-mean-or-in-l1",
    "href": "FCSC/ch3/ch3_exercises.html#convergence-in-mean-or-in-l1",
    "title": "Chapter Three: Exercises",
    "section": "3.8 Convergence in mean or in \\(L^1\\)",
    "text": "3.8 Convergence in mean or in \\(L^1\\)\nIf \\(X_n \\to X\\) in \\(L^1(\\Omega, \\mathcal{F}, \\P)\\), then \\(X_n \\to X\\) in probability.\nProof:\n\\(X_n \\to X\\) in \\(L^1\\) iff \\(\\E(|X_n - X|) \\to 0\\) as \\(n \\to \\infty\\).\nBy Markov’s inequality, for any \\(\\delta > 0\\)\n\\[\n\\begin{align}\n\\P(|X_n - X| > \\delta) &\\leq \\frac{1}{\\delta} \\E(|X_n - X|).\n\\end{align}\n\\] The RHS tends to zero as \\(n \\to \\infty\\) so the LHS must also. That is, \\(X_n \\to X\\) in probability as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#fractional-brownian-motion",
    "href": "FCSC/ch3/ch3_exercises.html#fractional-brownian-motion",
    "title": "Chapter Three: Exercises",
    "section": "3.9 Fractional Brownian Motion",
    "text": "3.9 Fractional Brownian Motion\nFractional Brownian motion \\((Y_t, t\\geq 0)\\) with index \\(0 < H < 1\\), is the Gaussian process with mean zero and covariance \\[\n\\Cov(Y_s, Y_t) = \\frac{1}{2}(t^{2H} + s^{2H} - \\left | t - s \\right |^{2H}).\n\\]\n\nThe standard Brownian motion corresponds to \\(H= 1/2\\).\n\nProof:\nWhen \\(H=1/2\\), \\[\n\\begin{align}\n\\Cov(Y_s, Y_t) &= \\frac{1}{2}(t + s - \\left | t - s\\right |) \\\\\n&= s \\wedge t.\n\\end{align}\n\\]\n\n\\(Y_{at}\\) has the same distribution as \\(a^{2H} Y_t\\) for \\(a > 0\\).\n\nProof:\n\\[\n\\begin{align}\n\\Cov(Y_{as}, Y_{at}) &= \\frac{1}{2}(a^{2H}t^{2H} + a^{2H}s^{2H} - \\left | at - as\\right |^{2H}) \\\\\n&= a^{2H} \\frac{1}{2}(t^{2H} + s^{2H} - \\left | t - s \\right |^{2H}) \\\\\n&= a^{2H} \\Cov(Y_s, Y_t) \\\\\n&= \\Cov(a^H Y_s, a^H Y_t).\n\\end{align}\n\\]\n\nThe increment \\(Y_t - Y_s\\) has a Gaussian distribution and is stationary.\n\nProof:\nBy definition, \\((Y_t, Y_s)\\) is a Gaussian vector so \\(Y_t - Y_s\\) is Gaussian. The covariance depends only on \\(t - s\\) so \\(Y_t - Y_s\\) is stationary:\n\\[\n\\begin{align}\n\\Var(Y_t - Y_s) &= \\E((Y_t - Y_s)^2) \\\\\n&= \\E(Y_t^2 - 2 Y_tY_s + Y_s^2) \\\\\n&= \\E(Y_t^2) -2 \\E(Y_tY_s) + \\E(Y_s^2) \\\\\n&= t^{2H} + s^{2H} -(t^{2H} + s^{2H} - \\left| t - s \\right|^{2H}) \\\\\n&= \\left| t - s \\right|^{2H}.\n\\end{align}\n\\]\n\nThe increments \\(Y_t - Y_s\\) are independent if and only if \\(H=1/2\\). The increments are negatively correlated if \\(H < 1/2\\) and positively correlated if \\(H > 1/2\\).\n\nProof:\nIf \\(H=1/2\\), then the process is the standard Brownian motion and so the increments are independent.\nSuppose that the increments are independent. Since they are Gaussian, this is equivalent to the covariance of the increments being zero:\nfor all \\(a \\leq b \\leq c \\leq d\\) \\[\n\\begin{align}\n\\E((Y_a - Y_b)(Y_c - Y_d)) &= \\E(Y_aY_c) - \\E(Y_a Y_d) - \\E(Y_b Y_c) + \\E(Y_b Y_d) \\\\\n&= \\frac{1}{2}\\left ( a^{2H} + c^{2H} - |a - c|^{2H} - a^{2H} - d^{2H} + |a - d|^{2H} - b^{2H} - c^{2H} + |b - c|^{2H} + b^{2H} +d^{2H} - |b -d |^{2H}\\right) \\\\\n&= \\frac{1}{2} \\left ( (d -a)^{2H} - (c -a )^{2H} + (c -b)^{2H} - (d -b )^{2H}\\right) \\\\\n&= 0.\n\\end{align}\n\\]\nIn particular, If we set \\(a=0, b=1, c=1, d=2\\), then the condition becomes\n\\[\n\\begin{align}\n(d -a)^{2H} - (c -a )^{2H} + (c -b)^{2H} - (d -b )^{2H} \\\\\n= 2^{2H} - 1^{2H} + 0^{2H} - 1^{2H} \\\\\n= 2^{2H} - 2 = 0\n\\end{align}\n\\]\nand so \\(H=1/2\\).\nLet \\(0 \\leq s_1 \\leq t_1 \\leq s_2 \\leq t_2\\). Set\n\\(a_1 = t_2 - s_1\\), \\(a_2 = t_2 - t_1\\), \\(b_1 = s_2 - s_1\\) and \\(b_2 = s_2 - t_1\\).\n\\[\n\\E((Y_{t_1} - Y_{s_1})(Y_{t_2}-Y_{s_2})) = \\frac{1}{2} \\left ( f(a_1) - f(a_2) - (f(b_1) - f(b_2)) \\right )\n\\]\nwhere \\(f=x^{2H}\\). \\(a_1 - a_2 = b_1 - b_2 = t_1 - s_1\\) and so using convexity of \\(f\\) when \\(H > 1/2\\), the covariance is positive; when \\(H < 1/2\\), the covariance is negative because f is concave.\n\n\n\n\n\n\nNote\n\n\n\n\n\nCode\ndef f(x, H):\n    return np.power(x,2*H)\n\nH = 99/100\n\nplt.xlim(0.5, 4)\n\n\nax = [3.5, 2]\nfor i,x in enumerate(ax):\n    plt.axvline(x, color='r', linestyle='--', label = f'a_{i}')\n\nplt.plot(ax, f(ax, H))\n\nbx = [2.5, 1]\nfor i,x in enumerate(bx):\n    plt.axvline(x, color='r', linestyle='-', label = f'b_{i}')\n\nplt.plot(bx, f(bx, H))\n\nx = np.linspace(0,4, 1000)\nplt.plot(x, f(x, H))\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\\[\nf(a_1) - f(a_2) \\geq f(a_1) -f(b_2) \\geq f'(b_2) (a_1 - a_2) = f'(b_2) (t_1 -s_1)\n\\]\n\\[\nf(b_1) - f(b_2) \\leq f'(b_1) (b_1 - b_2) = f'(b_1) (t_1 - s_1)\n\\]\n\\[\nf(a_1) - f(a_2) -(f(b_1) - f(b_2)) \\geq 0.\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#the-arcsine-law-on-0-t",
    "href": "FCSC/ch3/ch3_exercises.html#the-arcsine-law-on-0-t",
    "title": "Chapter Three: Exercises",
    "section": "3.10 The Arcsine Law on \\([0, T]\\)",
    "text": "3.10 The Arcsine Law on \\([0, T]\\)\nBy the scaling property of Brownian motions, \\((B_t, t \\in [0,T])\\) has the same distribution as \\((\\frac{1}{\\sqrt{T}} B_{Tt}, t \\in [0, 1])\\). The multiplicative factor does not change the proportion of time the path is positive: the CDF is the same as for motion on \\([0,1]\\)."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#an-application-of-the-monotone-convergence-theorem",
    "href": "FCSC/ch3/ch3_exercises.html#an-application-of-the-monotone-convergence-theorem",
    "title": "Chapter Three: Exercises",
    "section": "3.11 An Application of the Monotone Convergence Theorem",
    "text": "3.11 An Application of the Monotone Convergence Theorem\nLet \\(X_n \\geq 0\\) be a sequence of random variables on \\((\\Omega, \\mathcal{F}, \\P)\\). Then\n\\[\n\\E(\\sum_{n\\geq 1} X_n) = \\sum_{n \\geq n} \\E(X_n).\n\\]\nProof:\nDefine\n\\[\nY_k = \\sum_{n=1}^k X_n.\n\\] Then \\(Y_k\\) is a sequence of random variables such that \\(0 \\leq Y_k \\leq Y_{k+1}\\) for all \\(k\\).\nBy the monotone convergence theorem,\n\\[\n\\lim_{k \\to \\infty} \\E(Y_k) = \\E(\\lim_{k \\to \\infty} Y_k).\n\\] That is,\n\\[\n\\sum_{n \\geq 1} \\E(X_n) = \\E(\\sum_{n \\geq 1} X_n).\n\\]"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#borel-contelli-lemma-i",
    "href": "FCSC/ch3/ch3_exercises.html#borel-contelli-lemma-i",
    "title": "Chapter Three: Exercises",
    "section": "3.12 Borel-Contelli Lemma I",
    "text": "3.12 Borel-Contelli Lemma I\nLet \\((A_n)\\) be a sequence of events in \\((\\Omega, \\mathcal{F}, \\P)\\) such that\n\\[\n\\sum_n \\P(A_n) < \\infty.\n\\]\nThen \\[\n\\P(\\{ \\omega \\in \\Omega: \\omega \\in A_n \\text{ for infinitely many } n\\}) = 0.\n\\]\nProof:\nImmediately, \\[\n\\begin{align}\n\\sum_n \\P(A_n) = \\sum_n \\E(\\mathbb{1}_{A_n}) < \\infty.\n\\end{align}\n\\]\nBy the monotone convergence theorem\n\\[\n\\sum_n \\E(\\mathbb{1}_{A_n}) = \\E(\\sum_n \\mathbb{1}_{A_n}) < \\infty.\n\\]\nIt follows that \\[\n\\P(\\sum_n \\mathbb{1}_{A_n} < \\infty) = 1.\n\\]\nThe sum of indicator functions is a sum of ones and zeros. For the sum to be finite for any \\(\\omega\\), \\(\\mathbb{1}_{A_n}(\\omega) = 1\\) for only finitely many \\(n\\) with probability 1 i.e. any \\(\\omega \\in A_n\\) for only finitely many \\(n\\). Therefore,\n\\[\n\\P(\\left \\{\\omega \\in \\Omega : \\omega \\in A_n \\text{ for finitely many } n\\right \\}) = 1\n\\] and the result follows by taking the complement."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#convergence-in-probability-nrightarrow-convergence-almost-surely",
    "href": "FCSC/ch3/ch3_exercises.html#convergence-in-probability-nrightarrow-convergence-almost-surely",
    "title": "Chapter Three: Exercises",
    "section": "3.13 Convergence in Probability \\(\\nRightarrow\\) Convergence Almost Surely",
    "text": "3.13 Convergence in Probability \\(\\nRightarrow\\) Convergence Almost Surely\nLet \\(U\\) be a uniform random variable taking values in \\([0, 1]\\).\nExpress each \\(n\\) as\n\\[\nn = 2^j + k\n\\] where \\(k= 0, \\ldots, 2^j-1\\).\nDefine\n\\[\nX_n = \\mathbb{1}_{A_n}\n\\] where \\(A_n = \\{\\omega \\in \\Omega : U(\\omega) \\in [2^{-j}k, 2^{-j}(k+1) ]\\}\\).\n\\(X_n\\) converges to \\(0\\) in probability.\nProof:\n\\[\n\\begin{align}\n\\P( |X_n| > \\delta ) &= \\begin{cases}\n2^{-j} & 0 < \\delta <= 1, \\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\n&\\to 0\n\\end{align}\n\\]\nas \\(n \\to \\infty\\).\n\\(\\square\\)\n\\(X_n\\) does not converge to \\(0\\) almost surely.\nProof:\nLet \\(\\omega \\in \\Omega\\). Let \\(x = U(\\omega)\\). For \\(n = 2^j + k\\),\n\\(x \\in [2^{-j}k, 2^{-j}(k+1)]\\) for exactly one \\(k\\). Therefore, \\((X_n(\\omega))\\) is an infinite sequence of \\(0\\) and \\(1\\) which never converges.\n\\(\\square\\)"
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#but-ok-on-a-subsequence",
    "href": "FCSC/ch3/ch3_exercises.html#but-ok-on-a-subsequence",
    "title": "Chapter Three: Exercises",
    "section": "3.14 But OK on a Subsequence",
    "text": "3.14 But OK on a Subsequence\nLet \\(X_n \\to X\\) in probability as \\(n \\to \\infty\\) and let \\(\\delta > 0\\). For each \\(n\\), there is \\(m(n)\\) such that\n\\[\n\\P(|X_{n_k} - X| > \\delta) < \\frac{1}{2^n}\n\\] for \\(n_k \\geq m(n)\\). Choose one such \\(n_k\\) for each \\(n\\) to create a subsequence for \\((X_{n_k})\\).\n\\[\n\\begin{align}\n\\sum_k \\P(|X_{n_k} - X| > \\delta) &< \\sum_n \\frac{1}{2^n} < \\infty.\n\\end{align}\n\\]\nThis subsequence will converge almost surely."
  },
  {
    "objectID": "FCSC/ch3/ch3_exercises.html#construction-of-the-poisson-process",
    "href": "FCSC/ch3/ch3_exercises.html#construction-of-the-poisson-process",
    "title": "Chapter Three: Exercises",
    "section": "3.15 Construction of the Poisson Process",
    "text": "3.15 Construction of the Poisson Process\n\n\n\n\n\n\nWarning\n\n\n\nThe book says that \\(\\tau_k\\) are exponential with parameter \\(1/\\lambda\\). I think the parameter should be \\(\\lambda\\). The mean would then be \\(1/\\lambda\\): maybe there’s some confusion here between the parameter and the mean arrival time?\n\n\nWe know that\n\\[\n\\tau_1 + \\cdots + \\tau_k \\sim \\ \\operatorname{Gamma}\\left (k+1, \\lambda \\right).\n\\]\nThe PDF of \\(\\operatorname{Gamma}\\left (k, \\lambda \\right)\\) is\n\\[\nf(x) = \\frac{\\lambda^k}{(k-1)!} x^{k-1} e^{-\\lambda x}.\n\\]\nTherefore,\n\\[\n\\begin{align}\n\\P(\\tau_1 + \\ldots + \\tau_k \\leq t) &= \\int_0^t \\frac{\\lambda^k}{ (k-1)!} x^{k-1} e^{-\\lambda x}\\, dx \\\\\n&= \\frac{1}{(k-1)!}\\int_0^{\\lambda t} x^{k-1} e^{-x}\\, dx \\\\\n&= 1 - e^{-\\lambda t} \\sum_{n=0}^{k-1} \\frac{t^{n} \\lambda^{n}}{n!}\n\\end{align}\n\\]\nNow \\[\\P(N_t \\geq k) = \\P(\\tau_1 + \\cdots \\tau_k \\leq t)\\] and so\n\\[\n\\begin{align}\n\\P(N_t = k) &= \\P(N_t \\geq k+ 1) - \\P(N_t \\geq k) \\\\\n&= \\frac{e^{-\\lambda t}}{k!} t^k\\lambda^k.\n\\end{align}\n\\]\nThat is, \\(N_t\\) has Poisson distribution with parameter \\(\\lambda t\\).\nDefine \\(J_n = \\sum_{i=1}^n \\tau_i\\). Then using the memory loss property of exponential variables\n\\[\n\\begin{align}\n\\P(\\tau_{l+1} - (s - J_l) > t | N_s = l) &= \\P(\\tau_{l+1} > t + s - J_l | J_l \\leq s, J_{l+1} > s) \\\\\n&= \\P(\\tau_{l+1} > t + s - J_l | J_l \\leq s, \\tau_{l+1} > s - J_l) \\\\\n&= \\int_0^s \\P(\\tau_{l+1} > t + s - x | \\tau_{l+1} > s - x) d\\P_{J_l}(x) \\\\\n&= \\int_0^s \\P(\\tau_{l+1} > t) d\\P_{J_l}(x) \\\\\n&= \\P(\\tau_{l+1} > t) \\\\\n&= \\P(\\tau_1 > t).\n\\end{align}\n\\] Therefore, \\(\\tau_{l+1} - (s - J_l)\\) conditioned on \\(N_s = l\\) is exponential with parameter \\(\\lambda\\).\nFor \\(i > 1\\), \\(\\tau_{l+i}\\) are independent of \\(\\tau_j\\) for \\(j \\leq l\\) and so are independent of \\((N_r)_{r \\leq s}\\). So, conditioned on \\(N_s=l\\), \\(\\tau_{l+i}\\) are exponential with parameter \\(\\lambda\\) for \\(i \\geq 1\\).\nDefine\n\\[\nt_i = \\begin{cases}\n\\tau_{l+1} - (s - J_l) & \\text{ for } i = 0, \\\\\n\\tau_{l+i} & \\text{ otherwise}\n\\end{cases}.\n\\]\nWe have shown so far that \\(t_i\\) are exponentially distributed with parameter \\(\\lambda\\), conditioned on \\(N_s = l\\). They are also independent since\n\\[\n\\P(t_1 > s_1, \\ldots, t_k > s_k | N_s = l) = \\P(\\tau_1 > s_1, \\ldots, \\tau_k > s_k).\n\\]\nIf we use the law of total probability \\[\n\\begin{align}\n\\P(t_1 > s_1, \\ldots, t_k > s_k) &= \\sum_l \\P(t_1 > s_1, \\ldots, t_k > s_k | N_s = l) \\P(N_s = l) \\\\\n&= \\P(\\tau_1 > s_1, \\ldots, \\tau_k > s_k)\n\\end{align}\n\\] we see that \\((t_i)\\) are exponentially distributed with parameter \\(\\lambda\\) without conditioning! They are also independent without conditioning. We can also work backwards to state that \\((t_i)\\) are independent of \\(N_s\\). In fact, \\((t_i)\\) are independent of \\((N_r)_{r \\leq s}\\).\nTherefore, \\(Y_t\\) is the sum of IID exponential variables and has Poisson distribution with parameter \\(\\lambda t\\):\n\\[\n\\begin{align}\n\\P(Y_t \\geq k) &= \\frac{e^{-\\lambda t}}{k!} t^k\\lambda^k.\n\\end{align}\n\\]\nThat is, \\(Y_t = N_{t + s} - N_s\\) is Poisson distributed with parameter \\(\\lambda t\\) or, what is the same, \\(N_t - N_s\\) is Poisson distributed with parameter \\(\\lambda(t - s)\\).\nWe established that \\(N_s\\) and \\((t_i)\\) are independent. Since \\(Y_t\\) is a sum of \\((t_i)\\), it must be independent of \\((N_r)_{r \\leq s}\\). That is, \\(N_t - N_s\\) is independent of \\(N_s = N_s - N_0\\). So, at least two increments, \\(N_t - N_s\\) and \\(N_s - N_0\\), are independent. Suppose that \\(t_1 = 0 < t_1 < t_2 < \\ldots < t_n\\). For \\(n=2\\), we have established that \\((N_{t_{i+1}} - N_{t_i})_{i=0}^n\\) are independent. Suppose that the increments of a Poisson process are independent for \\(n=k\\) and consider the case \\(n=k+1\\). The increments \\((N_{t_{i+1}} - N_{t_i})_{i=1}^{k}\\) are independent because these can be considered to be \\(n\\) increments of the Poisson process \\(N'_t = N_t - N_{t_1}\\). We know that each of these increments is independent of \\(N_{t_1}\\) which completes the proof by induction."
  },
  {
    "objectID": "PML/ch2.html",
    "href": "PML/ch2.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "I only cover starred exercises here\n\n\n\nFor solutions to non-starred exercises see https://probml.github.io/pml-book/solns-public.pdf.\n\n\n\nExercise 2.1\nLet \\(H \\in \\{1, \\ldots, K\\}\\) be a discrete random variable, and let \\(e_1\\) and \\(e_2\\) be the observed values of two other random variables \\(E_1\\) and \\(E_2\\).\nWe wish to calculate\n\\[\n\\vec{P}(H | e_1, e_2) = (P(H=1|e_1, e_2), \\ldots, P(H= K)).\n\\]\nFor \\(i \\in \\{1, \\ldots, K\\}\\),\n\\[\n\\begin{align}\nP(H=i|e_1, e_2) &= \\frac{P(H=i,e_1, e_2)}{P(e_1, e_2)} \\\\\n                &= \\frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)}.\n\\end{align}\n\\]\nSo, to perform the caculation, we need y - \\(P(e_1, e_2 |H = i)\\) for all \\(i \\in \\{1, \\ldots K\\}\\) i.e. \\(P(e_1, e_2 |H)\\) - \\(P(H=i)\\) for all \\(i \\in \\{1, \\ldots, K\\}\\) i.e. \\(P(H)\\) - \\(P(e_1, e_2)\\)\nSo the solution to (a) is (ii).\nNow we assume that \\(E_1 \\perp E_2 |H\\). This means that\n\\[\nP(e_1, e_2 |H=i) = P(e_1|H=i) P(e_2|H=i)\n\\] for all \\(i \\in \\{1, \\ldots, K\\}\\). With this assumption \\(\\{P(e_1, e_2), P(H), P(e_1|H), P(e_2|H)\\}\\), set (i), is now sufficient to perform the calculation:\n\\[\n\\begin{align}\nP(H=i|e_1, e_2) &= \\frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)} \\\\\n&= \\frac{P(e_1 |H=i) P(e_2|H=i) P(H=i)}{P(e_1, e_2)}\n\\end{align}\n\\] for all \\(i \\in \\{1, \\ldots, K\\}\\).\n\n\nExercise 2.3\nThe question is unclear about notation. We’ll assume that \\(p\\) will denote probability density functions i.e.\n\n\\(p(x,y|z)\\) denotes the joint distribution of \\(X\\) and \\(Y\\) conditioned on \\(Z\\)\n\\(p(x|z)\\) denotes the conditional distribution of \\(X\\) given \\(Z\\)\n\\(p(y|z)\\) denotes the conditional distribution of \\(Y\\) given \\(Z\\)\n\nSuppose there exists \\(g\\) and \\(h\\) such that\n\\[\np(x, y |z) = g(x, z) h(y, z).\n\\]\nBy definition \\[\n\\begin{align}\np(x | z) &= \\int p(x, y |z) dy \\\\\n         &= \\int g(x, z) h(y, z) dy \\\\\n         &= g(x, z) \\int h(y, z) dy.\n\\end{align}\n\\] Similarly, \\[\n\\begin{align}\np(y | z) &= \\int p(x, y |z) dx \\\\\n         &= \\int g(x, z) h(y, z) dx \\\\\n         &= h(y, z) \\int g(x, z) dx.\n\\end{align}\n\\]\nTherefore,\n\\[\n\\begin{align}\np(x |z)p (y | z) &= g(x, z) h(y, z) \\int \\int g(x, z) h(y,z) dx dy \\\\\n&= p(x, y| z) \\int \\int p(x, y |z) dx dy \\\\\n&= p(x, y | z)\n\\end{align}\n\\]\ni.e. \\(X \\perp Y | Z\\).\nThe other direction is trivial: if \\(X \\perp Y|Z\\) then we can set \\(g(x, z) = p(x | z)\\) and \\(h(y, z) = p(y |z)\\).\n\n\nExercise 2.5\nSuppose that \\(X, Y\\) are two points sampled indpendently and uniformly at random from the interval \\([0, 1]\\). What is the expected location of the leftmost point?\nBy independence, we can express the joint CDF of \\(X\\) and \\(Y\\) as follows: \\[\n\\begin{align}\nF_{X,Y}(x, y) &= P(X \\leq x, Y \\leq y) \\\\\n&= P(X \\leq x) P(Y \\leq y) \\\\\n& = \\begin{cases}\n1  & \\text{for } x, y > 1,\\\\\nx & \\text{for } x  \\in [0,1], y > 1,\\\\\ny & \\text{for } y \\in [0,1], x > 1,\\\\\nxy & \\text{for } x, y \\in [0,1],\\\\\n0  & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nThe joint probability density is then \\[\n\\begin{align}\np(x, y) &= \\frac{\\partial^2}{\\partial_x \\partial_y}F_{X,Y}(x, y) \\\\\n&= \\begin{cases}\n1 & \\text{for } x,y \\in [0,1], \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nUsing LOTUS,\n\\[\n\\begin{align}\nE(\\min(X, Y)) &= \\int_0^1 \\int_0^1 \\min(x, y) dx dy \\\\\n&= \\int_0^1 (\\int_0^y x dx + \\int_y^1 y dx) dy \\\\\n&= \\int_0^1 (y - \\frac{y^2}{2}) dy \\\\\n&= \\frac{y^2}{2} -\\frac{y^3}{6} \\rvert_0^1 \\\\\n&= \\frac{1}{2} - \\frac{1}{6} \\\\\n&=  \\frac{1}{3}.\n\\end{align}\n\\]\nAnother approach is to use the fact that for \\(Z \\geq 0\\)\n\\[\nE(Z) = \\int_0^{\\infty} P(Z > z) dz.\n\\]\nIf we set \\(Z = \\min(X, Y)\\), then\n\\[\n\\begin{align}\nE(Z) &= \\int_0^{\\infty} P(\\min(X,Y) > z) dz \\\\\n&= \\int_0^1 P(\\min(X, Y) > z) dz \\\\\n&= \\int_0^1 P(X > z, Y > z) dz \\\\\n&= \\int_0^1 P(X > z) P(Y > z) dz \\\\\n&= \\int_0^1 (1 - z)^2 dz \\\\\n&= \\frac{-(1 - z)^3}{3} \\rvert_0^1 \\\\\n&= \\frac{1}{3}.\n\\end{align}\n\\]\n\n\nExercise 2.7\nLet \\(X \\sim Ga(a, b)\\) and \\(Y = 1/X\\). Derive the distribution of \\(Y\\).\nBy definition\n\\[\nGa(x | a, b) =  = \\frac{b^a}{\\Gamma(a)} x^{a-1}e^{-xb}.\n\\]\n\\[\nP(Y \\leq y) = P(1/X \\leq y) = P(X \\geq 1/y)\n\\]\n\\[\nP(X \\geq 1/y) = 1 - P( X < 1/y) = 1 - \\int_0^{1/y} Ga(x |a, b) dx\n\\]\nTaking the derivative with respect to \\(y\\)\n\\[\n\\begin{align}\n\\frac{d}{dy} P(Y \\leq y) &=  - \\frac{d}{dy} \\int_0^{1/y} Ga(x |a, b) dx \\\\\n&= -Ga(1/y |a, b) \\frac{d}{dy}(1/y) \\\\\n&= \\frac{1}{y^2} Ga(1/y |a, b).\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nI’ve used the second fundamental theorem of calculus and chain rule\n\\[\n\\frac{d}{dx} \\int_{g(x)}^{f(x)}  h(t) dt = h(f(x))f'(x) - h(g(x))g'(x).\n\\]\n\n\n\nTherefore, the pdf of \\(Y\\) is \\[\n\\frac{b^a}{\\Gamma(a)} (1/y)^{a-1}(1/y)^2e^{-b/y} = \\frac{b^a}{\\Gamma(a)} y^{-(a+1)}e^{-b/y}\n\\]\n\n\nExercise 2.9\nLet \\(D\\) be the event that you have a disease. Let \\(+\\) be the event that you test positive for this disease and let \\(-\\) be the event that you test negative for the same disease.\nWe are told that \\(P(+|D) = P(- | D^c) = 0.99\\). It follows that\n\\[\nP(+ | D^c) = 1 - P(-| D^C) = 10^{-2}.\n\\] We are also told that \\[\nP(D) = 10^{-4}.\n\\]\nThe probability of having the disease given a positive test can be expressed using Bayes’ Theorem:\n\\[\n\\begin{align}\nP(D | +)  &= \\frac{P(+ | D) P(D)}{P(+)} \\\\\n&= \\frac{P(+ | D) P(D)}{P(+| D)P(D) + P(+|D^c)P(D^c)} \\\\\n&= \\frac{0.99.10^{-4}}{0.99.10^{-4} + 10^{-2}(1 - 10^{-4})}.\n\\end{align}\n\\]\nUsing python as a calculator\n\naccuracy= 99/100 \nprevalance=1/10000\n\nanswer= accuracy*prevalance/(accuracy*prevalance + (1-accuracy)*(1 -prevalance))\nprint(answer)\n\n0.009803921568627442\n\n\nSo, with the test for this disease, a positive test means you have about 1% chance of having the disease.\n\n\nExercise 2.11\nLet’s phrase the problem in terms of a game with coins:\nI toss two fairs coins without showing you the result.\n\nYou ask if any of the coins are heads and I respond, truthfully, “yes”: what is the probability that one coin is a tail?\nI ask you to choose a coin to be revealed: you choose and it is a head. What is the probability that the other coin is a tail?\n\nLet \\(A\\) be the event that there is at least one head and \\(B\\) denote the event that one head has been revealed. When the event \\(B\\) occurs we know that there is at least one head and so \\(B\\) also occurs i.e. \\(B \\subseteq A\\). It follows that\n\\[\nP(B) \\leq P(A).\n\\]\nThe two events do not convey the same information: \\(B\\) tells you that after random sampling, the revealed coin is heads whereas \\(A\\) tells you that after looking at both coins, I reveal that at least one is heads. So before doing any calculation I’d guess that\n\\[\nP(B) < P(A).\n\\]\nLet \\(T\\) be the event of at least one tail, \\(H\\) be the event of at least one head and subscript these to indicate if a specific coin is a head or tail e.g. \\(T_1\\) is the event that coin \\(1\\) is a tail. Denote by \\(C_i\\) the event that coin \\(i\\) is chosen.\nWe can write \\[\n\\begin{align}\nP(A) &= P(T | H) \\\\\n     &= \\frac{P(T \\cap H)}{P(H)} \\\\\n     &= \\frac{P((T_1 \\cup T_2) \\cap (H_1 \\cup H_2))}{P(H_1 \\cup H_2)} \\\\\n     &= \\frac{P((T_1 \\cap H_2) \\cup (T_2 \\cap H_1))}{P(H_1 \\cup H_2)}.\n\\end{align}\n\\]\nNow,\n\\[\nP(H_1 \\cup H_2) = 3/4\n\\] because there are \\(3\\) out of \\(4\\) equally likely outcomes that have at least one head,\nand\n\\[\n\\begin{align}\nP((T_1 \\cap H_2) \\cup (T_2 \\cap H_1)) &= P(T_1 \\cap H_2) + P(T_2 \\cap H_1) \\\\\n&= P(T_1)P(H_2) + P(T_2)P(H_1) \\\\\n&= 1/4 + 1/4 \\\\\n& = 1/2.\n\\end{align}\n\\]\nTherefore, \\[\nP(A) = \\frac{1/2}{3/4} = 2/3.\n\\]\nSimilarly,\n\\[\n\\begin{align}\nP(B) &= P(T | (H_1 \\cap C_1) \\cup (H_2 \\cap C_2)) \\\\\n     &= \\frac{P(T \\cap ((H_1 \\cap C_1) \\cup (H_2 \\cap C_2)))}{P((H_1 \\cap C_1) \\cup (H_2 \\cap C_2))} \\\\\n     &= \\frac{P((H_1 \\cap T_2 \\cap C_1) \\cup (H_2 \\cap T_1 \\cap C_2))}{P((H_1 \\cap C_1) \\cup (H_2 \\cap C_2))}.\n\\end{align}\n\\]\nWe know that the events \\(C_1\\) and \\(C_2\\) are disjoint. So\n\\[\n\\begin{align}\nP((H_1 \\cap T_2 \\cap C_1) \\cup (H_2 \\cap T_1 \\cap C_2)) &= P(H_1 \\cap T_2 \\cap C_1) + P(H_2 \\cap T_1 \\cap C_2) \\\\\n&= P(C_1 | H_1 \\cap T_2)P(H_1 \\cap T_2) + P(C_2 |H_2 \\cap T_1) P(H_2 \\cap T_1) \\\\\n&= \\frac{1}{4}  (P(C_1|H_1 \\cap T_2) + P(C_2| H_2 \\cap T_1))\n\\end{align}\n\\]\nSimilarly,\n\\[\n\\begin{align}\nP((H_1 \\cap C_1) \\cup (H_2 \\cap C_2)) &= P(H_1 \\cap C_1) + P(H_2 \\cap C_2) \\\\\n&= P(C_1 | H_1)P(H_1) + P(C_2| H_2)P(H_2) \\\\\n&= \\frac{1}{2} (P(C_1 |H_1) + P(C_2| H_2)).\n\\end{align}\n\\]\nTherefore, \\[\n\\begin{equation}\nP(B) = \\frac{1}{2} \\frac{P(C_1 | H_1 \\cap T_2) + P(C_2|H_2 \\cap T_1)}{P(C_1|H_1) + P(C_2|H_2))}\n\\end{equation}\n\\tag{1}\\] Since the choice of coin is independent of the results of coin tosses (it must be because you know nothing about the results of the coin tosses)\n\\[\nP(B) = 1/2.\n\\]\n\n\n\n\n\n\nWhat happens if I pick the coins for you?\n\n\n\nThen the choice of coin is not necessarily independent: I could, for example, only show a head if both coin tosses result in a head. Then\n\\[\nP(C_1|H_1 \\cap T_2) = P(C_2|H_2 \\cap T_1) = 0\n\\] and so, by Equation 1, \\(P(B) = 0\\).\nOn the other hand, I could always show a head if it’s available and then \\[\nP(B) = 2/3\n\\] because this is essentially the same as answering the question “are there heads”.\nNow suppose that \\(0 \\leq \\alpha \\leq 1\\) and I adopt the following scheme: with probability \\(\\alpha\\) I show you a head when one is available and with probability \\((1 - \\alpha)\\) I only show you a head if both tosses were heads. Then\n\\[\nP(B) = \\frac{2 \\alpha}{3}.\n\\]\nIn summary, when I’m free to look at the coins and decide which coin to show, I can come up with a process with results in \\(P(B)\\) being any chosen value in \\([0, 2/3]\\).\n\n\nWe can use a simulation to support our conclusion and highlight the difference between the two scenarios:\n\nimport random\n\ndef head(coin):\n    return coin == 0\n\ndef tail(coin):\n    return coin == 1\n\ndef has_heads(coin):\n    return head(coin[0]) or head(coin[1])\n\ndef has_tails(coin):\n    return tail(coin[0]) or tail(coin[1])\n\niterations = 10000\n\ntails = 0\nfor i in range(0, iterations):\n    coins = []\n    while True:\n        # keep trying until we have at least one head\n        coins= [random.randint(0,1), random.randint(0,1)]\n        if has_heads(coins):\n            break\n    if has_tails(coins):\n        tails += 1\n\nprint(tails/iterations)\n\ntails = 0\n\nfor i in range(0, iterations):\n    def head_revealed():\n        while True:\n            # keep trying tosses and random coin picks until\n            # the revealed coin is a head\n            coins= [random.randint(0,1), random.randint(0,1)]\n            coin_choice = random.randint(0,1)\n            revealed_coin = coins[coin_choice]\n            remaining_coin = coins[(coin_choice+1)%2]\n            if head(revealed_coin):\n                return remaining_coin\n    if tail(head_revealed()):\n        tails +=1\n\nprint(tails/iterations)\n\n0.6744\n\n\n0.497"
  }
]