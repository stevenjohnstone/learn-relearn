---
title: "Probabilistic Machine Learning"
subtitle: "Chapter Two Exercises"
toc: true
toc-location: body
execute: 
  freeze: true
description: "Solutions to starred exercises"

format:
    html:
        code-fold: false
        html-math-method: mathjax
---

::: {.callout-note}
# I only cover starred exercises here

For solutions to non-starred exercises see <https://probml.github.io/pml-book/solns-public.pdf>.
:::

# Exercise 2.1

Let $H \in \{1, \ldots, K\}$ be a discrete random variable,
and let $e_1$ and $e_2$ be the observed values of two other
random variables $E_1$ and $E_2$.

We wish to calculate

$$
\vec{P}(H | e_1, e_2) = (P(H=1|e_1, e_2), \ldots, P(H= K)).
$$

For $i \in \{1, \ldots, K\}$,

$$
\begin{align}
P(H=i|e_1, e_2) &= \frac{P(H=i,e_1, e_2)}{P(e_1, e_2)} \\ 
                &= \frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)}.
\end{align}
$$

So, to perform the caculation, we need

- $P(e_1, e_2 |H = i)$ for all $i \in \{1, \ldots K\}$ i.e. $P(e_1, e_2 |H)$
- $P(H=i)$ for all $i \in \{1, \ldots, K\}$ i.e. $P(H)$
- $P(e_1, e_2)$

So the solution to (a) is (ii).

Now we assume that $E_1 \perp E_2 |H$. This means that

$$
P(e_1, e_2 |H=i) = P(e_1|H=i) P(e_2|H=i)
$$
for all $i \in \{1, \ldots, K\}$. With this assumption $\{P(e_1, e_2), P(H), P(e_1|H), P(e_2|H)\}$, set (i), is now sufficient to perform the calculation:

$$
\begin{align}
P(H=i|e_1, e_2) &= \frac{P(e_1, e_2 |H=i) P(H=i)}{P(e_1, e_2)} \\ 
&= \frac{P(e_1 |H=i) P(e_2|H=i) P(H=i)}{P(e_1, e_2)}
\end{align}
$$
for all $i \in \{1, \ldots, K\}$.
    
# Exercise 2.3
    
The question is unclear about notation. We'll assume that
$p$ will denote probability density functions i.e.

- $p(x,y|z)$ denotes the joint distribution of $X$ and $Y$ conditioned on $Z$
- $p(x|z)$ denotes the conditional distribution of $X$ given $Z$
- $p(y|z)$ denotes the conditional distribution of $Y$ given $Z$


Suppose there exists $g$ and $h$ such that

$$
p(x, y |z) = g(x, z) h(y, z).
$$

By definition
$$
\begin{align}
p(x | z) &= \int p(x, y |z) dy \\ 
         &= \int g(x, z) h(y, z) dy \\ 
         &= g(x, z) \int h(y, z) dy.
\end{align}
$$
Similarly,
$$
\begin{align}
p(y | z) &= \int p(x, y |z) dx \\ 
         &= \int g(x, z) h(y, z) dx \\ 
         &= h(y, z) \int g(x, z) dx.
\end{align}
$$

Therefore,

$$
\begin{align}
p(x |z)p (y | z) &= g(x, z) h(y, z) \int \int g(x, z) h(y,z) dx dy \\
&= p(x, y| z) \int \int p(x, y |z) dx dy \\
&= p(x, y | z)
\end{align}
$$

i.e. $X \perp Y | Z$.

The other direction is trivial: if $X \perp Y|Z$ then we can set $g(x, z) = p(x | z)$ and $h(y, z) = p(y |z)$.

# Exercise 2.5

Suppose that $X, Y$ are two points sampled indpendently and uniformly
at random from the interval $[0, 1]$. What is the expected location
of the leftmost point?

By independence, we can express the joint CDF of $X$ and $Y$ as follows:
$$
\begin{align}
F_{X,Y}(x, y) &= P(X \leq x, Y \leq y) \\ 
&= P(X \leq x) P(Y \leq y) \\ 
& = \begin{cases}
1  & \text{for } x, y > 1,\\ 
x & \text{for } x  \in [0,1], y > 1,\\
y & \text{for } y \in [0,1], x > 1,\\
xy & \text{for } x, y \in [0,1],\\ 
0  & \text{otherwise}
\end{cases}
\end{align}
$$

The joint probability density is then
$$
\begin{align}
p(x, y) &= \frac{\partial^2}{\partial_x \partial_y}F_{X,Y}(x, y) \\ 
&= \begin{cases}
1 & \text{for } x,y \in [0,1], \\
0 & \text{otherwise}
\end{cases}
\end{align}
$$

Using [LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician),

$$
\begin{align}
E(\min(X, Y)) &= \int_0^1 \int_0^1 \min(x, y) dx dy \\ 
&= \int_0^1 (\int_0^y x dx + \int_y^1 y dx) dy \\
&= \int_0^1 (y - \frac{y^2}{2}) dy \\
&= \frac{y^2}{2} -\frac{y^3}{6} \rvert_0^1 \\ 
&= \frac{1}{2} - \frac{1}{6} \\ 
&=  \frac{1}{3}.
\end{align}
$$

Another approach is to use the fact that for $Z \geq 0$

$$
E(Z) = \int_0^{\infty} P(Z > z) dz.
$$

If we set $Z = \min(X, Y)$, then

$$
\begin{align}
E(Z) &= \int_0^{\infty} P(\min(X,Y) > z) dz \\ 
&= \int_0^1 P(\min(X, Y) > z) dz \\
&= \int_0^1 P(X > z, Y > z) dz \\
&= \int_0^1 P(X > z) P(Y > z) dz \\
&= \int_0^1 (1 - z)^2 dz \\
&= \frac{-(1 - z)^3}{3} \rvert_0^1 \\
&= \frac{1}{3}.
\end{align}
$$



# Exercise 2.11

Let's phrase the problem in terms of a game with coins:

I toss two fairs coins without showing you the result.

(a) You ask if any of the coins are heads and I respond, truthfully, "yes": what is the probability that one coin is a tail?
(b) I ask you to choose a coin to be revealed: you choose and it is a head. What is the probability that the other coin is a tail?

Let $A$ be the event that there is at least one head and $B$ denote the event that one head has been revealed. When the event $B$ occurs we know that there is at least one head and so $B$ also occurs i.e. $B \subseteq A$. It follows that

$$
P(B) \leq P(A).
$$

The two events do not convey the same information: $B$ tells you that after random sampling, the revealed coin is heads whereas $A$ tells you that after looking at both coins, I reveal that at least one is heads. So before doing any calculation I'd guess that

$$
P(B) < P(A).
$$

Let $T$ be the event of at least one tail, $H$ be the event of at least one head and subscript these to indicate if a specific coin is a head or tail e.g. $T_1$ is the event that coin $1$ is a tail. Denote by $C_i$ the event that coin $i$ is chosen.

We can write
$$
\begin{align}
P(A) &= P(T | H) \\
     &= \frac{P(T \cap H)}{P(H)} \\ 
     &= \frac{P((T_1 \cup T_2) \cap (H_1 \cup H_2))}{P(H_1 \cup H_2)} \\
     &= \frac{P((T_1 \cap H_2) \cup (T_2 \cap H_1))}{P(H_1 \cup H_2)}.
\end{align}
$$

Now, 

$$
P(H_1 \cup H_2) = 3/4
$$
because there are $3$ out of $4$ equally likely outcomes that have at least one head, 

and

$$
\begin{align}
P((T_1 \cap H_2) \cup (T_2 \cap H_1)) &= P(T_1 \cap H_2) + P(T_2 \cap H_1) \\ 
&= P(T_1)P(H_2) + P(T_2)P(H_1) \\ 
&= 1/4 + 1/4 \\
& = 1/2.
\end{align}
$$

Therefore,
$$
P(A) = \frac{1/2}{3/4} = 2/3.
$$

Similarly,

$$
\begin{align}
P(B) &= P(T | (H_1 \cap C_1) \cup (H_2 \cap C_2)) \\ 
     &= \frac{P(T \cap ((H_1 \cap C_1) \cup (H_2 \cap C_2)))}{P((H_1 \cap C_1) \cup (H_2 \cap C_2))} \\
     &= \frac{P((H_1 \cap T_2 \cap C_1) \cup (H_2 \cap T_1 \cap C_2))}{P((H_1 \cap C_1) \cup (H_2 \cap C_2))}.
\end{align}
$$

We know that the events $C_1$ and $C_2$ are disjoint. So

$$
\begin{align}
P((H_1 \cap T_2 \cap C_1) \cup (H_2 \cap T_1 \cap C_2)) &= P(H_1 \cap T_2 \cap C_1) + P(H_2 \cap T_1 \cap C_2) \\
&= P(C_1 | H_1 \cap T_2)P(H_1 \cap T_2) + P(C_2 |H_2 \cap T_1) P(H_2 \cap T_1) \\ 
&= \frac{1}{4}  (P(C_1|H_1 \cap T_2) + P(C_2| H_2 \cap T_1))
\end{align}
$$

Similarly,

$$
\begin{align}
P((H_1 \cap C_1) \cup (H_2 \cap C_2)) &= P(H_1 \cap C_1) + P(H_2 \cap C_2) \\ 
&= P(C_1 | H_1)P(H_1) + P(C_2| H_2)P(H_2) \\ 
&= \frac{1}{2} (P(C_1 |H_1) + P(C_2| H_2)).
\end{align}
$$

Therefore,
$$
\begin{equation}
P(B) = \frac{1}{2} \frac{P(C_1 | H_1 \cap T_2) + P(C_2|H_2 \cap T_1)}{P(C_1|H_1) + P(C_2|H_2))}
\end{equation}
$${#eq-p_B}
Since the choice of coin is independent of the results of coin tosses (it must be because you know nothing about the results of the coin tosses)

$$
P(B) = 1/2.
$$

::: {.callout-note}
# What happens if I pick the coins for you?

Then the choice of coin is not necessarily independent: I could, for example, only show a head if both coin tosses result in a head. Then

$$
P(C_1|H_1 \cap T_2) = P(C_2|H_2 \cap T_1) = 0
$$
and so, by @eq-p_B, $P(B) = 0$.

On the other hand, I could always show a head if it's available
and then
$$
P(B) = 2/3
$$
because this is essentially the same as answering the question "are there heads".

Now suppose that $0 \leq \alpha \leq 1$ and I adopt the following scheme: with probability $\alpha$ I show you a head when one is available and with probability $(1 - \alpha)$ I only show you a head if both tosses were heads. Then

$$
P(B) = \frac{2 \alpha}{3}.
$$

In summary, when I'm free to look at the coins and decide which coin
to show, I can come up with a process with results in $P(B)$ being any
chosen value in $[0, 2/3]$.
:::


We can use a simulation to support our conclusion and highlight the
difference between the two scenarios:

```{python}
import random

def head(coin):
    return coin == 0

def tail(coin):
    return coin == 1

def has_heads(coin):
    return head(coin[0]) or head(coin[1])

def has_tails(coin):
    return tail(coin[0]) or tail(coin[1])

iterations = 10000

tails = 0
for i in range(0, iterations):
    coins = []
    while True:
        # keep trying until we have at least one head
        coins= [random.randint(0,1), random.randint(0,1)]
        if has_heads(coins):
            break
    if has_tails(coins):
        tails += 1

print(tails/iterations)

tails = 0

for i in range(0, iterations):
    def head_revealed():
        while True:
            # keep trying tosses and random coin picks until
            # the revealed coin is a head
            coins= [random.randint(0,1), random.randint(0,1)]
            coin_choice = random.randint(0,1)
            revealed_coin = coins[coin_choice]
            remaining_coin = coins[(coin_choice+1)%2]
            if head(revealed_coin):
                return remaining_coin
    if tail(head_revealed()):
        tails +=1

print(tails/iterations)
```




